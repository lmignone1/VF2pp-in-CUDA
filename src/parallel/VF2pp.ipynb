{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20137,"status":"ok","timestamp":1720087299834,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"OVKsPm9V3gQL","outputId":"4ac43593-baba-4333-8a20-2ef7a9b4001e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","os.chdir(\"drive/Othercomputers/pc/VF2pp-in-CUDA/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1719513584901,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"_IbccJNAcjIa","outputId":"c296cbb9-eb7b-469d-f97e-1142164ef141"},"outputs":[{"data":{"text/plain":["['LICENSE', 'README.md', 'data', 'tmp', 'src', '.git', '.gitignore', '.tips']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir()"]},{"cell_type":"markdown","metadata":{"id":"h498tzxnwDyi"},"source":["CUDA SETUP"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163743,"status":"ok","timestamp":1720087463574,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"rbh20Z5utmFQ","outputId":"4ab65fc5-9caa-4897-df89-d91c583311d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nvcc4jupyter\n","  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n","Installing collected packages: nvcc4jupyter\n","Successfully installed nvcc4jupyter-1.2.1\n","Collecting pycuda\n","  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytools>=2011.2 (from pycuda)\n","  Downloading pytools-2024.1.6-py2.py3-none-any.whl (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Collecting mako (from pycuda)\n","  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n","Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n","Building wheels for collected packages: pycuda\n","  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661206 sha256=66031e549dbcc1333870b26775423fa3507a1d40154a4efda73dadf0e8af2d9d\n","  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n","Successfully built pycuda\n","Installing collected packages: appdirs, pytools, mako, pycuda\n","Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.6\n"]}],"source":["!pip install nvcc4jupyter\n","!pip install pycuda"]},{"cell_type":"markdown","metadata":{"id":"8_P-CD0hwSmY"},"source":["GPU TYPE"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":589,"status":"ok","timestamp":1720087464148,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"ntofwbc0tEKS","outputId":"d0c09ce2-e191-4627-fc24-aa0adebff931"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jul  4 10:04:22 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp248176o9\".\n"]}],"source":["!nvidia-smi\n","!nvcc --version\n","%load_ext nvcc4jupyter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1719513751044,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"KfQ4iKK_p0hY","outputId":"b10ea098-99a4-49e7-e4b7-9b311ba18da3"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 device(s) found.\n","Device #0: Tesla T4\n"," Compute Capability: 7.5\n"," Total Memory: 14 GB\n"]}],"source":["import pycuda.driver as drv\n","import pycuda.autoinit\n","drv.init()\n","print(\"%d device(s) found.\" % drv.Device.count())\n","for i in range(drv.Device.count()):\n","  dev = drv.Device(i)\n","  print(\"Device #%d: %s\" % (i, dev.name()))\n","  print(\" Compute Capability: %d.%d\" % dev.compute_capability())\n","  print(\" Total Memory: %s GB\" % (dev.total_memory() // (1024 * 1024 * 1024)))"]},{"cell_type":"markdown","metadata":{"id":"8X72on3Cwj7C"},"source":["GPU INFO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2553,"status":"ok","timestamp":1719513753594,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"u9x2_pq4wwyI","outputId":"74d0aeb9-0451-4315-b098-dddb821e9438"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device number: 0\n","  Device name: Tesla T4\n","  Compute capability: 7.5\n","\n","  Clock Rate: 1590000 kHz\n","  Total SMs: 40 \n","  Shared Memory Per SM: 65536 bytes\n","  Registers Per SM: 65536 32-bit\n","  Max threads per SM: 1024\n","  L2 Cache Size: 4194304 bytes\n","  Total Global Memory: 15835660288 bytes\n","  Memory Clock Rate: 5001000 kHz\n","\n","  Max threads per block: 1024\n","  Max threads in X-dimension of block: 1024\n","  Max threads in Y-dimension of block: 1024\n","  Max threads in Z-dimension of block: 64\n","\n","  Max blocks in X-dimension of grid: 2147483647\n","  Max blocks in Y-dimension of grid: 65535\n","  Max blocks in Z-dimension of grid: 65535\n","\n","  Shared Memory Per Block: 49152 bytes\n","  Registers Per Block: 65536 32-bit\n","  Warp size: 32\n","\n","\n"]}],"source":["%%cuda\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","void deviceQuery()\n","{\n","  cudaDeviceProp prop;\n","  int nDevices=0, i;\n","  cudaError_t ierr;\n","\n","  ierr = cudaGetDeviceCount(&nDevices);\n","  if (ierr != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierr)); }\n","\n","\n","\n","  for( i = 0; i < nDevices; ++i )\n","  {\n","     ierr = cudaGetDeviceProperties(&prop, i);\n","     printf(\"Device number: %d\\n\", i);\n","     printf(\"  Device name: %s\\n\", prop.name);\n","     printf(\"  Compute capability: %d.%d\\n\\n\", prop.major, prop.minor);\n","\n","     printf(\"  Clock Rate: %d kHz\\n\", prop.clockRate);\n","     printf(\"  Total SMs: %d \\n\", prop.multiProcessorCount);\n","     printf(\"  Shared Memory Per SM: %lu bytes\\n\", prop.sharedMemPerMultiprocessor);\n","     printf(\"  Registers Per SM: %d 32-bit\\n\", prop.regsPerMultiprocessor);\n","     printf(\"  Max threads per SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n","     printf(\"  L2 Cache Size: %d bytes\\n\", prop.l2CacheSize);\n","     printf(\"  Total Global Memory: %lu bytes\\n\", prop.totalGlobalMem);\n","     printf(\"  Memory Clock Rate: %d kHz\\n\\n\", prop.memoryClockRate);\n","\n","\n","     printf(\"  Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n","     printf(\"  Max threads in X-dimension of block: %d\\n\", prop.maxThreadsDim[0]);\n","     printf(\"  Max threads in Y-dimension of block: %d\\n\", prop.maxThreadsDim[1]);\n","     printf(\"  Max threads in Z-dimension of block: %d\\n\\n\", prop.maxThreadsDim[2]);\n","\n","     printf(\"  Max blocks in X-dimension of grid: %d\\n\", prop.maxGridSize[0]);\n","     printf(\"  Max blocks in Y-dimension of grid: %d\\n\", prop.maxGridSize[1]);\n","     printf(\"  Max blocks in Z-dimension of grid: %d\\n\\n\", prop.maxGridSize[2]);\n","\n","     printf(\"  Shared Memory Per Block: %lu bytes\\n\", prop.sharedMemPerBlock);\n","     printf(\"  Registers Per Block: %d 32-bit\\n\", prop.regsPerBlock);\n","     printf(\"  Warp size: %d\\n\\n\", prop.warpSize);\n","\n","  }\n","}\n","\n","int main() {\n","    deviceQuery();\n","}"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2249,"status":"ok","timestamp":1720091603554,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"8japRd-9Hqws","outputId":"8b9f23b7-c59b-4bf0-bca7-4a214be1b3f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","max rarity\n","1\n","maxrarity filter\n","1 1 1 1 0 1 0 1 0 0 \n","max degree\n","7\n","maxdegree filter\n","0 1 0 0 0 0 0 0 0 0 \n","next node\n","1\n","\n","Levels at depth 0\n","0 1 0 0 0 0 0 0 0 0 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 1 0 0 0 0 0 0 0 0 \n","max conn\n","0\n","maxconn filter\n","0 1 0 0 0 0 0 0 0 0 \n","max degree\n","7\n","maxdegree filter\n","0 1 0 0 0 0 0 0 0 0 \n","max rarity\n","1\n","maxrarity filter\n","0 1 0 0 0 0 0 0 0 0 \n","next node\n","1\n","update conn\n","0 0 1 1 1 1 1 0 1 1 \n","\n","Levels at depth 1\n","0 0 1 1 1 1 1 0 1 1 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 1 1 1 1 1 0 1 1 \n","max conn\n","1\n","maxconn filter\n","0 0 1 1 1 1 1 0 1 1 \n","max degree\n","7\n","maxdegree filter\n","0 0 0 0 0 0 0 0 0 1 \n","max rarity\n","2\n","maxrarity filter\n","0 0 0 0 0 0 0 0 0 1 \n","next node\n","9\n","update conn\n","1 1 2 1 2 1 2 1 2 1 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 1 1 1 1 1 0 1 0 \n","max conn\n","2\n","maxconn filter\n","0 0 1 0 1 0 1 0 1 0 \n","max degree\n","5\n","maxdegree filter\n","0 0 1 0 0 0 0 0 1 0 \n","max rarity\n","1\n","maxrarity filter\n","0 0 1 0 0 0 0 0 0 0 \n","next node\n","2\n","update conn\n","2 2 2 1 2 2 2 1 3 2 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 0 1 1 1 1 0 1 0 \n","max conn\n","3\n","maxconn filter\n","0 0 0 0 0 0 0 0 1 0 \n","max degree\n","5\n","maxdegree filter\n","0 0 0 0 0 0 0 0 1 0 \n","max rarity\n","2\n","maxrarity filter\n","0 0 0 0 0 0 0 0 1 0 \n","next node\n","8\n","update conn\n","3 3 3 1 2 2 2 2 3 3 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 0 1 1 1 1 0 0 0 \n","max conn\n","2\n","maxconn filter\n","0 0 0 0 1 1 1 0 0 0 \n","max degree\n","4\n","maxdegree filter\n","0 0 0 0 1 0 0 0 0 0 \n","max rarity\n","1\n","maxrarity filter\n","0 0 0 0 1 0 0 0 0 0 \n","next node\n","4\n","update conn\n","4 4 3 1 2 2 2 3 3 4 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 0 1 0 1 1 0 0 0 \n","max conn\n","2\n","maxconn filter\n","0 0 0 0 0 1 1 0 0 0 \n","max degree\n","3\n","maxdegree filter\n","0 0 0 0 0 1 1 0 0 0 \n","max rarity\n","1\n","maxrarity filter\n","0 0 0 0 0 1 1 0 0 0 \n","next node\n","6\n","update conn\n","4 5 3 2 2 2 2 3 3 5 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 0 1 0 1 0 0 0 0 \n","max conn\n","2\n","maxconn filter\n","0 0 0 1 0 1 0 0 0 0 \n","max degree\n","5\n","maxdegree filter\n","0 0 0 1 0 0 0 0 0 0 \n","max rarity\n","1\n","maxrarity filter\n","0 0 0 1 0 0 0 0 0 0 \n","next node\n","3\n","update conn\n","5 6 3 2 2 3 3 4 3 5 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 0 0 0 1 0 0 0 0 \n","max conn\n","3\n","maxconn filter\n","0 0 0 0 0 1 0 0 0 0 \n","max degree\n","3\n","maxdegree filter\n","0 0 0 0 0 1 0 0 0 0 \n","max rarity\n","1\n","maxrarity filter\n","0 0 0 0 0 1 0 0 0 0 \n","next node\n","5\n","update conn\n","5 7 4 3 2 3 3 4 3 5 \n","\n","Levels at depth 2\n","1 0 0 0 0 0 0 1 0 0 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","1 0 0 0 0 0 0 1 0 0 \n","max conn\n","5\n","maxconn filter\n","1 0 0 0 0 0 0 0 0 0 \n","max degree\n","5\n","maxdegree filter\n","1 0 0 0 0 0 0 0 0 0 \n","max rarity\n","1\n","maxrarity filter\n","1 0 0 0 0 0 0 0 0 0 \n","next node\n","0\n","update conn\n","5 7 5 4 3 3 3 4 4 6 \n","\n","init is_good\n","1 1 1 1 1 1 1 1 1 1 \n","unoreder filter\n","0 0 0 0 0 0 0 1 0 0 \n","max conn\n","4\n","maxconn filter\n","0 0 0 0 0 0 0 1 0 0 \n","max degree\n","4\n","maxdegree filter\n","0 0 0 0 0 0 0 1 0 0 \n","max rarity\n","1\n","maxrarity filter\n","0 0 0 0 0 0 0 1 0 0 \n","next node\n","7\n","update conn\n","5 7 5 5 4 3 3 4 5 7 \n","Order:\t1 9 2 8 4 6 3 5 0 7 \n","Graphs are isomorphic\n","Mapping\n","0 -> 0\n","1 -> 1\n","2 -> 2\n","3 -> 3\n","4 -> 4\n","5 -> 5\n","6 -> 6\n","7 -> 7\n","8 -> 8\n","9 -> 9\n","\n"]}],"source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <stdbool.h>\n","#include <cuda_runtime.h>\n","#include <limits.h>\n","#include <string.h>\n","\n","#define FILENAME_QUERY \"data/graph_query_500.csv\"\n","#define FILENAME_TARGET \"data/graph_target_500.csv\"\n","#define LABELS 10\n","#define INF 99999\n","\n","#define CUDA_CHECK_ERROR(err)           \\\n","    if (err != cudaSuccess) {            \\\n","        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n","        printf(\"Error in file: %s, line: %i\\n\", __FILE__, __LINE__); \\\n","        exit(EXIT_FAILURE);              \\\n","    }\n","\n","/***** STRUCTS *****/\n","typedef struct {\n","    int* matrix;\n","    int numVertices;\n","    int* nodesToLabel;\n","    int** labelToNodes;\n","    int* labelsCardinalities;\n","    int* degrees;\n","} Graph;\n","\n","typedef struct {\n","    int *mapping1;  // mapping from query to target\n","    int *mapping2;  // mapping from target to query\n","    int *T1;        // Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are neighbors of nodes that are.\n","    int *T2;\n","    int* T1_out;     //Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti. Cioe nodi che non sono in mapping e non sono vicini di nodi coperti\n","    int* T2_out;\n","} State;\n","\n","typedef struct {\n","    int vertex;\n","    int* candidates;\n","    int sizeCandidates;\n","    int candidateIndex;\n","} Info;\n","\n","typedef struct StackNode {\n","    Info* info;\n","    struct StackNode* next;\n","} StackNode;\n","\n","/***** GRAPH PROTOTYPES *****/\n","void initGraph(Graph*);\n","Graph* createGraph();\n","void addEdge(Graph*, int, int);\n","Graph* readGraph(char*);\n","void printGraph(Graph*);\n","void freeGraph(Graph*);\n","void setLabel(Graph*, int, int);\n","\n","/***** STATE PROTOTYPES *****/\n","State* createState(Graph*, Graph*);\n","void freeState(State*);\n","void printState(State*, int);\n","bool isMappingFull(Graph*, State*);\n","void updateState(Graph*, Graph*, State*, int, int);\n","void restoreState(Graph*, Graph*, State*, int, int);\n","\n","/***** VF2++ PROTOTYPES *****/\n","void vf2pp(Graph*, Graph*, State*);\n","bool checkGraphProperties(Graph*, Graph*);\n","bool checkSequenceDegree(int*, int*, int);\n","int compare(const void*, const void*);\n","int* ordering(Graph*, Graph*);\n","int* bfs(Graph*, int, int*);\n","int findRoot(int, int*, int*, int*);\n","void processDepth(int*, int*, int*, int*, int*, int*, int, int*, int*, int*, int*);\n","int* findCandidates(Graph*, Graph*, State*, int, int*);\n","bool cutISO(Graph*, Graph*, State*, int, int);\n","\n","/***** STACK PROTOTYPES *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex);\n","StackNode* createStackNode(Info*);\n","void push(StackNode**, int*, int, int);\n","Info* pop(StackNode**);\n","bool isStackEmpty(StackNode*);\n","void freeStack(StackNode*);\n","void printStack(StackNode*);\n","void printInfo(Info*);\n","void freeInfo(Info*);\n","StackNode* createStack();\n","Info* peek(StackNode**);\n","\n","int main() {\n","    // printf(\"%s\\n\", FILENAME_QUERY);\n","    Graph* g1 = readGraph(FILENAME_QUERY);\n","    // printf(\"%s\\n\", FILENAME_TARGET);\n","    Graph* g2 = readGraph(FILENAME_TARGET);\n","    State* s = createState(g1, g2);\n","\n","    // printf(\"\\n\");\n","    // printState(s, g1->numVertices);\n","    // printf(\"\\n\");\n","\n","    vf2pp(g1, g2, s);\n","\n","     printf(\"Mapping\\n\");\n","     for(int i = 0; i < g1->numVertices; i++) {\n","         printf(\"%d -> %d\\n\", i, s->mapping1[i]);\n","     }\n","\n","    freeGraph(g1);\n","    freeGraph(g2);\n","    freeState(s);\n","\n","    return EXIT_SUCCESS;\n","}\n","\n","/***** GRAPH FUNCTIONS *****/\n","void initGraph(Graph* g) {\n","    g->matrix = (int*)malloc(g->numVertices * g->numVertices * sizeof(int));\n","    g->nodesToLabel = (int*)malloc(g->numVertices * sizeof(int));\n","    g->labelsCardinalities = (int*)malloc(LABELS * sizeof(int));\n","    g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));\n","    g->degrees = (int*)malloc(g->numVertices * sizeof(int));\n","\n","    if (g->nodesToLabel == NULL || g->labelsCardinalities == NULL || g->labelToNodes == NULL || g->degrees == NULL || g->matrix == NULL) {\n","        printf(\"Error allocating memory in initGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for (int vertex = 0; vertex < g->numVertices; vertex++) {\n","        g->nodesToLabel[vertex] = -1;\n","        g->degrees[vertex] = 0;\n","\n","        for (int adjVertex = 0; adjVertex < g->numVertices; adjVertex++) {\n","            g->matrix[vertex * g->numVertices + adjVertex] = 0;\n","        }\n","    }\n","\n","    for (int label = 0; label < LABELS; label++) {\n","        g->labelsCardinalities[label] = 0;\n","        g->labelToNodes[label] = (int*)malloc(g->numVertices * sizeof(int));\n","    }\n","}\n","\n","void setLabel(Graph* g, int node, int label) {\n","    if (g->nodesToLabel[node] == -1) {\n","        g->nodesToLabel[node] = label;\n","        g->labelsCardinalities[label]++;\n","        g->labelToNodes[label][g->labelsCardinalities[label] - 1] = node;\n","    }\n","}\n","\n","void addEdge(Graph* g, int src, int target) {\n","    g->matrix[src * g->numVertices + target] = 1;\n","    g->matrix[target * g->numVertices + src] = 1;\n","    g->degrees[src]++;\n","    g->degrees[target]++;\n","}\n","\n","Graph* createGraph() {\n","    Graph* g = (Graph*)malloc(sizeof(Graph));\n","\n","    if (g == NULL) {\n","        printf(\"Error allocating memory in createGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    g->matrix = NULL;\n","    g->numVertices = 0;\n","    g->nodesToLabel = NULL;\n","    g->labelsCardinalities = NULL;\n","    g->degrees = NULL;\n","    g->labelToNodes = NULL;\n","    return g;\n","}\n","\n","Graph* readGraph(char* path) {\n","    int src, target, srcLabel, targetLabel;\n","    Graph* g = createGraph();\n","\n","    FILE* f = fopen(path, \"r\");\n","    if (f == NULL) {\n","        printf(\"Error opening file\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    char line[128];\n","    fgets(line, sizeof(line), f);\n","    sscanf(line, \"%*s%*s%*s%d\", &g->numVertices);\n","    fgets(line, sizeof(line), f); // skip the header\n","\n","    initGraph(g);\n","\n","    while (fgets(line, sizeof(line), f)) {\n","        sscanf(line, \"%d,%d,%d,%d\", &src, &target, &srcLabel, &targetLabel);\n","        addEdge(g, src, target);\n","        setLabel(g, src, srcLabel);\n","        setLabel(g, target, targetLabel);\n","    }\n","\n","    fclose(f);\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        g->labelToNodes[label] = (int*)realloc(g->labelToNodes[label], g->labelsCardinalities[label] * sizeof(int));\n","    }\n","\n","    return g;\n","}\n","\n","void printGraph(Graph* g) {\n","    for (int i = 0; i < g->numVertices; i++) {\n","        for (int j = 0; j < g->numVertices; j++) {\n","            printf(\"%d \", g->matrix[i * g->numVertices + j]);\n","        }\n","        printf(\"\\tVertex %d, label %d, degree %d\\n\", i, g->nodesToLabel[i], g->degrees[i]);\n","    }\n","\n","    printf(\"\\nCardinalities\\n\");\n","    for (int i = 0; i < LABELS; i++) {\n","        printf(\"Label %d: %d\\n\", i, g->labelsCardinalities[i]);\n","    }\n","\n","    for(int i = 0; i < LABELS; i++) {\n","       printf(\"\\nLabel %d\\n\", i);\n","       for(int j = 0; j < g->labelsCardinalities[i]; j++) {\n","           printf(\"%d \", g->labelToNodes[i][j]);\n","       }\n","    }\n","}\n","\n","void freeGraph(Graph* g) {\n","    for(int i = 0; i < LABELS; i++) {\n","        free(g->labelToNodes[i]);\n","    }\n","    free(g->labelToNodes);\n","    free(g->matrix);\n","    free(g->nodesToLabel);\n","    free(g->labelsCardinalities);\n","    free(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","/***** STATE FUNCTIONS *****/\n","State* createState(Graph* g1, Graph* g2) {\n","    State* s = (State*)malloc(sizeof(State));\n","\n","    s->mapping1 = (int*)malloc(g1->numVertices * sizeof(int));\n","    s->mapping2 = (int*)malloc(g2->numVertices * sizeof(int));\n","    s->T1 = (int*)malloc(g1->numVertices * sizeof(int));\n","    s->T2 = (int*)malloc(g2->numVertices * sizeof(int));\n","    s->T1_out = (int*)malloc(g1->numVertices * sizeof(int));\n","    s->T2_out = (int*)malloc(g2->numVertices * sizeof(int));\n","\n","    if (s == NULL || s->mapping1 == NULL || s->mapping2 == NULL || s->T1 == NULL || s->T2 == NULL || s->T1_out == NULL || s->T2_out == NULL) {\n","        printf(\"Error allocating memory in createState\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for (int i = 0; i < g1->numVertices; i++) {\n","        s->mapping1[i] = -1;\n","        s->T1[i] = -1;\n","        s->T1_out[i] = 1;\n","\n","        s->mapping2[i] = -1;\n","        s->T2[i] = -1;\n","        s->T2_out[i] = 1;\n","    }\n","\n","    return s;\n","}\n","\n","void freeState(State* s) {\n","    free(s->mapping1);\n","    free(s->mapping2);\n","    free(s->T1);\n","    free(s->T2);\n","    free(s->T1_out);\n","    free(s->T2_out);\n","    free(s);\n","    s = NULL;\n","}\n","\n","void printState(State* s, int numVertices) {\n","    printf(\"Mapping 1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping1[i]);\n","    }\n","\n","    printf(\"\\nMapping 2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping2[i]);\n","    }\n","\n","    printf(\"\\nT1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1[i]);\n","    }\n","\n","    printf(\"\\nT2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2[i]);\n","    }\n","\n","    printf(\"\\nT1_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1_out[i]);\n","    }\n","\n","    printf(\"\\nT2_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2_out[i]);\n","    }\n","}\n","\n","bool isMappingFull(Graph* g, State* state) {\n","    for(int i = 0; i < g->numVertices; i++) {\n","        if(state->mapping1[i] == -1) {\n","            return false;\n","        }\n","    }\n","    return true;\n","}\n","\n","__global__ void updateStateKernel(int* matrix, int V, int* mapping, int* T, int* T_out, int node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( idx < V) {\n","        if(matrix[node * V + idx] == 1 && mapping[idx] == -1) {\n","            T[idx] = 1;\n","            T_out[idx] = -1;\n","        }\n","    }\n","\n","}\n","\n","void updateState(Graph* g1, Graph* g2, State* state, int node, int candidate) {\n","    int* d_matrix1, *d_matrix2;\n","    int* d_mapping1, *d_mapping2;\n","    int* d_T1, *d_T1_out;\n","    int* d_T2, *d_T2_out;\n","\n","    cudaStream_t stream1, stream2;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix1, g1->numVertices * g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_mapping1, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T1, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T1_out, g1->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix1, g1->matrix, g1->numVertices * g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_mapping1, state->mapping1, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T1, state->T1, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T1_out, state->T1_out, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int blockSize = 256;\n","    int gridSize = (g1->numVertices + blockSize - 1) / blockSize;\n","\n","    updateStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_matrix1, g1->numVertices, d_mapping1, d_T1, d_T1_out, node);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix2, g2->numVertices * g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_mapping2, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2_out, g2->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix2, g2->matrix, g2->numVertices * g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_mapping2, state->mapping2, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2, state->T2, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2_out, state->T2_out, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    gridSize = (g2->numVertices + blockSize - 1) / blockSize;\n","\n","    updateStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_matrix2, g2->numVertices, d_mapping2, d_T2, d_T2_out, candidate);\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T1, d_T1, g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T1_out, d_T1_out, g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream1);\n","    cudaFree(d_matrix1);\n","    cudaFree(d_mapping1);\n","    cudaFree(d_T1);\n","    cudaFree(d_T1_out);\n","\n","    state->T1[node] = -1;\n","    state->T1_out[node] = -1;\n","\n","    cudaStreamSynchronize(stream2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T2, d_T2, g2->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T2_out, d_T2_out, g2->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream2);\n","    cudaFree(d_matrix2);\n","    cudaFree(d_mapping2);\n","    cudaFree(d_T2);\n","    cudaFree(d_T2_out);\n","\n","    state->T2[candidate] = -1;\n","    state->T2_out[candidate] = -1;\n","}\n","\n","__global__ void restoreStateKernel(int* matrix, int V, int node, int* T, int* T_out, int* mapping) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx >= V) {\n","        return;\n","    }\n","\n","    int isAdded = 0;\n","\n","    if(matrix[node * V + idx] == 1) {\n","\n","        if(mapping[idx] != -1) {\n","            atomicExch(&T[node], 1);\n","            isAdded = 1;\n","        }\n","        else {\n","            int hasCoveredNeighbor = 0;\n","            for(int adjVertex2 = 0; adjVertex2 < V; adjVertex2++) {\n","                if(matrix[idx * V + adjVertex2] == 1 && mapping[adjVertex2] != -1) {\n","                    hasCoveredNeighbor = 1;\n","                    break;\n","                }\n","            }\n","\n","            if(hasCoveredNeighbor == 0) {\n","                T[idx] = -1;\n","                T_out[idx] = 1;\n","            }\n","        }\n","    }\n","\n","    if(isAdded == 0) {\n","        atomicExch(&T_out[node], 1);\n","    }\n","}\n","\n","void restoreState(Graph* g1, Graph* g2, State* state, int node, int candidate) {\n","    int* d_matrix1, *d_matrix2;\n","    int* d_mapping1, *d_mapping2;\n","    int* d_T1, *d_T1_out;\n","    int* d_T2, *d_T2_out;\n","\n","    cudaStream_t stream1, stream2;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix1, g1->numVertices * g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_mapping1, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T1, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T1_out, g1->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix1, g1->matrix, g1->numVertices * g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_mapping1, state->mapping1, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T1, state->T1, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T1_out, state->T1_out, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int blockSize = 256;\n","    int gridSize = (g1->numVertices + blockSize - 1) / blockSize;\n","\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_matrix1, g1->numVertices, node, d_T1, d_T1_out, d_mapping1);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix2, g2->numVertices * g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_mapping2, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2_out, g2->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix2, g2->matrix, g2->numVertices * g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_mapping2, state->mapping2, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2, state->T2, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2_out, state->T2_out, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    gridSize = (g2->numVertices + blockSize - 1) / blockSize;\n","\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_matrix2, g2->numVertices, candidate, d_T2, d_T2_out, d_mapping2);\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T1, d_T1, g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T1_out, d_T1_out, g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream1);\n","    cudaFree(d_matrix1);\n","    cudaFree(d_mapping1);\n","    cudaFree(d_T1);\n","    cudaFree(d_T1_out);\n","\n","    cudaStreamSynchronize(stream2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T2, d_T2, g2->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(state->T2_out, d_T2_out, g2->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream2);\n","    cudaFree(d_matrix2);\n","    cudaFree(d_mapping2);\n","    cudaFree(d_T2);\n","    cudaFree(d_T2_out);\n","}\n","\n","/***** VF2++ FUNCTIONS *****/\n","bool checkGraphProperties(Graph* g1, Graph* g2) {\n","    if (g1->numVertices != g2->numVertices || g1->numVertices == 0 || g2->numVertices == 0) {\n","        return false;\n","    }\n","\n","    if (!checkSequenceDegree(g1->degrees, g2->degrees, g1->numVertices)) {\n","        return false;\n","    }\n","\n","    for(int i = 0; i < LABELS; i++) {\n","        if (g1->labelsCardinalities[i] != g2->labelsCardinalities[i]) {\n","            return false;\n","        }\n","    }\n","\n","    return true;\n","}\n","\n","bool checkSequenceDegree(int* degree1, int* degree2, int size) {\n","    int* tmp1 = (int*)malloc(size * sizeof(int));\n","    int* tmp2 = (int*)malloc(size * sizeof(int));\n","\n","    memcpy(tmp1, degree1, size * sizeof(int));\n","    memcpy(tmp2, degree2, size * sizeof(int));\n","\n","    qsort(tmp1, size, sizeof(int), compare);\n","    qsort(tmp2, size, sizeof(int), compare);\n","   \n","    bool ret = true;\n","    for (int i = 0; i < size; i++) {\n","        if (tmp1[i] != tmp2[i]) {\n","            ret = false;\n","            break;\n","        }\n","    }\n","\n","    free(tmp1);\n","    free(tmp2);\n","    return ret;\n","}\n","\n","int compare(const void* a, const void* b) {\n","    return (*(int*)a - *(int*)b);\n","}\n","\n","__global__ void bfsKernel(int* matrix, int V, int* levels, int* d_done, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    __shared__ int s_done;\n","    extern __shared__ int s_levels[];\n","\n","    if(threadIdx.x == 0) {\n","        s_done = 0;\n","    }\n","\n","    // it copies the whole levels array in shared memory\n","    // example: thread 0 loads levels[0] and levels[4] if blockSize is 4\n","    for(int i = threadIdx.x; i < V; i += blockDim.x) {\n","      s_levels[i] = levels[i];\n","    }\n","\n","    __syncthreads();\n","\n","    // levels is used as visited too\n","    if(idx < V && s_levels[idx] == depth) {    // it blocks all thread with size greater than V and all threads not at the current depth\n","\n","        for(int adjVertex = 0; adjVertex < V; adjVertex++) {\n","            if(matrix[idx * V + adjVertex] == 1 && s_levels[adjVertex] == -1) {\n","                atomicExch(&levels[adjVertex], depth + 1);\n","                s_done = 1;\n","            }\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {\n","        atomicExch(d_done, s_done);\n","    }\n","}\n","\n","__global__ void maxRarityKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_labelRarity[d_nodesToLabel[idx]];\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityFilterKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_labelRarity[d_nodesToLabel[idx]] != *d_maxRarity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void maxDegreeKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_degrees[idx];\n","    }\n","    else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxDegree, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxDegreeFilterKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_degrees[idx] != *d_maxDegree) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void findNodeKernel(int V, int* is_good, int* d_node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if(idx < V && is_good[idx]) {\n","        atomicExch(d_node, idx);\n","    }\n","}\n","\n","__global__ void initArrayKernel(int* d_array, int size, int value) {\n","    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","    if (idx < size) {\n","        d_array[idx] = value;\n","    }\n","}\n","\n","int findRoot(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_degrees) {\n","    int* d_root, *d_is_good, *d_maxRarity, *d_maxDegree;\n","    int h_root = -1, h_maxRarity = INF, h_maxDegree = -INF;\n","\n","    int blockSize = 256;\n","    int gridSize = (V + blockSize - 1) / blockSize;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_root, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_is_good, V * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxDegree, sizeof(int)));\n","\n","    initArrayKernel<<<gridSize, blockSize>>>(d_is_good, V, 1);\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_maxRarity, &h_maxRarity, sizeof(int), cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_maxDegree, &h_maxDegree, sizeof(int), cudaMemcpyHostToDevice));\n","    cudaDeviceSynchronize();\n","\n","    size_t sharedMemSize = blockSize * sizeof(int); // each block has a shared memory of size blockSize\n","\n","    maxRarityKernel<<<gridSize, blockSize, sharedMemSize>>>(V, d_labelRarity, d_nodesToLabel, d_maxRarity, d_is_good);\n","    cudaDeviceSynchronize();\n","    maxRarityFilterKernel<<<gridSize, blockSize>>>(V, d_labelRarity, d_nodesToLabel, d_maxRarity, d_is_good);\n","    cudaDeviceSynchronize();\n","    maxDegreeKernel<<<gridSize, blockSize, sharedMemSize>>>(V, d_degrees, d_maxDegree, d_is_good);\n","    cudaDeviceSynchronize();\n","    maxDegreeFilterKernel<<<gridSize, blockSize>>>(V, d_degrees, d_maxDegree, d_is_good);\n","    cudaDeviceSynchronize();\n","    findNodeKernel<<<gridSize, blockSize>>>(V, d_is_good, d_root);\n","    cudaDeviceSynchronize();\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(&h_root, d_root, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaFree(d_maxDegree);\n","    cudaFree(d_is_good);\n","    cudaFree(d_maxRarity);\n","    cudaFree(d_root);\n","    return h_root;\n","}\n","\n","__global__ void findLevelNodesKernel(int* levels, int depth, int* levelNodes, int V, int* levelSize) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    __shared__ int s_levelSize;\n","\n","    if(threadIdx.x == 0) {\n","        s_levelSize = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < V && levels[idx] == depth) {\n","        atomicAdd(&s_levelSize, 1);\n","        levelNodes[idx] = 1;\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {\n","        atomicAdd(levelSize, s_levelSize);\n","    }\n","}\n","\n","int* ordering(Graph* g1, Graph* g2) {\n","    int* d_g1_matrix, *d_g1_nodesToLabel, *d_g1_degrees, *d_V1Unordered, *d_labelRarity, *d_connectivityG1;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g1_matrix, g1->numVertices * g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g1_nodesToLabel, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g1_degrees, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_V1Unordered, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelRarity, LABELS * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_connectivityG1, g1->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g1_matrix, g1->matrix, g1->numVertices * g1->numVertices * sizeof(int), cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g1_nodesToLabel, g1->nodesToLabel, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g1_degrees, g1->degrees, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_labelRarity, g1->labelsCardinalities, LABELS * sizeof(int), cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemset(d_V1Unordered, -1, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemset(d_connectivityG1, 0, g1->numVertices * sizeof(int)));\n","\n","    int root = findRoot(g1->numVertices, d_labelRarity, d_g1_nodesToLabel, d_g1_degrees);\n","\n","    int* levels = (int*)malloc(g1->numVertices * sizeof(int));\n","    int* order = (int*)malloc(g1->numVertices * sizeof(int));   // order of the nodes of g1\n","    int order_index = 0;\n","\n","    if (order == NULL || levels == NULL) {\n","        printf(\"Error allocating memory in ordering\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    for(int i = 0; i < g1->numVertices; i++) {\n","        levels[i] = -1;\n","    }\n","\n","    int* d_levels, *d_done, *d_levelNodes, *d_levelSize;\n","    int depth = 0;\n","    int h_done;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levels, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_done, sizeof(int)));\n","\n","    levels[root] = depth;\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_levels, levels, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice));\n","\n","    int blockSize = 256;\n","    int gridSize = (g1->numVertices + blockSize - 1) / blockSize;\n","    size_t sharedMemSize = sizeof(int) * g1->numVertices;\n","\n","    do {\n","        h_done = 0;\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_done, &h_done, sizeof(int), cudaMemcpyHostToDevice));\n","\n","        bfsKernel<<<gridSize, blockSize, sharedMemSize>>>(d_g1_matrix, g1->numVertices, d_levels, d_done, depth);\n","        cudaDeviceSynchronize();\n","\n","        CUDA_CHECK_ERROR(cudaMemcpy(&h_done, d_done, sizeof(int), cudaMemcpyDeviceToHost));\n","        depth++;\n","    } while(h_done);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(levels, d_levels, g1->numVertices*sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelNodes, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelSize, sizeof(int)));\n","\n","    gridSize = (g1->numVertices + blockSize - 1) / blockSize;\n","    for (int d = 0; d < depth; d++) {\n","        CUDA_CHECK_ERROR(cudaMemset(d_levelNodes, 0, g1->numVertices * sizeof(int)));\n","        CUDA_CHECK_ERROR(cudaMemset(d_levelSize, 0, sizeof(int)));\n","\n","        findLevelNodesKernel<<<gridSize, blockSize, 0>>>(d_levels, d, d_levelNodes, g1->numVertices, d_levelSize);\n","        cudaDeviceSynchronize();\n","        processDepth(order, &order_index, d_connectivityG1, d_labelRarity, d_V1Unordered, d_levelNodes,\n","                    g1->numVertices, d_g1_degrees, d_g1_nodesToLabel, d_g1_matrix, d_levelSize);\n","    }\n","\n","    free(levels);\n","\n","    cudaFree(d_g1_matrix);\n","    cudaFree(d_g1_nodesToLabel);\n","    cudaFree(d_g1_degrees);\n","    cudaFree(d_V1Unordered);\n","    cudaFree(d_labelRarity);\n","    cudaFree(d_connectivityG1);\n","    cudaFree(d_levels);\n","    cudaFree(d_done);\n","    cudaFree(d_levelNodes);\n","    cudaFree(d_levelSize);\n","\n","    return order;\n","}\n","\n","__global__ void maxConnectivityKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) { // d_is_good already contains the information about the nodes of the current level\n","        int conn = d_connectivityG1[idx];\n","        s_data[threadIdx.x] = conn;\n","    } else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxConnectivity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxConnectivityFilterKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_connectivityG1[idx] != *d_maxConnectivity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void unorderedFilterKernel(int* d_is_good, int* d_V1Unordered, int* d_levelNodes, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= V) {\n","        return;\n","    }\n","\n","    if(d_levelNodes[idx]) {\n","        if(d_V1Unordered[idx] == 1) {\n","            d_is_good[idx] = 0;\n","        }\n","    } else {\n","        d_is_good[idx] = 0;\n","    }\n","}\n","\n","__global__ void updateConnKernel(int* matrix, int V, int* connectivity, int node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(matrix[node * V + idx] == 1) {\n","            atomicAdd(&connectivity[idx], 1);\n","        }\n","    }\n","}\n","\n","void processDepth(int* order, int* order_index, int* d_connectivityG1, int* d_labelRarity, int* d_V1Unordered, int* d_levelNodes, int V,\n","                    int* d_degrees, int* d_nodesToLabel, int* d_matrix, int* d_levelSize) {\n","\n","    int* d_is_good, *d_maxDegree, *d_maxRarity, *d_maxConnectivity, *d_nextNode;\n","    int h_nextNode, h_levelSize, h_maxRarity = INF, h_maxConnectivity = -INF, h_maxDegree = -INF;\n","    int* h_labelRarity = (int*)malloc(LABELS * sizeof(int));\n","    int* h_nodesToLabel = (int*)malloc(V * sizeof(int));\n","    int *h_V1Unordered = (int*)malloc(V * sizeof(int));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(&h_levelSize, d_levelSize, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_is_good, V * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxDegree, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxConnectivity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nextNode, sizeof(int)));\n","\n","    int blockSize = 256;\n","    int gridSize = (V + blockSize - 1) / blockSize;\n","    size_t sharedMemSize = blockSize * sizeof(int);\n","\n","    cudaStream_t stream;\n","    cudaStreamCreate(&stream);\n","\n","    while(h_levelSize > 0) {\n","        initArrayKernel<<<gridSize, blockSize>>>(d_is_good, V, 1);\n","        cudaDeviceSynchronize();\n","\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_maxRarity, &h_maxRarity, sizeof(int), cudaMemcpyHostToDevice));\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_maxDegree, &h_maxDegree, sizeof(int), cudaMemcpyHostToDevice));\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_maxConnectivity, &h_maxConnectivity, sizeof(int), cudaMemcpyHostToDevice));\n","\n","        unorderedFilterKernel<<<gridSize, blockSize>>>(d_is_good, d_V1Unordered, d_levelNodes, V);\n","        cudaDeviceSynchronize();\n","        maxConnectivityKernel<<<gridSize, blockSize, sharedMemSize>>>(d_connectivityG1, d_maxConnectivity, d_is_good, V);\n","        cudaDeviceSynchronize();\n","        maxConnectivityFilterKernel<<<gridSize, blockSize>>>(d_connectivityG1, d_maxConnectivity, d_is_good, V);\n","        cudaDeviceSynchronize();\n","        maxDegreeKernel<<<gridSize, blockSize, sharedMemSize>>>(V, d_degrees, d_maxDegree, d_is_good);\n","        cudaDeviceSynchronize();\n","        maxDegreeFilterKernel<<<gridSize, blockSize>>>(V, d_degrees, d_maxDegree, d_is_good);\n","        cudaDeviceSynchronize();\n","        maxRarityKernel<<<gridSize, blockSize, sharedMemSize>>>(V, d_labelRarity, d_nodesToLabel, d_maxRarity, d_is_good);\n","        cudaDeviceSynchronize();\n","        maxRarityFilterKernel<<<gridSize, blockSize>>>(V, d_labelRarity, d_nodesToLabel, d_maxRarity, d_is_good);\n","        cudaDeviceSynchronize();\n","        findNodeKernel<<<gridSize, blockSize>>>(V, d_is_good, d_nextNode);\n","        cudaDeviceSynchronize();\n","\n","        CUDA_CHECK_ERROR(cudaMemcpy(&h_nextNode, d_nextNode, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","        updateConnKernel<<<gridSize, blockSize, 0, stream>>>(d_matrix, V, d_connectivityG1, h_nextNode);\n","\n","        order[(*order_index)++] = h_nextNode;\n","        h_levelSize--;\n","\n","        CUDA_CHECK_ERROR(cudaMemcpy(h_labelRarity, d_labelRarity, LABELS * sizeof(int), cudaMemcpyDeviceToHost));\n","        CUDA_CHECK_ERROR(cudaMemcpy(h_nodesToLabel, d_nodesToLabel, V * sizeof(int), cudaMemcpyDeviceToHost));\n","        CUDA_CHECK_ERROR(cudaMemcpy(h_V1Unordered, d_V1Unordered, V * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","        h_labelRarity[h_nodesToLabel[h_nextNode]]--;\n","        h_V1Unordered[h_nextNode] = 1;\n","\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_labelRarity, h_labelRarity, LABELS * sizeof(int), cudaMemcpyHostToDevice));\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_nodesToLabel, h_nodesToLabel, V * sizeof(int), cudaMemcpyHostToDevice));\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_V1Unordered, h_V1Unordered, V * sizeof(int), cudaMemcpyHostToDevice));\n","\n","        cudaStreamSynchronize(stream);\n","    }\n","\n","    cudaStreamDestroy(stream);\n","    cudaFree(d_maxDegree);\n","    cudaFree(d_maxRarity);\n","    cudaFree(d_maxConnectivity);\n","    cudaFree(d_nextNode);\n","    cudaFree(d_is_good);\n","}\n","\n","__global__ void findCoveredNeighborsKernel(int* matrix1, int* mapping1, int node, int* coveredNeighbors, int* coveredNeighborsSize,\n","                                            int numVertices) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= numVertices)\n","        return;\n","\n","    if(matrix1[node * numVertices + idx] == 1 && mapping1[idx] != -1) {\n","        int index = atomicAdd(coveredNeighborsSize, 1);\n","        coveredNeighbors[index] = idx;\n","    }\n","}\n","\n","__global__ void findCandidatesKernel(int* coveredNeighborsSize, int g1_label, int maxSizeCandidates,\n","                                    int* g2_vertexList, int g1_degree, int* g2_degrees, int* T2_out, int* mapping2, int* candidates, int* candidateSize,\n","                                    int g2_numVertices, int* commonNodes, int* coveredNeighbors, int* g2_matrix, int* mapping1, int* g2_nodesToLabel) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(*coveredNeighborsSize == 0) {\n","\n","        if(idx < maxSizeCandidates) {\n","            int vertex = g2_vertexList[idx];  // g2_labelToNodes[label]\n","            if(g2_degrees[vertex] == g1_degree && T2_out[vertex] == 1 && mapping2[vertex] != 1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = vertex;\n","            }\n","        }\n","    }\n","    else {\n","\n","        if(idx < g2_numVertices) {\n","           commonNodes[idx] = 1;\n","\n","            for (int i = 0; i < *coveredNeighborsSize; i++) {\n","                int nbrG1 = coveredNeighbors[i];\n","                int mappedG2 = mapping1[nbrG1];\n","                if (g2_matrix[mappedG2 * g2_numVertices + idx] == 0) {\n","                    commonNodes[idx] = 0;\n","                }\n","            }\n","\n","            if (commonNodes[idx] && mapping2[idx] == 1) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_degrees[idx] != g1_degree) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_nodesToLabel[idx] != g1_label) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] == 1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = idx;\n","            }\n","        }\n","    }\n","}\n","\n","int* findCandidates(Graph* g1, Graph* g2, State* state, int node, int* sizeCandidates) {\n","    // output\n","    int* d_coveredNeighbors, *d_coveredNeighborsSize;\n","    int coveredNeighborsSize = 0;\n","\n","    // input\n","    int* d_matrix1, *d_mapping1;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighbors, g1->degrees[node] * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighborsSize, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix1, g1->numVertices * g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_mapping1, g1->numVertices * sizeof(int)));\n","\n","    cudaStream_t stream1, stream2;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix1, g1->matrix, g1->numVertices * g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_coveredNeighborsSize, &coveredNeighborsSize, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_mapping1, state->mapping1, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int blockSize = 256;\n","    int gridSize = (g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findCoveredNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_matrix1, d_mapping1, node, d_coveredNeighbors, d_coveredNeighborsSize,\n","        g1->numVertices);\n","\n","    int g1_label = g1->nodesToLabel[node];\n","    int maxSizeCandidates = g2->labelsCardinalities[g1_label];\n","    int g1_degree = g1->degrees[node];\n","\n","    int* d_g2_vertexList, *d_g2_degrees, *d_g2_matrix, *d_g2_nodesToLabel;\n","    int* d_T2_out, *d_mapping2;\n","    int* d_candidates, *d_candidateSize, *d_commonNodes;\n","    int* candidates;\n","    int candidateSize = 0;\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g2_vertexList, maxSizeCandidates * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g2_degrees, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2_out, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_mapping2, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidates, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidateSize, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_commonNodes, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g2_matrix, g2->numVertices * g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g2_nodesToLabel, g2->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_g2_vertexList, g2->labelToNodes[g1_label], maxSizeCandidates * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_g2_degrees, g2->degrees, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2_out, state->T2_out, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_mapping2, state->mapping2, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_candidateSize, &candidateSize, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_g2_matrix, g2->matrix, g2->numVertices * g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_g2_nodesToLabel, g2->nodesToLabel, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    blockSize = 256;\n","    gridSize = (g2->numVertices + blockSize - 1) / blockSize;\n","    findCandidatesKernel<<<gridSize, blockSize, 0, stream2>>>(d_coveredNeighborsSize, g1_label, maxSizeCandidates, d_g2_vertexList, g1_degree,\n","        d_g2_degrees, d_T2_out, d_mapping2, d_candidates, d_candidateSize, g2->numVertices, d_commonNodes, d_coveredNeighbors,\n","        d_g2_matrix, d_mapping1, d_g2_nodesToLabel);\n","\n","    cudaStreamSynchronize(stream2);\n","\n","    candidates = (int*)malloc(maxSizeCandidates * sizeof(int));\n","    CUDA_CHECK_ERROR(cudaMemcpy(candidates, d_candidates, maxSizeCandidates * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(&candidateSize, d_candidateSize, sizeof(int), cudaMemcpyDeviceToHost));\n","    *sizeCandidates = candidateSize;\n","\n","    cudaStreamDestroy(stream1);\n","    cudaStreamDestroy(stream2);\n","    cudaFree(d_coveredNeighbors);\n","    cudaFree(d_coveredNeighborsSize);\n","    cudaFree(d_matrix1);\n","    cudaFree(d_mapping1);\n","\n","    cudaFree(d_g2_vertexList);\n","    cudaFree(d_g2_degrees);\n","    cudaFree(d_T2_out);\n","    cudaFree(d_mapping2);\n","    cudaFree(d_candidates);\n","    cudaFree(d_candidateSize);\n","    cudaFree(d_commonNodes);\n","    cudaFree(d_g2_matrix);\n","    cudaFree(d_g2_nodesToLabel);\n","\n","    return candidates;\n","}\n","\n","\n","void vf2pp(Graph* g1, Graph* g2, State* state) {\n","    if (!checkGraphProperties(g1, g2)) {\n","        return;\n","    }\n","\n","    int* order = ordering(g1, g2);\n","\n","    //printf(\"Order:\\t\");\n","    //for(int i = 0; i < g1->numVertices; i++) {\n","    //    printf(\"%d \", order[i]);\n","    //}\n","    //printf(\"\\n\");\n","\n","    int sizeCandidates = 0;\n","    int* candidates = findCandidates(g1, g2, state, order[0], &sizeCandidates);\n","\n","    StackNode* stack = createStack();\n","    push(&stack, candidates, sizeCandidates, order[0]);\n","\n","    int matchingNode = 1;\n","    while (!isStackEmpty(stack)) {\n","        Info* info = peek(&stack);\n","        bool isMatch = false;\n","\n","        // printInfo(info);\n","\n","        for(int i = info->candidateIndex; i < info->sizeCandidates; i++) {\n","            int candidate = info->candidates[i];\n","            info->candidateIndex = i + 1;\n","\n","            int ret = cutISO(g1, g2, state, info->vertex, candidate);\n","\n","            // printf(\"CutISO: %d\\n\", ret);\n","            // printf(\"\\n\");\n","\n","            if(!ret) {\n","\n","                // printf(\"\\nMatch %d -> %d\\n\", info->vertex, candidate);\n","\n","                state->mapping1[info->vertex] = candidate;\n","                state->mapping2[candidate] = info->vertex;\n","\n","                if(isMappingFull(g1, state)) {\n","                    freeStack(stack);\n","                    free(order);\n","                    printf(\"Graphs are isomorphic\\n\");\n","                    return;\n","                }\n","\n","                updateState(g1, g2, state, info->vertex, candidate);\n","                candidates = findCandidates(g1, g2, state, order[matchingNode], &sizeCandidates);\n","                push(&stack, candidates, sizeCandidates, order[matchingNode]);\n","                matchingNode++;\n","                isMatch = true;\n","                break;\n","            }\n","        }\n","\n","        // no more candidates\n","        if(!isMatch) {\n","            Info* tmp = pop(&stack);\n","            freeInfo(tmp);\n","            matchingNode--;\n","\n","            // backtracking\n","            if(!isStackEmpty(stack)) {\n","                Info* prevInfo = peek(&stack);\n","                int candidate = state->mapping1[prevInfo->vertex];\n","                state->mapping1[prevInfo->vertex] = -1;\n","                state->mapping2[candidate] = -1;\n","                restoreState(g1, g2, state, prevInfo->vertex, candidate);\n","            }\n","        }\n","    }\n","    free(order);\n","    freeStack(stack);\n","}\n","\n","__global__ void findNeighborsKernel(int* matrix, int node, int* neighbors, int* size, int numVertices) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < numVertices) {\n","        if(matrix[node * numVertices + idx] == 1) {\n","            int index = atomicAdd(size, 1);\n","            neighbors[index] = idx;\n","        }\n","    }\n","}\n","\n","__global__ void checkLabelsKernel(int* neighbors1, int* neighbors2, int* nbrSize1, int* nbrSize2, int* labelsNbr, int numVertices,\n","    int* g1_nodesToLabel, int* g2_nodesToLabel, int* d_result) {   // idx is the id of the thread in the grid\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < *nbrSize1) {\n","        int nbr1 = neighbors1[idx];\n","        int labelNbr1 = g1_nodesToLabel[nbr1];\n","        bool found = false;\n","\n","        for(int i = 0; i < *nbrSize2; i++) {\n","            int nbr2 = neighbors2[i];\n","            if(labelNbr1 == g2_nodesToLabel[nbr2]) {\n","                found = true;\n","                labelsNbr[labelNbr1] = 1;\n","                break;\n","            }\n","        }\n","\n","        if(!found) {\n","            atomicExch(d_result, 0);   // d_result is initialized to 1 by default\n","        }\n","    }\n","}\n","\n","__global__ void findNodesOfLabelKernel(int* neighbors, int* g_nodesToLabel, int label, int maxSize, int* size, int* nodes) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;    \n","\n","    if(idx < maxSize) {\n","        int vertex = neighbors[idx];\n","\n","        if(g_nodesToLabel[vertex] == label) {\n","            int index = atomicAdd(size, 1);   // atomicAdd returns the index respect to global memory (so can't be used shared memory)\n","            nodes[index] = vertex;\n","        }\n","    }\n","}\n","\n","__global__ void intersectionCountKernel(int* nodes, int* size, int* stateSet, int* count) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;    // idx is the id of the thread in the grid among all threads\n","    __shared__ int localCount;  // each block has own localCount variable (shared memory)\n","\n","    if(threadIdx.x == 0) {   // only one thread in the block initializes the localCount\n","        localCount = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < *size) {\n","        int vertex = nodes[idx];\n","\n","        if(stateSet[vertex] == 1) {\n","            atomicAdd(&localCount, 1);\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {  // only the thread with id 0 of each block updates the global size\n","        atomicAdd(count, localCount);\n","    }\n","}\n","\n","bool cutISO(Graph* g1, Graph* g2, State* state, int node1, int node2){\n","    // output\n","    int* d_neighbors1, *d_neighbors2;\n","    int* d_nbrSize1, *d_nbrSize2;\n","    int nbrSize1, nbrSize2;\n","\n","    // input\n","    int* d_matrix1, *d_matrix2;\n","\n","    cudaStream_t stream1, stream2, stream3;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","    cudaStreamCreate(&stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors1, g1->degrees[node1] * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix1, g1->numVertices * g1->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize1, 0, sizeof(int), stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix1, g1->matrix, g1->numVertices * g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int blockSize = 256;\n","    int gridSize = (g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_matrix1, node1, d_neighbors1, d_nbrSize1, g1->numVertices);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors2, g2->degrees[node2] * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix2, g2->numVertices * g2->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize2, 0, sizeof(int), stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_matrix2, g2->matrix, g2->numVertices * g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    gridSize = (g2->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream2>>>(d_matrix2, node2, d_neighbors2, d_nbrSize2, g2->numVertices);\n","\n","    // input\n","    int* d_labelsNbr, *d_g1_nodesToLabel, *d_g2_nodesToLabel;\n","    int* labelsNbr = (int*)malloc(LABELS * sizeof(int));\n","\n","    // output\n","    int* d_result;\n","    int result;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelsNbr, LABELS * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_result, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g1_nodesToLabel, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g2_nodesToLabel, g2->numVertices * sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_labelsNbr, 0, LABELS * sizeof(int), stream3));\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_result, 1, sizeof(int), stream3));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_g1_nodesToLabel, g1->nodesToLabel, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream3));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_g2_nodesToLabel, g2->nodesToLabel, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream3));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(&nbrSize1, d_nbrSize1, sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(&nbrSize2, d_nbrSize2, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaFree(d_matrix1);\n","    cudaFree(d_matrix2);\n","\n","    blockSize = 256;\n","    gridSize = (nbrSize1 + blockSize - 1) / blockSize;\n","\n","    checkLabelsKernel<<<gridSize, blockSize, 0, stream3>>>(d_neighbors1, d_neighbors2, d_nbrSize1, d_nbrSize2,\n","        d_labelsNbr, g1->numVertices, d_g1_nodesToLabel, d_g2_nodesToLabel, d_result);\n","\n","    cudaStreamSynchronize(stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(&result, d_result, sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(labelsNbr, d_labelsNbr, LABELS * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaFree(d_labelsNbr);\n","    cudaFree(d_result);\n","\n","    if(result == 0) {\n","        cudaStreamDestroy(stream1);\n","        cudaStreamDestroy(stream2);\n","        cudaStreamDestroy(stream3);\n","\n","        cudaFree(d_neighbors1);\n","        cudaFree(d_neighbors2);\n","        cudaFree(d_nbrSize1);\n","        cudaFree(d_nbrSize2);\n","        cudaFree(d_g1_nodesToLabel);\n","        cudaFree(d_g2_nodesToLabel);\n","\n","        return true;\n","    }\n","\n","    cudaStream_t stream4, stream5, stream6;\n","    cudaStreamCreate(&stream4);\n","    cudaStreamCreate(&stream5);\n","    cudaStreamCreate(&stream6);\n","\n","    // istantiating variables on device stream1 and stream2\n","    int* d_nodes_g1, *d_nodes_g2;\n","    int* d_size_g1, *d_size_g2;\n","    int size1, size2;\n","\n","    // stream 1\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g1, nbrSize1 * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g1, sizeof(int)));\n","\n","    // stream 2\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g2, nbrSize2 * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g2, sizeof(int)));\n","\n","    //instantiating variables on device stream3, stream4, stream5, stream6\n","    int* d_count3, *d_count4, *d_count5, *d_count6;\n","    int count3, count4, count5, count6;\n","    int* d_T1, *d_T2;\n","    int* d_T1_out, *d_T2_out;\n","\n","    // stream 3\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T1, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T1, state->T1, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream3));\n","\n","    // stream 4\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count4, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2, state->T2, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","    // stream 5\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count5, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T1_out, g1->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T1_out, state->T1_out, g1->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream5));\n","\n","    // stream 6\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count6, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_T2_out, g2->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_T2_out, state->T2_out, g2->numVertices * sizeof(int), cudaMemcpyHostToDevice, stream6));\n","\n","    bool ret = false;\n","    for(int label = 0; label < LABELS; label++) {\n","        if(labelsNbr[label] == 1) {\n","\n","            // stream 1\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g1, 0, sizeof(int), stream1));\n","\n","            gridSize = (nbrSize1 + blockSize - 1) / blockSize;\n","\n","            findNodesOfLabelKernel<<<gridSize, blockSize, 0, stream1>>>(d_neighbors1, d_g1_nodesToLabel, label, nbrSize1, d_size_g1, d_nodes_g1);\n","\n","            // stream 2\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g2, 0, sizeof(int), stream2));\n","\n","            gridSize = (nbrSize2 + blockSize - 1) / blockSize;\n","\n","            findNodesOfLabelKernel<<<gridSize, blockSize, 0, stream2>>>(d_neighbors2, d_g2_nodesToLabel, label, nbrSize2, d_size_g2, d_nodes_g2);\n","\n","            // stream 3\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count3, 0, sizeof(int), stream3));\n","\n","            // stream 4\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count4, 0, sizeof(int), stream4));\n","\n","            // stream 5\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count5, 0, sizeof(int), stream5));\n","\n","            // stream 6\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count6, 0, sizeof(int), stream6));\n","\n","            cudaStreamSynchronize(stream1);\n","            cudaStreamSynchronize(stream2);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpy(&size1, d_size_g1, sizeof(int), cudaMemcpyDeviceToHost));\n","            CUDA_CHECK_ERROR(cudaMemcpy(&size2, d_size_g2, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","            gridSize = (size1 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream3>>>(d_nodes_g1, d_size_g1, d_T1, d_count3);\n","\n","            gridSize = (size2 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream4>>>(d_nodes_g2, d_size_g2, d_T2, d_count4);\n","\n","            gridSize = (size1 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream5>>>(d_nodes_g1, d_size_g1, d_T1_out, d_count5);\n","\n","            gridSize = (size2 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream6>>>(d_nodes_g2, d_size_g2, d_T2_out, d_count6);\n","\n","            cudaStreamSynchronize(stream3);\n","            CUDA_CHECK_ERROR(cudaMemcpy(&count3, d_count3, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","            cudaStreamSynchronize(stream4);\n","            CUDA_CHECK_ERROR(cudaMemcpy(&count4, d_count4, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","            cudaStreamSynchronize(stream5);\n","            CUDA_CHECK_ERROR(cudaMemcpy(&count5, d_count5, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","            cudaStreamSynchronize(stream6);\n","            CUDA_CHECK_ERROR(cudaMemcpy(&count6, d_count6, sizeof(int), cudaMemcpyDeviceToHost));\n","\n","            if(count3 != count4 || count5 != count6) {\n","                ret = true;\n","                break;\n","            }\n","        }\n","    }\n","    cudaStreamDestroy(stream1);\n","    cudaStreamDestroy(stream2);\n","    cudaStreamDestroy(stream3);\n","    cudaStreamDestroy(stream4);\n","    cudaStreamDestroy(stream5);\n","    cudaStreamDestroy(stream6);\n","\n","    cudaFree(d_neighbors1);\n","    cudaFree(d_neighbors2);\n","    cudaFree(d_nbrSize1);\n","    cudaFree(d_nbrSize2);\n","    cudaFree(d_g1_nodesToLabel);\n","    cudaFree(d_g2_nodesToLabel);\n","\n","    cudaFree(d_count3);\n","    cudaFree(d_count4);\n","    cudaFree(d_count5);\n","    cudaFree(d_count6);\n","\n","    cudaFree(d_T1);\n","    cudaFree(d_T2);\n","    cudaFree(d_T1_out);\n","    cudaFree(d_T2_out);\n","\n","    cudaFree(d_nodes_g1);\n","    cudaFree(d_size_g1);\n","    cudaFree(d_nodes_g2);\n","    cudaFree(d_size_g2);\n","\n","    return ret;\n","}\n","\n","/***** STACK FUNCTIONS *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex) {\n","    Info* info = (Info*)malloc(sizeof(Info));\n","    if (info == NULL) {\n","        printf(\"Error allocating memory in createInfo\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    info->vertex = vertex;\n","    info->candidates = candidates;\n","    info->sizeCandidates = sizeCandidates;\n","    info->candidateIndex = 0;\n","    return info;\n","}\n","\n","StackNode* createStackNode(Info* info) {\n","    StackNode* node = (StackNode*)malloc(sizeof(StackNode));\n","    if (node == NULL) {\n","        printf(\"Error allocating memory in createStackNode\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    node->info = info;\n","    node->next = NULL;\n","    return node;\n","}\n","\n","void push(StackNode** top, int* candidates, int sizeCandidates, int vertex) {\n","    Info* info = createInfo(candidates, sizeCandidates, vertex);\n","    StackNode* node = createStackNode(info);\n","    node->next = *top;\n","    *top = node;\n","}\n","\n","Info* pop(StackNode** top) {\n","    if (isStackEmpty(*top)) {\n","        printf(\"Stack is empty, cannot pop\\n\");\n","        return NULL;\n","    }\n","    StackNode* node = *top;\n","    Info* info = node->info;\n","    *top = node->next;\n","    free(node);\n","    return info;\n","}\n","\n","bool isStackEmpty(StackNode* top) {\n","    return top == NULL;\n","}\n","\n","void freeStack(StackNode* top) {\n","    while (!isStackEmpty(top)) {\n","        StackNode* node = top;\n","        top = top->next;\n","        freeInfo(node->info);\n","        free(node);\n","    }\n","}\n","\n","void printStack(StackNode* top) {\n","    StackNode* current = top;\n","    while (current != NULL) {\n","        printInfo(current->info);\n","        current = current->next;\n","    }\n","}\n","\n","void printInfo(Info* info) {\n","    printf(\"\\nVertex: %d\\n\", info->vertex);\n","    printf(\"Index seen: %d\\n\", info->candidateIndex);\n","    printf(\"Candidates: \");\n","    for (int i = 0; i < info->sizeCandidates; i++) {\n","        printf(\"%d \", info->candidates[i]);\n","    }\n","    printf(\"\\n\");\n","}\n","\n","void freeInfo(Info* info) {\n","    free(info->candidates);\n","    free(info);\n","}\n","\n","StackNode* createStack() {\n","    return NULL;\n","}\n","\n","Info* peek(StackNode** top) {\n","    return (*top)->info;\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
