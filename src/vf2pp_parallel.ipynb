{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20468,"status":"ok","timestamp":1721124314887,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"OVKsPm9V3gQL","outputId":"1bbd7558-9177-4f2e-9002-c69d197602e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","os.chdir(\"drive/MyDrive/VF2pp-in-CUDA/\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2282,"status":"ok","timestamp":1721124317165,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"_IbccJNAcjIa","outputId":"e03ccebc-8eaa-47f5-e2a9-6913f5d08344"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm -f src/lib/stack.o src/lib/queue.o src/lib/graph.o src/lib/state.o src/vf2pp_sequential.o vf2pp_sequential\n"]}],"source":["os.listdir()\n","!make clean\n","!make\n","# for i in ['ciao', 'lorenzo']:\n","#   cmd = f'echo {i}'\n","#   !{cmd}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!nvcc -arch=sm_75 src/vf2pp_parallel.cu -o vf2pp_parallel\n","# !ncu --metrics sm__warps_active.avg.pct_of_peak_sustained_active ./vf2pp_parallel\n","\n","!./vf2pp_sequential 3000 3\n","!./vf2pp_parallel 3000 3"]},{"cell_type":"markdown","metadata":{"id":"h498tzxnwDyi"},"source":["CUDA SETUP"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164693,"status":"ok","timestamp":1721124481855,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"rbh20Z5utmFQ","outputId":"562de352-dc05-4bb7-83f2-ddf4d6ccc9f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nvcc4jupyter\n","  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n","Installing collected packages: nvcc4jupyter\n","Successfully installed nvcc4jupyter-1.2.1\n","Collecting pycuda\n","  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytools>=2011.2 (from pycuda)\n","  Downloading pytools-2024.1.8-py3-none-any.whl (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Collecting mako (from pycuda)\n","  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.2 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n","Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n","  Downloading siphash24-1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n","Building wheels for collected packages: pycuda\n","  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661204 sha256=1f652d08366177650a9bd139a22ee3dabd6d1f59492f243542b12620291cb3db\n","  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n","Successfully built pycuda\n","Installing collected packages: appdirs, siphash24, mako, pytools, pycuda\n","Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.8 siphash24-1.6\n"]}],"source":["!pip install nvcc4jupyter\n","!pip install pycuda"]},{"cell_type":"markdown","metadata":{"id":"8_P-CD0hwSmY"},"source":["GPU TYPE"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":424,"status":"ok","timestamp":1721124482267,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"ntofwbc0tEKS","outputId":"1c891bec-b1fb-4b83-d04c-509dfda7f16b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Jul 16 10:08:01 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp0pcg8kke\".\n"]}],"source":["!nvidia-smi\n","!nvcc --version\n","%load_ext nvcc4jupyter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1720436857013,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"KfQ4iKK_p0hY","outputId":"67de1cce-ea1d-4a9f-e6a6-9b17047575aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 device(s) found.\n","Device #0: Tesla T4\n"," Compute Capability: 7.5\n"," Total Memory: 14 GB\n"]}],"source":["import pycuda.driver as drv\n","import pycuda.autoinit\n","drv.init()\n","print(\"%d device(s) found.\" % drv.Device.count())\n","for i in range(drv.Device.count()):\n","  dev = drv.Device(i)\n","  print(\"Device #%d: %s\" % (i, dev.name()))\n","  print(\" Compute Capability: %d.%d\" % dev.compute_capability())\n","  print(\" Total Memory: %s GB\" % (dev.total_memory() // (1024 * 1024 * 1024)))"]},{"cell_type":"markdown","metadata":{"id":"8X72on3Cwj7C"},"source":["GPU INFO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1796,"status":"ok","timestamp":1721043130771,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"u9x2_pq4wwyI","outputId":"2b483e5f-4f73-4759-d15c-4260249dd14d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device number: 0\n","  Device name: Tesla T4\n","  Compute capability: 7.5\n","\n","  Clock Rate: 1590000 kHz\n","  Total SMs: 40 \n","  Shared Memory Per SM: 65536 bytes\n","  Registers Per SM: 65536 32-bit\n","  Max threads per SM: 1024\n","  L2 Cache Size: 4194304 bytes\n","  Total Global Memory: 15835660288 bytes\n","  Memory Clock Rate: 5001000 kHz\n","\n","  Max threads per block: 1024\n","  Max threads in X-dimension of block: 1024\n","  Max threads in Y-dimension of block: 1024\n","  Max threads in Z-dimension of block: 64\n","\n","  Max blocks in X-dimension of grid: 2147483647\n","  Max blocks in Y-dimension of grid: 65535\n","  Max blocks in Z-dimension of grid: 65535\n","\n","  Shared Memory Per Block: 49152 bytes\n","  Registers Per Block: 65536 32-bit\n","  Warp size: 32\n","\n"," Constant Memory: 65536\n","\n"]}],"source":["%%cuda\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","void deviceQuery()\n","{\n","  cudaDeviceProp prop;\n","  int nDevices=0, i;\n","  cudaError_t ierr;\n","\n","  ierr = cudaGetDeviceCount(&nDevices);\n","  if (ierr != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierr)); }\n","\n","\n","\n","  for( i = 0; i < nDevices; ++i )\n","  {\n","     ierr = cudaGetDeviceProperties(&prop, i);\n","     printf(\"Device number: %d\\n\", i);\n","     printf(\"  Device name: %s\\n\", prop.name);\n","     printf(\"  Compute capability: %d.%d\\n\\n\", prop.major, prop.minor);\n","\n","     printf(\"  Clock Rate: %d kHz\\n\", prop.clockRate);\n","     printf(\"  Total SMs: %d \\n\", prop.multiProcessorCount);\n","     printf(\"  Shared Memory Per SM: %lu bytes\\n\", prop.sharedMemPerMultiprocessor);\n","     printf(\"  Registers Per SM: %d 32-bit\\n\", prop.regsPerMultiprocessor);\n","     printf(\"  Max threads per SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n","     printf(\"  L2 Cache Size: %d bytes\\n\", prop.l2CacheSize);\n","     printf(\"  Total Global Memory: %lu bytes\\n\", prop.totalGlobalMem);\n","     printf(\"  Memory Clock Rate: %d kHz\\n\\n\", prop.memoryClockRate);\n","\n","\n","     printf(\"  Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n","     printf(\"  Max threads in X-dimension of block: %d\\n\", prop.maxThreadsDim[0]);\n","     printf(\"  Max threads in Y-dimension of block: %d\\n\", prop.maxThreadsDim[1]);\n","     printf(\"  Max threads in Z-dimension of block: %d\\n\\n\", prop.maxThreadsDim[2]);\n","\n","     printf(\"  Max blocks in X-dimension of grid: %d\\n\", prop.maxGridSize[0]);\n","     printf(\"  Max blocks in Y-dimension of grid: %d\\n\", prop.maxGridSize[1]);\n","     printf(\"  Max blocks in Z-dimension of grid: %d\\n\\n\", prop.maxGridSize[2]);\n","\n","     printf(\"  Warp size: %d\\n\\n\", prop.warpSize);\n","     printf(\" Constant Memory: %d\\n\", prop.totalConstMem);\n","     printf(\" Max resident blocks per SM: %d\\n\", prop.maxBlocksPerMultiProcessor);\n","  }\n","}\n","\n","int main() {\n","    deviceQuery();\n","}"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59674,"status":"ok","timestamp":1721124905952,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"QNGy3BpeqZGs","outputId":"803605dc-7575-4224-b067-15b57052cb37"},"outputs":[{"name":"stdout","output_type":"stream","text":["Order:\t3908 2798 6212 6489 7138 3982 9737 8659 8600 1495 288 9147 8207 3467 1802 1060 3576 5770 4237 5888 1300 8925 1857 4580 653 9204 4099 603 1166 9580 6056 2274 259 4852 9275 367 6306 521 3738 4555 8879 2014 4944 131 756 8360 1655 7970 1694 7654 5631 9407 3497 8626 7310 2303 2623 5377 5399 6584 7785 9719 5456 886 3118 2751 5595 3288 4503 1499 2033 3962 4499 8146 6087 6773 1311 9516 9258 1187 8467 2271 9932 3847 3093 53 2878 6015 6991 989 2305 875 6593 5749 6688 4192 7047 9116 584 6810 3437 2050 1959 724 5265 8097 1370 9481 9974 1774 4350 3236 5083 2522 773 8140 221 5996 8061 9415 2575 5304 617 3196 2710 4628 173 9746 9832 591 679 5369 5217 6596 8006 2482 4975 7400 2563 8957 4276 4482 2853 9468 544 8134 360 3770 8197 871 7381 9220 4832 2314 1587 271 9491 8847 8824 3863 6249 8371 1329 7256 7309 7815 6095 6945 9191 9 6976 5103 9689 9067 6613 6493 4413 4451 4592 2489 8677 3155 5232 6737 1844 1956 2351 465 7819 1124 5165 481 140 451 6202 2278 7042 9425 4453 4684 976 5852 5930 4509 4729 2114 2656 4632 3748 7413 1769 549 6841 3100 2868 2950 2877 1918 4319 2484 3008 2194 9820 1552 4559 3621 5682 3609 6406 7363 4793 3687 3463 9354 8126 1161 9280 3355 2636 7634 3801 1470 8498 9546 1303 594 1930 1159 1240 890 3156 4927 7421 5754 4486 2064 791 4335 2979 9915 8427 9942 4847 797 3530 2192 1751 4991 28 4174 5526 812 2256 945 452 5038 7821 6849 7100 8456 8399 7121 629 2970 9925 1262 3270 5110 3586 418 9616 3308 2921 8209 8714 1674 6164 8162 7474 9531 4343 8916 9690 866 9293 4801 864 6504 9075 3183 4407 6843 6711 1072 1649 7855 2652 3886 5011 7402 9893 6317 8161 283 6183 6345 9228 9404 3255 6622 2800 770 9329 3980 4953 3120 5037 3197 4573 8934 2881 6909 1415 3928 9851 450 8068 7834 334 2300 5117 4090 671 282 1308 5573 1510 5364 7369 9012 5655 8315 5719 4196 7338 301 6025 8203 2638 8928 227 273 3617 9741 3410 8444 7415 3494 9523 4124 6352 8224 9658 2916 5168 3482 3849 1462 775 9739 7727 222 4516 6079 7223 2275 1263 98 8752 2792 7801 3475 1859 4778 3582 454 7345 3077 369 8460 3939 999 4622 7913 5063 3336 994 9006 5007 1258 7514 6326 6512 5799 9800 1868 2658 5357 5227 1412 4190 7268 8939 7653 535 2822 4182 296 7379 9060 8692 2588 2038 9176 4882 5555 5864 8464 568 5362 1921 3902 106 2042 1948 4513 4976 8621 6147 7498 5717 2240 7765 7627 1176 6213 606 6946 930 7827 9732 7860 4253 6844 2802 5092 9730 5324 6775 1441 5848 7006 1195 2092 3763 4286 8573 966 197 6124 9327 2333 7130 719 3684 8428 8609 4929 2203 6589 5831 5356 7534 7109 1219 9511 2284 8232 4918 9297 4121 778 2769 2689 7043 6173 9725 8063 7387 6926 5299 980 7206 6771 9210 2106 3280 8653 270 8666 8864 1259 9603 2122 512 6464 4410 2207 5457 7543 2917 5928 3594 9934 8623 7086 8870 9076 3192 7734 2218 563 7221 9808 4052 7141 9819 9827 5541 2857 2137 3871 596 6052 9633 1819 3659 371 3807 4603 7147 3682 7408 9464 5939 8487 1355 8931 534 6535 7887 9485 7236 8773 8627 8791 3570 1333 7661 3297 8386 5811 598 2473 6931 3310 1530 5860 9848 1039 4339 7091 3634 3267 6228 664 1634 1592 3233 5211 988 3993 4936 5845 5605 1645 164 912 2328 618 6696 9548 914 4278 8142 6217 9195 3084 1787 1229 6374 6176 9261 2935 2153 3352 7883 4658 9031 5079 3834 5530 7563 9874 5868 4634 7769 1170 949 6013 4815 9997 5080 2908 8862 4562 1996 6865 8619 5035 8349 8046 2682 2129 2745 8765 2552 833 2737 7318 1713 4086 4656 5383 5805 3048 2441 4642 8373 7216 5459 7836 277 2126 2454 5871 6266 1345 4612 420 925 4532 9699 7890 9586 6318 5675 3231 8971 2795 3447 1376 2555 9476 6644 1179 1573 5187 9413 7092 9118 7677 5812 817 12 5425 5008 3190 7119 3832 7186 2453 4971 2242 132 6915 4440 7621 2521 8382 5453 7385 4229 1878 2542 8459 4826 847 6262 3223 6120 2096 7458 6325 5339 5424 4289 4614 4653 5226 1947 3442 3071 9727 9286 1512 4388 9736 2733 5091 3251 9818 9781 7074 4696 8863 4477 9372 9461 30 5721 2197 9764 4163 2550 3164 4154 3942 488 647 2899 6846 9705 398 8454 1214 739 5100 1487 1228 7779 1011 5152 4247 6799 7754 5353 9132 5840 4675 9364 3461 4960 1791 119 3638 8407 242 9889 3566 3604 7923 3009 3628 8781 7507 2524 5537 7878 9841 359 7129 302 7097 4497 8015 5898 9802 8968 4911 8597 9515 330 9065 262 4461 2113 6481 2264 5379 6502 6768 1727 8310 8449 7848 4719 6098 586 2155 4209 2039 6879 1811 8241 1115 2199 7115 734 9211 4874 8008 7167 2376 4235 3790 3796 4290 5979 8796 2481 9919 2081 5225 670 5204 672 4798 3672 2183 7591 8548 2276 469 6828 688 8517 5433 6301 1513 3349 7583 2127 840 3381 8737 3188 4984 5088 2502 7302 3050 6400 7685 7764 6574 8273 2931 9760 1907 4188 9033 4557 7470 9985 1682 100 1600 5206 9205 4483 7780 9025 7134 4372 8520 6520 1155 5878 9762 1129 5544 5380 8159 6096 4540 48 5470 4294 1090 3038 5539 403 8132 785 4252 8895 846 2165 8199 8947 1684 2236 6085 885 2426 2855 963 7612 9015 1215 33 1180 6767 2326 9584 4654 8325 6219 7813 5500 9186 3842 9835 9018 9085 9731 6004 3261 442 9664 67 3344 4768 7475 4378 5273 4546 5344 8279 5026 7210 7652 1167 7814 5644 9028 9148 3999 2063 2670 6676 2668 5578 7066 1817 8172 8975 3379 267 7504 6944 3035 2176 8844 986 4137 3561 8805 2080 1346 111 3906 254 641 1953 9290 5557 5830 6474 7411 2993 3004 5577 9837 2001 3245 8257 5810 7077 4752 1418 8421 636 5132 138 320 9324 2739 5522 380 6229 2341 9385 973 2406 9587 5738 5177 7199 2413 4394 6492 4662 8964 9567 6827 4814 6525 4141 2128 4236 5775 1330 6918 3199 5281 5718 1618 318 9207 3356 749 2683 4197 6968 3979 9108 7021 8780 6338 7230 327 6683 9697 3200 2601 7485 9700 3727 431 3597 8885 4679 8313 284 1200 2495 1972 2075 6901 9707 8024 269 4549 6951 7921 7009 206 729 3615 4640 4536 4825 9439 8871 9572 2398 2249 8320 7320 3920 6594 3109 6309 3941 9955 2056 8050 5761 9769 142 6806 2734 1139 7633 5325 5621 4296 798 9214 3026 8119 7016 5184 9294 1015 250 5790 8246 9291 5258 2631 6037 3656 7792 6389 2998 2339 4254 6440 7918 2720 3563 366 7744 2505 7442 1413 3633 2499 2268 3955 8998 4961 2556 9446 4014 1030 3378 3350 867 6277 8950 2287 1616 8249 1822 8049 7527 4861 4875 2070 9553 1248 5478 7479 7936 3150 1301 3702 5592 4036 881 8632 4095 8631 9040 7003 2 8918 8617 4537 6554 3696 6282 7944 7669 2697 5449 6571 1295 8886 9901 6585 186 7271 9465 7324 9365 7313 1516 8491 5180 8592 5959 6207 9906 9558 951 8432 4384 1821 6610 808 3176 6772 2258 8250 4134 7709 2693 3252 1747 631 1663 3304 389 9578 792 7142 1523 6932 4897 7185 2330 4342 5441 2340 3769 646 9929 8073 7585 7179 2743 3635 4176 8019 1062 2424 6642 5597 4157 2289 3710 9902 354 3944 99 1061 3253 4359 351 3061 4956 2628 7406 8270 5141 7568 6066 6733 276 4485 5020 4207 3869 1003 5391 154 9597 8758 4327 5327 6045 1614 5199 3921 8066 108 3507 7452 4985 6223 9518 1007 825 3900 7372 4665 946 5090 9638 6785 4865 1459 4283 520 4738 3736 5160 26 1730 9873 265 7523 695 9013 3296 7697 8814 476 127 6788 1163 5256 8155 6446 2494 1352 170 7597 8500 8296 1623 6720 4199 9875 3571 3977 4393 9394 7305 1884 2820 9777 3893 7981 3643 5999 6026 4672 1910 2237 6324 3076 3924 9081 6905 8530 4201 1863 1471 1013 5667 9652 2302 5172 7950 5447 1775 8830 1546 4071 3449 3173 3313 4677 737 6850 8740 1480 9783 5043 2814 3022 9927 9224 1749 5101 7106 9663 7549 8242 5315 8583 7961 1095 720 3595 3632 3250 3823 4361 4418 4876 5149 7603 6893 4377 6963 2703 7843 518 4502 1532 8054 8974 9977 1594 1906 3284 2799 3070 6528 3121 716 602 1704 9336 6689 8001 7146 4324 292 8164 4663 2529 5024 9283 1322 8742 1721 484 1021 8932 319 1045 4692 2359 90 4858 2557 5300 9409 5837 1780 2787 3541 5135 6185 933 3095 152 2053 7687 8720 2669 7665 9836 8684 8478 835 6075 6789 1559 7828 4902 2983 8724 7651 8333 7991 8429 5647 6151 519 223 2744 4973 9068 2186 1553 9529 5822 685 6361 4206 3728 9563 7044 6181 1538 7683 2681 9799 4443 5431 6527 4371 609 772 7300 4733 9072 3753 8596 8958 6631 5329 659 2327 8204 3464 215 7509 4216 3020 7565 5045 633 6457 8650 3388 7332 788 4082 3211 435 2823 2511 9766 2821 1734 1985 4360 9920 1239 4055 9839 907 744 6804 557 6941 128 7856 9954 9805 6534 8375 6866 82 257 2939 4673 2891 516 3260 786 7914 901 3128 5910 5817 6784 7632 4543 2953 4716 1653 3013 8656 7123 7465 4833 7804 7135 4310 9484 9097 2228 4049 7572 1492 2469 2162 9886 440 540 1964 1928 2500 2968 7075 2085 5650 3778 2434 2346 4185 2599 1425 1889 2519 1808 7616 6040 6511 8778 7557 409 8987 5769 4841 5068 1652 6726 383 687 117 8405 2277 5639 5126 4802 3478 8683 4925 4164 6743 6555 2032 1543 8887 3016 4993 7636 3605 5797 6195 5469 5388 9747 7976 3553 2012 3271 5003 662 3711 436 6370 8538 6500 5143 9667 3376 4981 122 6623 7954 4264 1675 6872 5856 727 9956 3368 86 6783 5150 3662 6127 9620 1386 6011 4795 5701 1473 7299 2036 116 2819 7774 6566 9864 2811 9557 1408 5700 944 4117 3850 203 3513 7909 854 7433 7055 3974 5122 3862 3462 7365 4717 3036 9391 839 501 3360 7449 4660 4749 3729 5745 7983 2903 2135 6793 3505 2600 79 3163 3201 9137 759 89 3699 1547 5411 9011 5422 1924 4627 6226 9450 3811 235 1680 2428 1744 224 2551 9156 8420 3919 7248 5286 104 939 7795 4914 4989 921 9119 3935 5606 477 9543 9510 4416 9359 9062 6427 2533 72 8604 7398 9383 2540 972 9375 4180 5087 8263 264 855 6377 5854 4023 5338 6449 9056 9395 9693 8672 2649 6321 4097 1393 3508 8056 9460 1977 7205 5289 9857 3105 9217 1448 9438 3234 6094 4231 7898 8302 3779 9914 3642 4885 1876 9444 3972 7491 7035 1275 7908 7404 595 8 2144 8435 3742 599 7117 9197 2200 5784 3152 7200 6665 7609 3997 7352 2573 4781 2986 8038 4836 9272 1388 5328 9598 4010 9644 7196 362 6369 8107 1351 8694 3214 3755 4579 1932 4069 8004 2384 8599 4481 5030 4230 3065 4173 298 8013 8826 8712 1562 9559 9773 515 9975 9671 6672 4831 4248 6755 1601 1832 2178 6715 8064 493 1904 9113 3981 7758 9824 644 4009 5055 1123 337 5863 3366 4340 8474 5576 7178 6604 6572 6412 3535 3976 8537 7172 9486 3087 9738 9885 8794 6710 4325 1931 4578 6727 9276 6422 7492 4307 7028 9822 4284 3544 1853 8570 7089 6617 3923 9720 7742 2222 4651 1528 4105 1519 137 8480 3293 9862 8071 3639 5446 3695 1565 3174 9826 462 2804 3419 7051 3764 2109 2612 822 2306 7569 4475 2797 4690 1056 9101 472 6255 3612 4822 5607 9244 9778 8777 5452 503 1404 6838 7902 4755 992 2730 1974 5269 3680 237 1574 2637 4312 7740 200 6384 4492 4195 2708 5089 2337 5189 4997 1536 2596 5505 8702 8275 790 8479 4596 9264 3618 9254 1246 9939 7980 5004 8274 4978 6136 7680 1828 2045 9774 9717 461 3891 1453 9316 8043 5109 1310 6832 5105 5768 7592 7145 4568 5151 3827 8852 3111 504 183 9077 8759 2827 2293 8440 4202 1916 396 8763 1899 1145 7287 3285 837 7787 2815 9660 7281 8210 7977 9029 838 6979 2146 8118 7626 8410 9104 7283 6641 7873 8569 7260 363 2615 7059 3030 2629 3903 4214 9590 3018 6399 1361 5792 1008 6494 5865 8388 3264 5764 1529 5611 3784 5566 6521 8550 2653 581 5418 6068 8264 8260 9443 415 2801 3247 9780 8993 4330 5218 782 2468 8268 3499 7397 5695 7033 2865 4338 2028 348 249 8769 3915 6425 5093 4972 2397 1569 9032 2254 4514 2389 6725 9334 9166 8990 5521 120 2239 9963 7455 6955 8749 4364 191 5305 7517 8708 2120 1488 7233 6476 4884 3773 31 1827 3904 4868 2895 9219 9866 4988 3135 5363 5243 5687 829 1422 8319 6984 2220 1427 1087 8936 3140 9155 9183 3828 3588 715 8094 5967 8884 4129 6998 3666 1557 101 593 4449 612 163 3649 7695 7521 1954 4932 6402 8809 8984 1731 2262 2809 2130 7482 5337 9174 8858 8342 2583 7622 1010 1923 1898 9556 7486 4704 941 2867 4926 924 5940 3055 6930 1648 9300 1662 7782 7227 4425 5346 1389 3775 9030 8719 3097 7148 4613 3521 9960 2520 2584 8836 9145 7763 69 5466 2915 8718 6159 3067 7772 8865 5479 8070 7894 3745 3890 6288 1636 6200 5806 4570 1461 8288 1823 981 2803 6808 5585 7195 5731 3912 1334 6279 5250 2836 2005 4782 1396 1558 5392 7330 5867 5684 4056 1756 1082 8348 2717 4203 2914 3654 3307 5821 6927 3841 6885 764 1456 6817 1957 126 3063 212 5006 1767 6435 5730 2371 3149 7935 8757 2947 1590 5756 4356 3782 9680 9767 6140 4741 2257 6611 2072 1192 4279 9058 4955 6376 5961 4700 6263 5629 4239 2003 8902 8912 858 6016 2251 9626 2083 9545 630 9010 5384 4922 6583 2255 1925 9199 1221 780 1096 3457 8326 7798 8314 7427 3665 2709 2451 5373 3386 7493 3112 6822 5859 6158 896 7284 2774 6380 2776 2527 5395 3814 6545 9834 3357 3479 9252 4251 9233 2882 3395 691 1628 7531 743 3181 6097 2594 3427 3956 5672 3583 9242 4646 7711 701 6766 5065 984 3671 9005 9858 7596 148 226 2375 9000 5617 5506 1524 179 6485 4531 8139 2929 1888 6028 1293 6046 6039 3722 8697 3474 6659 9716 781 7973 8227 916 8710 7905 1000 2138 1970 9938 622 7712 8810 8494 1463 5932 578 4006 7214 8585 5476 2243 7526 9339 6411 4261 6700 6394 6385 7962 651 4269 2408 863 7388 8128 709 639 4263 4484 4204 9474 9675 1830 39 635 6729 6603 6177 434 8965 49 7861 6503 3516 8300 5095 5121 8721 6307 3606 5322 5113 411 4879 1409 8281 3947 3354 2338 2160 1226 5857 9226 1357 8005 1983 94 9358 8816 2459 2201 1961 2991 8523 8704 2272 678 4432 8926 305 1934 9429 7952 1112 2317 5191 2000 5169 1869 6261 7250 9445 7966 1697 8544 6701 2461 2560 9475 9948 7464 9168 8556 7700 9765 1394 5828 9273 1765 9821 5350 640 4438 7988 2768 7739 1582 167 7418 9676 6430 6625 2726 1723 8394 6888 1444 4267 1758 4722 3099 2108 8818 7073 804 9341 3857 9999 9815 9135 9215 2537 2610 6298 5161 9121 9400 1175 5766 8095 9704 8439 7728 5438 9850 2166 2390 2216 2087 2470 7497 2812 3085 4779 4217 8354 5915 4657 177 5332 9900 1842 2449 3045 7478 502 6892 6645 5638 5209 5835 4823 7472 2516 6346 3023 1184 4566 5440 4652 657 8477 3552 7784 9367 2591 5603 6547 2910 2267 1681 9203 6160 5791 9794 5317 5531 5005 5146 2949 5054 2667 5000 9688 7447 565 8754 2752 3529 2154 4552 4872 3894 9122 1588 1172 1533 4013 4737 3056 5248 7871 8914 882 3865 7638 3 1919 7876 9879 2324 6332 2362 1927 258 4863 9303 6179 667 6524 7593 5690 7766 7395 1203 9126 5982 6491 638 5625 4118 4238 4152 3658 9722 6053 967 1865 725 6456 287 1778 4834 1577 204 7672 621 5081 3987 1852 6433 8182 4128 6064 6002 9756 7564 2465 5594 1143 1622 4506 5174 3187 7536 3369 9605 2372 7017 3062 4218 7392 752 5658 5303 6398 4572 934 5683 6780 3102 3145 643 7959 6291 4113 5262 892 3235 4311 7306 4625 6658 8370 4404 4422 2148 7396 8572 5345 1908 2368 487 4948 5659 7276 1762 1855 7949 7194 4810 2767 5482 416 4871 9662 3534 9895 6529 9053 109 7177 3074 157 5178 4001 8135 5318 9695 9249 9890 3789 7032 1017 2510 2666 8688 1816 6117 605 4362 9547 5774 6654 7751 7183 7204 5333 4835 4599 4367 3305 7571 2436 1323 4734 774 8361 6757 1685 6826 5118 7095 6765 3318 3210 2937 9506 6947 3295 947 9173 7162 8880 9069 7903 9287 6760 6686 2492 350 1920 6296 7845 3306 3330 8915 3029 4132 5107 6847 3171 8652 1864 4332 1500 1125 8901 9495 7191 3905 9884 3108 8163 611 6537 6599 570 3435 6335 7844 7688 1177 7261 5788 3423 410 5911 7971 5029 9281 2247 5071 1237 5413 5908 6921 4012 2851 4589 8287 3608 2322 5239 2082 707 1372 4341 2509 7495 7897 9542 4096 2844 7105 9512 9905 5723 4541 6666 1738 857 9421 673 1496 7190 2982 1431 868 5048 9164 1522 5891 4200 8367 2467 3047 4080 4747 9733 1152 3866 741 5196 6486 4305 1402 1426 733 8253 8087 1939 8495 8191 5210 2871 3042 9636 8873 9478 3673 9357 32 2475 4092 7378 6354 8231 3469 2547 7296 7336 9721 2513 4900 7533 9259 4222 6314 9714 8545 196 3195 5927 2136 8233 9234 7957 2969 4938 6628 1598 6209 5039 8855 6911 8906 3726 8251 9424 1683 2517 6974 29 8205 3377 8153 6165 4091 2794 8744 5491 7253 6197 5051 3724 4282 6276 7215 6182 3086 2400 7705 2448 1913 9038 3314 5134 6330 6144 9628 1122 81 6069 6467 3909 6977 9201 333 4083 1379 7842 3425 2598 3929 5016 2687 1198 4408 3204 5760 2188 4292 8328 745 3328 8171 9759 3973 2049 392 3791 7939 233 3510 6038 5436 8003 55 1289 5708 6086 5415 9128 3872 1083 5538 7756 9891 5750 787 2994 9560 3333 1153 1223 2530 2806 5824 1748 9828 1660 4034 2759 7225 5925 7184 6356 4417 2430 3446 3348 7076 1282 4905 7623 4883 5366 247 6303 7067 7714 7600 4351 3889 7422 95 4100 7291 467 2493 8741 15 7917 5073 9342 8923 238 3409 7311 8888 2731 1806 8807 3717 7778 4745 9330 3273 4220 8391 9870 7776 7499 5724 6488 8508 5809 2396 7096 6526 4240 4326 5154 789 2702 8341 4937 4345 261 1843 309 4060 8747 136 6653 1421 5159 3281 2807 7193 8564 6673 4595 1701 961 5214 9912 3806 8562 3867 449 4456 8400 8935 3088 8194 6347 4004 5378 7567 4703 40 1568 6839 6310 9538 5472 5255 2996 5498 9950 7907 87 7024 8127 8280 6825 8558 9480 850 5044 6126 7019 561 7104 5419 4829 7342 8996 8611 2570 8345 1690 4277 5728 2548 1050 5468 4155 7678 5901 3178 4853 1255 5284 92 3238 6116 1151 8513 922 9568 8733 9163 4859 762 9431 5693 2528 7846 8755 3373 9861 1505 9070 7663 8040 2559 2938 9497 1272 9734 8113 9918 7343 6877 7537 5587 9213 5777 4977 3752 1264 607 9686 494 3399 7718 4380 1347 4259 7157 9366 1481 3339 244 2919 8060 3115 1733 7576 9797 5323 4620 5877 3380 6285 1392 8674 7425 1955 9525 810 3027 5157 6523 4817 6936 5563 6157 6746 7794 3146 3125 6496 6365 2722 4952 3766 8079 9309 4111 331 8497 3179 9315 214 5319 9099 6118 6364 5553 4618 692 5219 9420 7803 8829 7831 4659 7946 853 6059 6548 6007 4328 4583 7290 3774 6236 6992 6437 1641 9369 3509 4794 4498 6145 4910 5074 4405 2357 7473 8913 7545 2134 1978 6958 3003 9323 2971 4523 4766 5389 6036 2431 8877 2723 5706 848 6630 1743 6215 1307 8797 1664 6732 940 8889 913 1403 8485 2024 4670 7224 146 8911 9809 5609 6776 2047 8392 3103 2290 1509 3172 4365 5370 6091 3957 6864 7020 7953 210 478 4266 9509 8377 8634 376 3991 3040 1671 8539 8876 8582 7743 8244 1084 3996 4256 5946 7331 5560 5429 3843 5994 505 4854 4921 5702 5998 8103 3221 8156 6943 4723 176 2887 2918 7692 9071 6619 5651 1160 2874 4428 7542 2664 6027 7885 1698 3175 145 1722 8751 9340 3926 3081 2471 6664 1728 987 5548 6392 2488 9416 7719 5876 5581 3706 1753 8069 8052 3883 4565 6110 6458 2350 7707 8466 9607 5430 6669 1651 6102 9670 5021 2746 3275 608 1735 832 2713 8543 993 6475 7940 2771 3431 8496 2976 6731 121 9706 9200 3119 8610 112 4710 7608 7826 9427 4037 2027 9239 4606 8020 9437 1861 8856 2966 2086 3697 7010 5885 6657 6661 693 1201 2926 2928 9703 3591 8812 2619 178 201 1781 6975 4063 5956 1951 1006 1291 2766 356 1106 4511 232 1235 5974 96 1740 5334 9903 3434 8593 8518 8696 3988 3918 4630 432 6886 8630 6225 5632 9842 6609 5714 8980 7725 4661 9761 1591 7581 2077 5741 1673 2119 1783 9024 4996 7676 1026 8977 6952 3046 9256 5097 9613 661 974 4812 8711 5990 5504 3709 1965 1048 8793 4126 5507 8843 6191 7854 8145 80 9748 4089 263 8425 8180 9107 3683 6472 3039 4389 6797 2954 4608 1046 9014 2466 4496 2458 6416 4545 9318 9378 7335 3025 7791 6802 6705 3758 2169 927 728 1227 2671 4697 6428 2214 2061 3983 923 6587 251 8220 3663 3934 3165 4519 377 9187 3015 7403 3206 2963 9849 6782 5412 6684 7065 5781 1696 20 8891 7730 8806 800 3562 3289 7207 9712 7956 307 6730 9230 7618 9898 468 899 3290 3168 1514 6113 9988 7505 7660 5148 3824 9911 5145 6204 714 7293 8331 2909 3776 1969 3139 1107 9916 2443 5992 6734 1905 3750 1746 8357 5883 4728 7570 7355 1776 9635 8772 4743 7682 8276 5945 4805 8192 2224 5758 2606 645 3630 7409 7850 6194 3385 8327 7525 1883 4465 3611 5762 9198 9749 6272 7068 7150 5238 317 9472 3810 1316 7046 9639 6252 5861 7614 2399 2932 2291 1055 6581 6601 796 8376 2544 8516 289 6080 5535 5924 3967 8907 2067 5139 8831 1711 5614 1063 1689 2104 7926 836 1736 2152 3580 6859 1273 6030 6350 5416 9351 3876 7166 8353 2781 9202 7810 8476 9853 3664 6563 3323 6650 8817 7552 2783 5375 1432 1940 4518 8743 1851 6257 1051 5914 869 5556 5223 3826 3326 990 3351 3936 3387 9789 7972 5330 2907 3953 2464 4776 9687 4127 6008 4007 1654 6081 4042 2367 6570 8490 189 2659 7357 7262 5720 9325 6965 6939 4271 9692 8261 5832 8176 6624 4767 7423 4890 8646 5943 9401 8779 3184 3364 5342 6304 5208 6453 4935 7974 6018 2206 971 2984 4045 2508 6003 6313 3799 9218 2245 2738 3585 7969 1703 2690 2319 3971 6331 6833 6761 7244 4824 5704 4110 9361 2457 6414 479 2132 7371 1196 4301 7476 1963 7339 6883 8424 2151 7004 5013 5508 5736 7648 8395 2872 5965 6956 3692 600 9150 3433 6608 4909 2861 1800 556 5046 8735 8143 1691 5666 6721 6532 6990 2692 3117 5042 7391 2474 3804 7875 9302 8417 5480 5515 3043 3101 8259 4102 8364 9650 2037 2679 7405 5489 4441 936 7986 615 8092 876 2035 4604 3198 2898 8362 5162 7924 6498 7752 8762 9967 1110 7891 8222 7547 7822 5698 9674 2023 8198 6235 281 5462 3603 1387 6774 3523 7113 88 1635 590 1841 9479 7589 1105 4073 4873 6355 9368 7904 9594 4172 5626 5529 4015 6243 9178 4000 5473 8635 4505 6089 555 1440 102 6506 47 7056 73 4839 7656 8524 8615 1797 7767 2190 726 1086 6637 8053 7116 9953 5970 6482 1665 7229 637 6803 1319 1469 1960 459 8989 6614 6305 1036 9037 2159 1638 5077 4114 1717 76 8893 880 2755 8166 1140 9084 1893 4181 3216 6462 8258 870 8091 7979 7807 5352 1377 8312 4585 9236 8857 52 8910 2870 4792 2558 1092 194 6320 7501 3017 3749 7528 4171 4260 696 4860 9078 5820 5870 25 2856 4682 8311 6461 425 7163 4888 4376 4529 9776 3382 9267 7937 6333 6479 2924 2957 5903 6292 1620 4302 7037 1544 347 5677 8352 5185 44 1390 8057 4828 5600 5763 8190 2168 9629 1144 6360 6390 3208 5408 6329 9377 6541 3759 9393 3218 2422 3346 36 3138 6265 5917 1872 938 4275 1818 68 208 6058 5179 7910 7912 9189 2391 3422 9913 6807 8981 8101 5640 2791 5274 8363 1619 5692 245 66 3319 1755 4550 339 4851 5266 8727 3408 1279 1309 6403 2227 4807 2786 1270 3660 507 9982 9002 4314 852 2175 8945 6060 1349 2345 4965 1556 956 2545 4348 5661 2283 5722 5892 7319 9432 4982 2762 8698 5628 9162 1576 6415 8908 8457 1353 4116 9984 8201 6198 2071 446 5471 632 313 9335 1320 4382 8316 6115 7805 5512 5293 669 805 700 5171 6586 5524 9222 8175 1150 4598 495 6090 7852 2410 7297 8849 8129 2226 1429 361 7865 399 5372 5620 7587 7469 1772 4444 3995 1034 2107 7975 9411 874 3011 8601 3540 1336 8770 2385 1712 471 7466 5276 5983 6146 4709 4032 7595 2260 9908 7487 1644 4088 3340 6041 6639 9471 6745 248 9944 7673 6487 8766 2597 9498 6561 4763 4522 4939 4064 4629 3490 7811 1449 9770 7535 5207 9247 4533 6835 5230 9396 8034 4050 71 3246 7437 5061 6378 7270 9346 3353 9654 1647 2210 8774 6922 4142 575 915 3487 6063 9969 6438 818 649 4609 919 9972 1460 7439 8337 4139 3311 5882 8131 5288 3114 7288 8238 2195 4967 7099 7901 9871 5652 7599 1798 3600 6366 8108 2621 1596 7896 1549 3158 5254 7893 6551 4395 1104 3359 8662 1837 8997 8707 6869 8152 1661 1518 156 750 2078 5509 8503 9105 6387 8854 5023 2377 2587 4942 8426 5829 7165 8663 7619 2784 3844 316 1042 9034 5744 3625 3573 2579 373 6899 9253 964 7454 6421 6203 5949 9702 738 1091 6289 7254 656 3049 5240 8088 8157 2514 7481 9193 1374 1114 4849 2793 3735 4524 7144 8568 13 9754 4018 7724 7036 517 1757 5335 2817 230 6234 3161 7334 5520 7070 7384 1337 4003 7646 1089 9441 1411 7604 6208 2885 4233 60 0 5428 5872 2920 2358 1976 6577 8745 6295 9050 7182 8717 554 4520 6961 216 2648 9604 9863 6518 1366 5297 7180 1929 198 2225 626 1804 8678 8955 6023 1099 3574 6341 8839 3607 1862 9270 4357 1108 8212 1847 8423 4406 1633 9611 1385 8036 1936 7658 8234 3965 6074 5787 9743 8595 932 3985 8206 3946 3390 8729 1371 6196 4756 9269 1482 5944 2059 2052 9134 9973 8736 6791 1439 1269 7461 7175 7242 1168 6986 5739 2758 2911 3838 5096 1933 1873 1356 830 3640 243 4864 979 5564 23 5490 6021 1656 2361 4683 7951 4072 7598 6674 4480 1306 2394 5147 8657 6386 731 4106 9574 444 2661 1298 6549 1438 4840 6913 1158 7266 509 7084 27 6193 5569 8472 447 6222 2845 9698 5082 1643 6896 2764 4241 9856 8150 3969 7761 6103 8184 7269 2057 2721 5503 4740 4987 8560 779 6244 3644 8784 8905 9045 1813 2955 6954 6381 5137 9036 6553 3714 8042 8483 1506 7667 2150 9537 7884 299 8591 8133 8436 8208 2829 6890 7361 6569 4726 4038 3875 7768 2177 6938 9935 4808 7675 1137 4268 8979 483 103 365 9019 6166 7435 2831 8664 7039 8136 7511 8699 1659 2309 8833 8504 382 2698 6468 8414 3248 5588 9619 4158 9196 4262 1986 1136 3079 8553 6796 6393 4842 2554 6794 9534 4538 6264 5347 4708 6522 8252 7420 2404 3137 6050 181 3856 6201 1540 2757 3734 2684 957 3327 445 9232 6919 4455 6238 8687 5968 7118 7314 6106 491 2580 280 4790 278 6970 5510 2164 1199 6558 1860 401 2714 2286 9838 9501 8463 7664 3623 9532 7383 7933 7062 1897 2756 162 9965 9609 275 2215 7462 394 180 2553 8254 340 3124 1718 7031 7374 3006 9983 6820 7863 7149 2860 9624 4320 706 14 654 2058 9347 5680 255 3818 5106 4439 314 3653 207 5116 43 5657 5283 9630 5849 5028 169 1831 7698 9452 559 5041 7748 2678 754 903 953 7344 8330 4560 3203 3961 3450 439 5371 4678 7289 751 8551 3518 8608 7419 4742 139 3272 3762 8586 5358 4554 7540 9673 3520 9504 4379 6174 5445 943 7013 4123 8602 1854 5589 3064 4525 8731 2163 5987 9816 9668 7235 1935 7736 489 4061 1901 9388 1317 7417 2753 5637 5855 5400 8121 2438 422 3322 7275 5916 4017 6057 3911 85 1414 4584 3320 386 1420 9185 4470 7000 1079 195 9924 6671 8120 5673 5562 1782 806 2765 579 9701 6724 5545 2988 4024 9140 4783 114 8783 2893 6180 3375 4724 7052 9306 4244 9102 9729 8803 4727 1987 8827 3451 9755 1146 8455 4347 811 458 4706 6874 1076 6340 63 5786 5574 8730 3768 1692 3068 1715 4333 8554 8761 7670 7399 9917 4232 4039 2673 4020 8217 6723 7002 2862 1188 1375 2093 5533 8214 1912 5931 9454 6019 293 6940 8368 9811 7282 1445 2852 9994 3747 1515 1380 3394 6452 5633 312 2074 61 6691 9941 4582 3701 2813 2263 5962 7416 2981 4478 7278 1251 5816 455 3975 2889 7239 6668 3219 3411 8089 8289 199 6281 3716 2140 7982 457 8403 9353 4891 2091 1548 7825 3589 3194 9614 1479 9096 1789 4616 8963 5310 2143 8966 3060 6439 9952 1178 3019 4446 9617 3249 5407 4138 9250 5540 1478 5894 9190 4179 8709 9561 75 6758 2171 9125 2373 7111 3532 1566 4510 1378 2294 4412 4145 6012 5773 6636 4445 4777 8000 9079 9144 6111 4274 2728 8506 8102 2987 3527 6278 513 9665 1848 7080 8753 7796 2191 1232 5203 8482 9535 2116 2569 5947 8284 8446 9440 1891 3533 4974 4913 1012 315 660 2945 9158 9136 1348 5236 5502 7958 8309 3525 8563 239 3123 2590 8868 9577 577 2380 4748 3691 9317 2356 1472 4448 8269 4369 1238 9165 9257 9763 3131 9390 2419 5866 4344 4473 1551 2172 9921 9753 9453 9877 7706 134 6155 1066 8528 1896 6813 3549 4122 300 7590 1597 4712 2832 5451 2686 3629 1503 3059 9500 352 4624 1971 7258 8671 7441 5120 7140 3930 5247 8673 6137 9583 6578 155 5285 3069 3800 7079 5815 3837 4397 5423 592 1078 3337 4979 329 7295 906 1109 9319 5127 6895 5261 3637 8946 5742 9937 8099 93 2017 1476 7886 5978 5622 3524 2927 2310 588 4462 7376 2523 3737 2090 1260 2204 2265 1839 1938 2866 7553 1018 8096 8240 3787 7443 6815 3144 9130 2016 1584 8540 1141 9066 6210 3277 9004 935 5114 246 6916 3917 2951 9787 3657 4189 3708 8618 7938 1073 6875 2512 9389 8603 7573 3631 3705 7624 8522 8181 1437 1630 9462 4666 1578 1148 3895 6923 6280 9170 9833 388 1502 473 1879 2858 652 9333 3282 3301 2044 1784 7786 5192 341 5952 2886 8465 220 6853 3483 3878 1629 4886 2416 7601 6442 2732 6786 9526 2828 3436 3852 9310 8561 9074 1770 8452 5729 3619 9243 9386 8633 1216 8716 2098 6692 3896 9001 3550 1624 7554 2423 1 5070 4617 3579 7120 9299 2444 5483 9585 1805 4623 3244 1040 703 8967 3374 7029 926 3424 4563 5753 2930 3237 5015 5481 6171 5354 1405 1244 9550 7838 3132 2539 8076 8318 1604 4228 7647 5220 3110 7866 9482 2941 3610 2189 5216 675 4899 6790 8475 3438 3021 4970 6005 1777 9623 3723 1606 4044 3854 1450 8185 8750 9961 905 6929 960 3492 2603 7407 5757 3741 5056 4619 5960 6156 522 3830 8574 7201 4581 6904 7629 9223 4143 8637 1631 8813 430 9448 1534 9691 4104 6505 849 4489 2662 3189 1128 7555 3829 891 3182 548 4144 1763 303 3879 4946 5550 5783 6716 7356 6152 3090 1224 2432 4474 6499 823 8200 7726 3407 2546 1838 5630 9049 7851 6071 7027 7228 4637 3058 7329 1520 758 2810 2740 6605 7489 3024 5542 7246 2069 523 2975 4721 217 9645 3950 2004 9622 5900 3444 2261 5059 8580 9882 8894 2826 37 5264 7048 8937 4285 2704 7532 1880 3548 5923 2205 3968 4035 7126 7745 763 2913 9592 5397 5009 7364 7747 5803 6357 3363 9095 2007 3551 4870 7125 8790 7380 8130 6319 7460 5709 8825 1856 7550 8655 2775 5133 1035 3488 9771 7657 1700 9596 3362 5301 722 6065 1615 4151 2675 5018 6397 8620 8418 2335 508 3833 6881 1202 7350 2285 2946 188 6444 9266 9957 7211 7440 423 6451 9823 5401 3572 1612 8014 777 3848 7348 9524 2476 1677 5102 7605 443 5966 2040 4352 3294 6906 4265 4895 9527 6032 2446 6300 9992 1724 9745 118 8124 5851 8179 6949 2209 5890 2507 1447 8598 3300 2013 9117 1561 5528 5434 6480 9051 4919 3882 6560 3868 6022 2234 9434 7520 8991 7512 4460 5257 3650 193 3316 3078 3312 7721 7198 9694 1054 1065 1911 7124 616 3624 2785 5052 9625 3298 5033 9883 4799 9328 9949 682 7882 4463 4187 7579 7483 723 1608 6964 1914 9520 6513 6337 129 4894 2626 2211 803 4193 9648 5823 9282 4821 4107 3767 6739 4769 6917 1147 2420 2238 5713 6122 3477 6971 3885 7018 5464 4950 6093 6029 7824 4120 464 1555 6001 9661 3860 7255 7152 8387 7471 7915 9360 7832 5001 2452 9528 5475 2196 2157 710 427 130 8450 7071 5532 7717 8705 8909 6612 7996 4318 1866 1028 3396 8881 3809 5129 4051 6471 453 5386 7941 7023 7925 3066 8838 5646 4058 1103 5602 6880 9936 6343 2244 2344 6168 8944 9790 1835 1022 8756 3414 3526 2355 5098 5796 3931 4765 2779 4908 5444 9513 1493 4725 1452 7731 2363 8090 6107 9946 5565 9447 6436 6125 8679 3459 539 5800 6363 6876 2496 4306 3700 5167 7103 3994 3222 228 2307 5 6275 1945 9669 6188 4047 1417 7280 390 2099 9311 3839 4249 6153 3626 6450 3398 9352 3286 1702 742 8158 325 391 9958 5175 3303 1672 1695 1720 6134 6812 9817 7635 5485 8874 9907 6033 8614 7467 6798 2525 6509 4736 9143 9899 1343 6408 474 4990 135 8223 690 7867 3329 4762 3283 1589 7349 5062 5307 3403 1274 8286 8448 5955 4133 7640 894 3795 2834 3655 9540 9408 648 5873 2879 8567 9008 9536 5263 291 1230 7209 407 5752 6995 3371 597 5613 9157 6150 3545 6627 1840 6682 1613 8218 7034 7359 807 9571 56 541 8072 4272 9362 4791 6795 7247 8488 9795 536 942 7775 408 6323 1097 9723 910 6740 1504 3916 6112 1398 2644 2748 1101 8080 1276 9110 7820 828 2193 9657 9295 6121 5975 1321 11 2433 732 1031 3106 767 708 6679 2462 7087 2483 6870 7286 9541 6443 4219 3428 3873 4245 5689 8237 879 4208 2374 1265 4941 1989 8640 2633 9930 2095 5922 3372 8680 4691 9517 2641 4686 7750 6759 381 4109 895 8988 9772 4951 524 8170 1484 253 929 1049 610 1165 1709 6933 8930 9573 2405 2989 8502 6423 2487 9039 1290 1719 2840 1052 7562 6821 7502 8921 5997 5495 6928 5929 8846 6247 3075 7963 8298 64 5349 8100 6924 9582 4838 827 7500 717 2353 6211 4962 4548 4169 1271 7516 4689 5260 1183 4420 2592 3861 5641 3765 2565 306 9876 241 6632 7429 4986 4912 2944 8148 1213 7012 6138 8576 8493 6868 8301 8535 9092 7789 4800 7057 1102 5499 2161 4735 1217 304 1688 5032 9320 2611 342 6510 2316 1335 364 1156 3094 1302 6224 1881 5233 8669 358 7370 6061 2497 3887 4291 7816 4315 8900 2974 4601 2894 5779 4057 8378 3645 8649 3805 6640 9227 712 7237 7354 4304 5887 8098 1121 240 8985 6756 1579 5610 166 7630 8305 2447 6708 6953 8867 8048 2230 998 9003 8442 4450 2068 533 5450 1705 1261 6083 4636 6567 4924 3384 4774 9374 6811 7316 1968 1009 8960 4564 8306 8529 1037 9894 9681 897 2142 192 2212 1256 8728 2760 4507 9161 9768 9154 6497 4161 954 3963 6368 3990 6142 6088 3853 1315 3959 42 5156 6860 6020 1077 7217 9881 2250 6242 7155 2873 7681 5681 9251 2672 699 7176 6139 6576 3715 776 6344 497 6353 9724 4758 6648 165 4556 3648 5176 6647 1194 4028 6302 4442 9549 3718 5938 1550 9785 441 4075 5973 7208 8029 4053 2925 9840 2123 414 8682 8033 5376 1142 8866 9579 9618 1338 4811 4775 9314 6540 8007 4945 4313 6626 5237 3491 438 3740 187 4517 7307 1887 1710 7108 3870 58 2439 3960 3274 8896 6327 1327 5427 8365 9496 8486 8512 2838 3502 2943 3913 7684 9363 1127 8940 3193 5246 24 8366 7841 8995 3712 3560 765 623 6908 5231 490 5988 5705 2425 4494 8801 7007 6717 5889 1197 4980 4403 6473 1725 2625 4850 4221 1424 7879 1799 9951 5591 8332 4084 2718 7438 8667 3788 2321 252 3797 332 8509 8346 6871 6969 3224 5368 5302 7945 8295 8215 4591 1984 1657 21 5618 3877 8343 7741 8787 8507 7515 2079 1570 9600 1650 8026 7294 2179 2124 400 4029 8216 7639 7696 6388 3177 3528 2066 3813 4309 1669 3429 1485 5331 6024 1242 6590 5465 6359 514 9277 1407 4764 9262 878 5995 3010 6186 4370 6752 7538 8531 5298 4165 1016 5590 1944 1858 8173 4005 7154 5616 5919 7496 6232 9981 8447 62 1277 2292 2538 7960 2531 5826 10 3945 2370 2117 6469 7839 8983 5598 5019 5612 2133 2934 7578 8334 6407 2688 5671 6336 3493 5853 3263 3647 2118 4334 4903 1992 9063 1501 7849 9455 2864 7806 368 1867 8654 84 256 9852 6248 9422 8347 2922 5355 5897 1169 2665 2304 1759 9514 2850 6747 1707 8077 3034 4521 3925 2021 620 7723 5571 8581 229 7757 6169 4025 7238 9655 8575 7987 7160 6372 4322 4699 6889 6633 7617 5487 3569 9349 4694 6104 8606 8299 9090 8323 1812 3096 6994 5296 9521 8802 9271 7892 1676 5516 1567 9021 6655 9489 4576 2490 4459 4198 1583 4715 5245 7746 826 6588 740 5793 900 1027 4381 7234 3686 1537 4788 8954 4373 5455 3229 7577 5123 5155 7899 4125 6719 1895 9909 8681 9477 9943 5460 8590 8703 3480 3557 8110 7263 7468 6978 1745 6175 9569 2411 9160 9962 1383 2842 1268 1325 8307 3341 1369 8853 3393 8695 5893 1486 3901 2235 4877 5559 7459 4468 7058 5084 6233 1981 564 560 7333 6882 5421 8408 9696 7251 5128 3537 7948 4288 3292 6530 3054 9179 3812 1824 2409 1019 1729 4567 6391 150 8117 4183 5112 4705 2995 5771 421 8125 8147 8144 6077 4011 6987 4526 8842 8084 2145 2711 9829 7702 4147 1564 1531 4571 898 834 6629 1995 8715 5707 1416 4054 7137 2388 7156 6253 5259 8123 8748 5309 551 4112 8579 7049 7964 1094 6073 9435 2712 4317 3302 4574 7715 2055 1117 4108 9708 5426 5234 1874 9483 9428 9265 2048 5798 7759 236 1365 5235 8929 5409 6800 9344 4167 18 7005 3694 3160 5287 4856 1149 9123 4031 7539 628 2846 9551 9978 9683 7997 1877 9027 642 78 1595 3614 4916 5751 7360 2158 2343 8193 1846 7659 5066 6855 6836 9305 4002 526 7171 6270 2054 5022 6697 4070 7414 5678 6709 5802 379 5244 702 378 1962 9403 498 4718 624 9878 3859 6770 7817 5989 7327 7920 6287 1023 1212 6778 3559 4454 8882 2270 6271 3543 51 2379 6903 7015 4293 311 668 6299 2883 8670 8471 9709 9398 3439 7153 328 3430 9345 5104 3383 2414 2415 3554 1942 4954 1247 6167 683 9653 694 6997 1368 9784 1185 7203 3519 486 1044 7666 297 2217 887 3148 9241 884 2131 4797 9684 6009 5874 6447 8624 5843 2837 9976 6109 5365 5025 9098 8431 532 1475 1602 3685 2259 3167 2536 1966 2002 1085 5124 8022 2624 5130 5194 7797 4639 5819 9086 4638 185 510 8470 4493 9106 4066 769 2213 5519 1233 4119 684 3265 1373 6220 1069 2833 3592 2780 6006 567 2518 5847 7450 5496 7546 862 9017 4590 2567 8025 9023 4213 211 5608 2848 7889 6141 1134 175 587 1304 8059 8953 996 9493 1225 3202 4323 9859 7030 1941 801 8111 8776 1632 4785 9715 6967 1442 8607 1666 9940 1693 310 6448 8322 2902 6082 7093 7323 6119 2609 8525 5241 5846 9043 8202 5486 6754 975 4947 3259 4452 3587 9986 8832 7932 6316 7174 3402 6996 2680 3242 8028 5697 6297 4211 552 5884 4845 4316 9380 6694 9991 2645 613 911 6424 8016 893 9463 9304 8336 6834 8903 35 4041 1209 889 1341 1384 4043 4816 8994 7870 5841 1950 2578 6764 4809 3230 8372 5417 3713 8505 6753 7264 6830 6129 3772 8892 6100 4881 3072 6678 7358 4400 1997 6579 4353 2835 6935 2125 4904 9054 9792 4547 7063 6470 2094 3536 8051 6031 977 8137 4843 9175 6259 5136 7611 6114 845 2691 4553 5511 6972 295 7586 8648 4844 2736 4671 7650 5737 968 2421 9423 1119 91 9035 1882 5881 4544 9588 8433 6216 9009 424 3940 3005 1621 6718 3907 7346 2772 7351 3122 2972 8811 6736 7292 4387 8734 3365 385 9288 7729 1542 3555 5623 948 4447 6600 7202 1208 7367 2208 3345 4434 6559 4934 1057 6620 9488 6546 4077 3627 8725 7197 4915 8443 4558 6562 1795 4508 2796 8369 4140 5648 9082 8385 7690 2030 917 9153 2677 9872 4430 920 274 7968 2977 6681 3512 8255 2876 433 3317 9338 1610 2233 4433 123 1967 3825 6993 3821 2761 8226 6934 8514 2100 2654 2549 4649 9289 4928 3343 8927 9868 8822 4436 8527 492 2634 405 1191 4257 8067 7679 5321 5164 3558 3141 746 2535 9679 7580 1814 6239 5197 5575 9931 8938 6750 6367 8265 2632 6634 747 7401 437 1742 3992 9869 2437 7823 6283 4504 6101 2139 7674 4225 4893 1870 6900 6852 9188 4754 4375 3590 3104 9245 8010 8177 8799 3816 665 7642 231 5153 7325 6891 2295 928 8221 8058 4079 5862 2904 7241 7265 802 9052 1900 8093 6135 4681 2884 3325 3262 4878 5381 5513 2025 4329 7900 1541 3454 9627 6575 844 9172 9782 6501 3922 9406 4605 7872 9672 3938 9591 4295 6902 4062 1181 7432 4597 4621 4664 374 8481 1754 3276 9968 5313 3798 5201 2622 1803 9490 7930 9492 65 2727 8154 6948 2707 1068 5414 3315 7001 8338 7252 1020 7050 6189 7434 286 4746 6465 4224 429 9987 5543 5696 2006 1829 5514 995 4820 4561 161 5808 1173 6404 5076 970 6418 4674 1312 6508 7793 9910 4615 4855 8282 7529 3836 735 2393 545 4101 9575 1186 1098 4419 1331 2642 1750 6983 2747 5958 4995 6966 1571 3564 6459 1131 50 1113 8792 7393 6762 8625 8078 7114 9073 6362 4648 698 3484 566 2403 8047 6514 9091 115 426 831 8613 5825 883 9047 1332 1890 7881 7377 3511 7232 6769 475 9087 4346 1189 1088 3514 9494 9775 7436 7060 4429 5527 6960 6000 2149 7368 6656 2417 5222 5173 7808 8434 5402 1686 2936 7645 2696 6170 8041 4992 1435 1080 5060 9610 2770 6805 2246 1466 6873 7990 6580 8248 3881 2647 8919 3667 7818 4170 6557 1708 4806 7353 9112 6515 6161 5268 6884 6842 7755 1297 4761 8584 8897 6894 2073 2863 5463 8409 2026 8533 4693 2532 5163 9225 2354 985 7613 1949 3678 8317 8055 1314 6914 7285 5493 5627 4528 918 1410 5844 3761 4243 54 1794 4846 8501 3910 2323 6698 542 9055 5360 9726 9111 1801 6538 9402 3037 5785 9274 2336 1768 8850 159 6777 2427 7386 5748 1457 4194 2167 7181 842 4135 463 5663 1288 5986 3565 3002 7267 1999 5031 7522 8789 4750 5546 2663 7151 4297 1339 8973 3760 2102 2956 799 9642 1294 6897 3460 9399 658 4178 7840 2790 6128 9562 2325 9678 5282 1926 5221 9522 3531 6582 3406 4906 6379 6863 9457 7713 5953 9268 2115 5067 9565 4464 4401 4731 7173 8828 3358 6845 9801 1498 2088 9412 7222 6184 8083 771 6322 2480 6466 8658 1070 6274 4423 8277 6675 1206 8266 402 7999 6315 7362 3153 856 7112 1358 8722 7463 2992 7699 2046 143 8999 6490 5215 3899 8389 7643 8283 8890 8441 260 6371 9371 7701 6985 9989 7548 8293 370 5746 761 8219 2418 1545 5645 4027 3098 1639 6256 6373 353 372 336 4287 7607 5936 6076 8986 5398 8764 5951 343 9115 174 3212 8872 2369 5703 6484 705 6660 5679 4273 9926 8383 5935 982 3151 7575 8027 7315 3033 4869 8557 5710 6824 1116 8819 3602 448 1283 9405 2778 6258 1785 2112 2967 7220 3721 2978 8074 3404 861 4115 70 1788 2010 571 3675 6273 9621 8081 5057 2041 5272 6962 6431 760 755 8837 9711 7078 1836 3186 2331 2352 9656 4355 6907 3757 3226 7347 8848 7038 5108 5202 736 8555 6162 6598 3470 6055 6495 8549 4827 5933 4466 3690 7430 357 6597 7880 2958 841 9751 5734 294 1133 2576 1296 5869 3028 1490 8256 4641 5902 8542 9212 6410 4431 8438 7510 6735 4159 6638 5290 9796 2101 1059 3731 5249 2735 7783 6240 5665 428 4857 4026 7128 9159 3719 9845 8589 5374 9854 8511 404 3952 9979 8786 1043 3933 3113 2729 7045 500 1067 8933 9503 124 1423 9643 3874 2479 9449 9149 9216 1998 5170 6432 6595 8397 5688 4048 5954 4803 2725 3539 7025 1903 8413 190 7799 1024 7308 6942 5985 9959 2818 2657 8746 8285 375 7298 4303 730 2912 3495 393 5642 2715 3646 6251 7083 7762 2097 4215 3486 1328 9634 3426 4300 8225 3733 3232 9593 6573 9499 9896 9964 614 7668 8660 4150 1324 8970 1670 7551 820 2724 4022 1081 5267 2960 3269 6742 5443 6445 9793 1871 1617 7566 6426 1004 5186 7132 4983 6814 5442 5410 711 1539 8638 7967 470 3458 2435 1771 4867 7574 1508 2875 1626 7770 5674 1741 8374 3136 3127 83 6809 9169 4959 5981 3468 9666 7127 1885 6519 2595 1401 877 8359 8412 7321 1946 8035 4427 4964 9332 1171 8962 7686 1367 6092 9167 7424 3949 757 7694 2051 4223 8917 4969 6034 9124 9888 6187 6293 7503 7506 9740 2571 572 5716 2581 3445 2543 5435 9139 6043 4153 9131 558 7245 5404 962 6898 6787 8430 5899 1047 1507 2034 1952 4136 2617 674 7446 1979 9922 4396 5715 9735 9825 9646 4457 8292 7662 3674 9892 1572 8324 4255 6763 3370 9426 395 9897 8451 8788 2719 6981 2749 3803 5224 8949 681 3441 171 6748 2854 9093 9602 8969 1760 7649 7594 6010 824 8642 4588 7637 3413 2121 4958 8329 8211 7853 3751 8358 3241 9301 5049 2110 349 5604 6925 3014 9181 2614 5277 768 6856 8992 5660 7136 4539 2905 2252 5601 8419 8271 8841 2089 6542 650 8972 6358 3546 1464 4943 4680 6552 3044 355 113 950 3596 4593 7556 6973 3012 2008 4166 6483 816 1875 8878 3575 4156 9855 5193 958 9307 8760 6409 5780 4016 2892 7922 4177 5599 793 5138 4358 4226 8559 6550 8149 5649 2896 397 7693 1521 3185 7444 1205 5014 3443 9880 6999 7426 7428 7082 7716 6704 3567 4270 9640 4713 4837 795 4074 2478 6047 1527 5010 6284 4920 3613 3840 3693 2607 9530 5432 3007 2187 4390 1773 2504 5198 3503 5549 7558 8691 5653 9807 7955 9566 983 1892 4535 2280 3053 4076 7326 1284 7390 9255 4281 9502 7480 1281 5242 1326 2381 2541 9612 8416 1138 9791 6163 4033 8243 7582 5166 5420 872 7931 1467 677 3142 1038 9321 6539 6722 7942 3455 7107 1489 2253 5567 1886 2332 1451 2618 1491 8726 6334 5394 1193 7625 5813 5183 1580 8075 4848 6014 4668 1627 9041 8445 8920 110 666 5439 2742 4205 8086 2646 1443 4702 6699 2694 1118 7928 5905 5086 7008 4789 6749 2401 7837 5907 6695 1154 8499 5976 2754 3501 3215 8823 7431 7998 7862 8082 8462 6199 2839 3440 7410 5969 6615 9487 1679 8437 5582 5765 8723 7584 4534 7490 4643 1391 2318 6383 9659 8869 1786 8952 3205 2364 2593 4184 2248 5624 1455 530 9414 2366 8643 1902 6857 8532 531 2281 2890 6779 7704 9384 38 9473 8628 6268 1075 5669 8356 8636 5895 9192 5314 7040 2627 218 4349 5833 7022 1716 4898 2232 1474 3820 6434 5971 2360 4337 4 5099 2897 7800 6149 9109 7164 7304 9263 57 1668 324 6703 6741 7273 5085 7412 2577 3207 4771 2859 562 2635 3032 5554 3089 3846 7341 1575 151 8835 7231 8188 5794 1211 5643 9046 7927 7382 9998 4655 3392 4398 2685 6254 529 3756 77 7249 3268 2999 9209 6685 8174 3744 8321 1249 7628 4250 9830 9750 4234 6035 2485 268 3927 8122 3970 4191 8639 8676 4008 7943 7317 627 8065 5934 4923 4402 8800 2296 6663 2701 5340 387 4383 4594 8941 4435 9129 1029 7187 7790 6054 6531 1609 5827 7 5711 9552 9022 1354 5292 7312 9331 1667 4175 6862 2020 22 8187 4148 3057 266 9601 1849 7303 2170 7720 5094 5280 2498 7631 9410 4999 2219 7738 3556 1525 5385 7859 5058 2202 6544 2849 7588 7733 1586 3577 9296 9088 809 5534 45 5838 8039 7158 3998 1958 9682 5403 4479 1603 1678 3412 3739 8411 753 2173 1611 5636 8795 7053 9533 7777 1458 8922 3209 9589 3162 8546 7375 7812 5906 4587 4392 7041 569 8168 4586 4751 2412 6084 3391 6351 5064 8713 6062 4530 8228 6840 308 2880 3897 4739 97 4149 1287 285 1002 5991 168 4786 5735 3331 9786 2805 704 7985 6395 8458 908 5727 2312 4068 321 8138 6405 9381 7081 748 2022 5733 9298 6957 4813 6454 2700 4866 5213 6417 2640 7773 2311 1428 2301 2676 6819 34 4569 1363 9142 1982 9810 5291 3471 2706 1305 9064 5839 5568 4701 7847 105 1554 7259 2660 6348 3937 4046 1190 8183 5359 3091 813 5654 5131 2605 1599 8785 7322 7069 3154 4491 1850 2952 9844 9505 783 553 9260 9867 3720 9016 4760 5212 8272 2460 7394 655 601 9469 7518 133 2616 6339 5492 5551 7530 8031 9803 7373 5909 2933 2923 8804 4687 6214 4685 3080 1909 5188 3220 1207 3258 4610 8380 8044 8229 4542 7139 3808 2562 904 4714 182 1359 5252 1253 4469 5294 713 1074 9248 9831 9742 4607 2076 412 4487 5075 213 8904 5488 2198 1218 8340 4067 5615 6328 1581 3116 2655 9637 4830 7094 4210 1446 3332 8245 1993 9995 7192 5726 511 3855 2297 3781 9240 1430 7869 1241 4103 3670 8571 8577 8468 9947 3240 2018 9718 5572 3783 3681 5635 8105 1257 7722 6044 5759 1526 7508 3951 5583 9278 2602 8278 482 5351 1132 5875 2869 2825 2777 8665 2111 3522 815 3754 9996 496 8851 7868 4421 4818 4957 821 8308 2429 3793 4512 3652 2572 6635 1560 5977 205 5937 8690 5017 6792 4933 9089 1041 6017 5920 3704 6375 5743 8469 4994 1397 5858 3225 125 6290 2816 3159 7143 6477 4787 4780 5053 7771 3698 5740 6652 6286 3266 6 9744 5804 8951 5111 4030 2830 8641 9138 1130 8381 676 7835 6706 2674 6867 4757 5921 5205 6738 2472 5586 3677 8661 3599 5789 2980 2308 1033 9728 5390 4644 7710 8738 5306 5308 4471 1585 4021 8104 3473 2965 2561 209 6861 8021 1706 2060 1563 3481 5278 9237 7098 7102 5182 7858 3771 234 8898 5732 1058 1607 3082 3001 525 8062 7995 4298 7864 583 9710 8160 1245 2223 1454 3517 8461 1477 8578 6591 7524 8489 1845 9370 2501 1975 7857 8526 2407 9606 6192 1468 7655 7218 5078 5382 2279 6643 7389 528 2990 144 4146 6607 3669 3622 3815 6801 7993 9519 5228 9971 1994 9752 5880 4354 4368 8018 1766 5348 7947 9713 2568 6349 8106 3966 4366 8651 8875 2009 9804 8453 3213 9843 3898 1434 4862 8588 4940 537 7877 7919 3743 7732 6823 9758 5561 3504 8675 7014 4385 686 6099 9904 8422 1517 8978 5474 5393 1820 1234 4804 6108 2387 547 6618 997 2651 6172 5570 1250 3130 7934 8085 3835 6690 4647 6311 6781 2901 1732 3831 6463 2288 1360 3126 7989 4081 680 2185 7054 5448 5190 1980 9182 9685 5656 9238 8768 9020 7188 6837 5694 279 8406 3157 4227 3892 9348 2313 7830 3465 3228 9814 3780 6621 9337 7895 978 3472 3257 4688 1810 2620 8536 1381 1642 2392 2029 7978 5270 1714 5320 272 7274 5536 5842 2445 4414 634 5181 3421 9554 4730 8290 6649 9466 9114 7703 6260 9133 4676 326 5948 8943 2382 8834 8114 8037 3367 5326 2315 4094 8976 1111 4501 3417 6851 3342 4611 6728 6396 8297 4527 8002 3083 860 859 3334 8398 9355 543 4331 1135 5896 456 1182 8860 2585 5818 3822 6662 3279 3239 7484 955 8521 6206 7240 8547 3679 5367 9418 8924 6713 6130 5387 1278 9221 8959 1497 1807 3485 7488 6920 2329 9966 4426 8594 7301 4085 6858 5834 3170 1764 4770 2103 2808 9141 184 3794 1342 4476 2534 1894 6516 3888 3601 4931 6250 5406 5980 7085 1973 4917 1483 8303 7101 1157 5547 1340 8239 7620 9322 3452 8815 1792 5807 7906 2824 8167 1625 5596 9470 3143 7735 158 499 6702 909 5518 6564 5886 9356 819 6651 6667 1014 3845 8109 8351 6950 46 3978 8612 4391 937 7064 3147 2906 7168 107 5778 5115 9507 3986 4602 8262 8686 4162 5140 2031 7061 8552 9615 9417 5437 9813 8616 6123 4880 6455 2959 9458 9312 4600 5912 4907 1399 2486 8685 1126 5458 7026 6959 625 2997 9007 4577 6133 172 8186 1254 2395 6818 8141 5795 4399 2564 1292 3584 589 6230 9177 1222 8116 3616 1833 6602 5664 6646 3031 7809 1162 8492 2843 8390 8587 8195 8012 851 2180 5461 9100 4437 9194 3689 5396 3416 413 2378 5747 9923 2273 5950 8840 41 2973 3418 7671 5336 4087 1737 2231 663 6568 9433 2174 7992 466 2491 141 2229 3884 5251 1915 4130 843 3651 3542 7457 4308 2900 3347 7161 573 8782 6441 5119 9865 6245 8883 17 1001 6178 1071 6067 7781 4515 4575 7802 3448 4321 5312 5699 9788 4411 9544 6401 3489 6051 6543 6078 1382 2526 5361 1267 9246 1436 7277 6237 9184 2643 1699 3169 9595 1687 9059 6980 9042 3389 8739 3287 6592 7691 8942 1535 2773 1826 3420 4040 8230 9376 4415 1809 1395 5295 8732 9970 8401 697 2019 8484 384 2630 1299 2015 485 9651 7072 576 2348 8165 2888 574 2695 8236 3073 1796 417 7519 9229 5670 4892 9451 9350 5034 3134 2320 1779 3496 5941 8023 6205 8510 1937 3598 6988 8335 9555 8693 718 9442 225 2365 4650 3819 766 7366 3291 1815 7451 2964 6878 1364 5814 2241 3506 1433 3500 5125 8009 3547 7337 9231 9933 1243 7644 6829 9044 3932 4633 2604 2084 3725 4773 8706 619 9677 7131 9343 8339 4019 5144 2741 3880 4424 7749 8647 1093 8404 5158 8032 9285 1991 3453 8845 4819 2141 7560 2841 4889 6556 4131 5725 7615 4160 5662 6887 1064 6989 8294 2402 1790 5271 4949 8415 5772 8798 9308 9860 580 5454 4930 6616 9326 4667 3092 2440 8115 873 4246 9235 2342 9206 7994 3785 9928 5341 3792 149 7340 6712 3864 5584 7737 2763 527 344 5668 2566 9945 3405 7090 5279 3676 9456 991 5484 6517 6507 5957 1266 6131 1605 8178 1164 9436 3243 9127 8151 1005 8629 2147 1922 888 4635 5801 9048 2506 3400 1917 2442 3041 6246 1210 2705 4887 2716 3256 8566 3338 9599 7606 5072 4363 3335 2503 6429 6231 9392 7916 6831 4707 1752 7272 9397 3593 4258 6294 1793 3858 9847 7243 9757 1362 9467 3299 3620 4098 8291 5002 7888 4490 2383 8534 3703 2282 8515 3851 6070 5343 2347 8393 5580 3914 9061 322 480 5275 7602 4695 8861 5993 5050 1053 3397 4772 2156 8565 6221 7911 959 1231 3476 5942 6132 3051 6982 1739 2477 8956 3227 8775 9641 9146 1313 1318 9798 3129 2181 2962 5316 3668 4645 5767 2961 6912 5142 5926 9632 1100 4968 3707 8899 2788 4796 4299 3989 8961 5517 4212 7559 3415 6677 5579 2298 5776 7279 5494 8948 3802 153 5525 7226 4551 6154 8384 2750 4495 7541 6714 1593 9649 5850 550 8605 8030 1400 5523 7169 7328 6072 3401 9387 6042 6670 4966 1465 7874 3324 9284 3191 6241 6816 338 5686 8541 9581 5477 9846 2062 9313 1640 2574 965 3948 8169 1637 6308 9171 3432 1761 2940 7760 3958 3786 1285 5229 2065 4242 721 1025 406 8402 9373 2463 6565 202 7984 2699 4065 9208 3515 7689 2613 4759 2043 784 2586 8355 6680 865 7929 3688 4386 7122 3254 1032 3278 3466 7544 5047 9990 3538 9080 8045 6342 8644 419 8267 7494 3180 6536 582 1658 8820 6269 4744 3581 6910 5619 5501 7477 3732 6267 8017 8235 5405 6420 4168 2182 5558 3498 969 2221 9026 5200 6048 5918 9419 5755 604 219 4711 585 2942 7133 2948 4626 6460 4059 1511 506 1220 9887 5984 2266 2299 7257 4280 5195 538 9459 8473 5676 3321 9993 7833 5972 1943 6693 3636 6533 8622 4409 8519 2782 1286 7708 2011 1406 6687 9083 7610 7965 4458 6478 5963 2334 9779 1120 6848 3052 5691 4631 8808 5836 1344 1419 6382 7212 2105 5634 1990 6143 8196 8821 9539 689 3361 1646 6312 4720 5904 4998 2847 8859 1204 9103 2515 8112 8689 9430 7011 1252 9980 4374 6105 3954 1494 7829 59 16 2985 6419 8344 4963 160 4500 8767 3217 5467 7561 9508 8379 6751 147 7788 290 3568 3107 5497 4784 7513 2608 9608 4753 460 5027 6744 9812 2349 9292 9180 8304 7753 8700 3000 9279 3578 4472 7213 3964 6606 8011 6707 8982 9576 9806 2650 4336 546 19 5253 3456 335 5012 2456 5879 346 1988 7110 4669 74 6148 5913 1236 1280 4078 9152 1726 7189 8189 1825 8668 4467 6190 814 8350 4186 323 2386 3984 3817 4901 1350 5964 3309 5593 2582 2639 2455 5040 3661 5782 7088 9570 5552 7445 8771 9379 1834 9631 5069 2789 7448 3746 9647 5712 2450 6049 6227 9564 8247 6413 5685 4488 9057 4698 4732 3133 2269 5036 7456 2589 9382 8213 931 794 345 902 7641 9094 2184 8645 6937 6218 3166 9151 6854 1174 952 9120 7453 8701 7159 5311 3730 8396 4093 3777 3943 4896 7219 3641 7170 \n","Graphs are isomorphic\n","\n"]}],"source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <stdbool.h>\n","#include <cuda_runtime.h>\n","#include <limits.h>\n","#include <string.h>\n","\n","#define FILENAME_QUERY \"data/graph_query_500_3.csv\"\n","#define FILENAME_TARGET \"data/graph_target_500_3.csv\"\n","#define STREAMS 6\n","#define LABELS 10\n","#define INF 99999\n","#define MAXCONSTMEM 16000\n","\n","int blockSize;\n","__constant__ int constMem[MAXCONSTMEM];\n","__constant__ int constMemVar;\n","\n","#define CUDA_CHECK_ERROR(err)           \\\n","    if (err != cudaSuccess) {            \\\n","        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n","        printf(\"Error in file: %s, line: %i\\n\", __FILE__, __LINE__); \\\n","        exit(EXIT_FAILURE);              \\\n","    }\n","\n","/***** STRUCTS *****/\n","typedef struct {\n","    int* matrix;\n","    int numVertices;\n","    int* nodesToLabel;\n","    int** labelToNodes;\n","    int* labelsCardinalities;\n","    int* degrees;\n","} Graph;\n","\n","typedef struct {\n","    int *mapping1;  // mapping from query to target\n","    int *mapping2;  // mapping from target to query\n","    int *T1;        // Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are neighbors of nodes that are.\n","    int *T2;\n","    int* T1_out;     //Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti. Cioe nodi che non sono in mapping e non sono vicini di nodi coperti\n","    int* T2_out;\n","} State;\n","\n","typedef struct {\n","    int vertex;\n","    int* candidates;\n","    int sizeCandidates;\n","    int candidateIndex;\n","} Info;\n","\n","typedef struct StackNode {\n","    Info* info;\n","    struct StackNode* next;\n","} StackNode;\n","\n","/***** GRAPH PROTOTYPES *****/\n","void initGraphGPU(Graph*);\n","Graph* createGraph();\n","void addEdge(Graph*, int, int);\n","Graph* readGraph(char*);\n","void printGraph(Graph*);\n","void freeGraph(Graph*);\n","void setLabel(Graph*, int, int);\n","\n","Graph* graphGPU(Graph*);\n","void freeGraphGPU(Graph*);\n","\n","/***** STATE PROTOTYPES *****/\n","State* createStateGPU(Graph*, Graph*, cudaStream_t*);\n","void freeStateGPU(State*);\n","void printState(State*, int);\n","void updateStateGPU(Graph*, Graph*, State*, int, int, cudaStream_t*);\n","void restoreStateGPU(Graph*, Graph*, State*, int, int, cudaStream_t*);\n","\n","/***** VF2++ PROTOTYPES *****/\n","void vf2ppGPU(Graph*, Graph*, State*, Graph*, Graph*, cudaStream_t*);\n","bool checkGraphPropertiesGPU(Graph*, Graph*, Graph*, Graph*, cudaStream_t*);\n","int compare(const void*, const void*);\n","int* orderingGPU(Graph*, Graph*, cudaStream_t*, Graph*);\n","void findRootGPU(Graph*, int*, int*, int*, int*, cudaStream_t*);\n","void processDepthGPU(Graph*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, cudaStream_t*, Graph*);\n","int* findCandidatesGPU(Graph*, Graph*, State*, int, int*, Graph*, Graph*, cudaStream_t*);\n","bool cutISOGPU(Graph*, Graph*, State*, int, int, Graph*, Graph*, cudaStream_t*);\n","\n","/***** STACK PROTOTYPES *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex);\n","StackNode* createStackNode(Info*);\n","void push(StackNode**, Info*);\n","Info* pop(StackNode**);\n","bool isStackEmpty(StackNode*);\n","void freeStack(StackNode*);\n","void printStack(StackNode*);\n","void printInfo(Info*);\n","void freeInfo(Info*);\n","StackNode* createStack();\n","Info* peek(StackNode**);\n","\n","int main() {\n","    blockSize = 4;\n","\n","    Graph* h_g1 = readGraph(FILENAME_QUERY);\n","    Graph* h_g2 = readGraph(FILENAME_TARGET);\n","    Graph* d_g1 = graphGPU(h_g1);\n","    Graph* d_g2 = graphGPU(h_g2);\n","\n","    cudaStream_t streams[STREAMS];\n","\n","    for(int i = 0; i < STREAMS; i++) {\n","        cudaStreamCreate(&streams[i]);\n","    }\n","\n","    State* d_state = createStateGPU(d_g1, d_g2, streams);\n","\n","    vf2ppGPU(d_g1, d_g2, d_state, h_g1, h_g2, streams);\n","\n","    int* mapping1 = (int*)malloc(d_g1->numVertices * sizeof(int));\n","\n","    if(mapping1 == NULL) {\n","        printf(\"Error allocating memory in main\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(mapping1, d_state->mapping1, d_g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    printf(\"Mapping\\n\");\n","    for(int i = 0; i < d_g1->numVertices; i++) {\n","        printf(\"%d -> %d\\n\", i, mapping1[i]);\n","    }\n","\n","    for(int i = 0; i < STREAMS; i++) {\n","        cudaStreamDestroy(streams[i]);\n","    }\n","\n","    freeGraph(h_g1);\n","    freeGraph(h_g2);\n","    freeGraphGPU(d_g1);\n","    freeGraphGPU(d_g2);\n","\n","    freeStateGPU(d_state);\n","\n","    return EXIT_SUCCESS;\n","}\n","\n","__global__ void initArrayKernel(int* d_array, int size, int value) {\n","    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","    if (idx < size) {\n","        d_array[idx] = value;\n","    }\n","}\n","\n","__global__ void initMatrixKernel(int* d_matrix, int V, int value) {\n","    int col = threadIdx.y + blockIdx.y * blockDim.y;\n","    int row = threadIdx.x + blockIdx.x * blockDim.x;\n","\n","    if (row < V && col < V) {\n","        d_matrix[row * V + col] = value;\n","    }\n","}\n","\n","/***** GRAPH FUNCTIONS *****/\n","void initGraphGPU(Graph* g) {\n","    g->matrix = (int*)malloc(g->numVertices * g->numVertices * sizeof(int));\n","    g->nodesToLabel = (int*)malloc(g->numVertices * sizeof(int));\n","    g->labelsCardinalities = (int*)malloc(LABELS * sizeof(int));\n","    g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));\n","    g->degrees = (int*)malloc(g->numVertices * sizeof(int));\n","\n","    if (g->nodesToLabel == NULL || g->labelsCardinalities == NULL || g->labelToNodes == NULL || g->degrees == NULL || g->matrix == NULL) {\n","        printf(\"Error allocating memory in initGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int *d_nodesToLabel, *d_degrees, *d_matrix;\n","\n","    cudaStream_t stream1, stream2, stream3;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","    cudaStreamCreate(&stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodesToLabel, g->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_degrees, g->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix, g->numVertices * g->numVertices * sizeof(int)));\n","\n","    int gridSize = (g->numVertices + blockSize - 1) / blockSize;\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_nodesToLabel, g->numVertices, -1);\n","    initArrayKernel<<<gridSize, blockSize, 0, stream2>>>(d_degrees, g->numVertices, 0);\n","\n","    int gridSizeX = (g->numVertices + blockSize - 1) / blockSize;\n","    int gridSizeY = (g->numVertices + blockSize - 1) / blockSize;\n","    dim3 gridSizeM(gridSizeX, gridSizeY);\n","\n","    dim3 blockSizeM(blockSize, blockSize);\n","    initMatrixKernel<<<gridSizeM, blockSizeM, 0, stream3>>>(d_matrix, g->numVertices, 0);\n","\n","    for (int label = 0; label < LABELS; label++) {\n","        g->labelsCardinalities[label] = 0;\n","        g->labelToNodes[label] = (int*)malloc(g->numVertices * sizeof(int));\n","    }\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","    cudaStreamSynchronize(stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->nodesToLabel, d_nodesToLabel, g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->degrees, d_degrees, g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->matrix, d_matrix, g->numVertices * g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream1);\n","    cudaStreamDestroy(stream2);\n","    cudaStreamDestroy(stream3);\n","\n","    cudaFree(d_nodesToLabel);\n","    cudaFree(d_degrees);\n","    cudaFree(d_matrix);\n","}\n","\n","void setLabel(Graph* g, int node, int label) {\n","    if (g->nodesToLabel[node] == -1) {\n","        g->nodesToLabel[node] = label;\n","        g->labelsCardinalities[label]++;\n","        g->labelToNodes[label][g->labelsCardinalities[label] - 1] = node;\n","    }\n","}\n","\n","void addEdge(Graph* g, int src, int target) {\n","    g->matrix[src * g->numVertices + target] = 1;\n","    g->matrix[target * g->numVertices + src] = 1;\n","    g->degrees[src]++;\n","    g->degrees[target]++;\n","}\n","\n","Graph* createGraph() {\n","    Graph* g = (Graph*)malloc(sizeof(Graph));\n","\n","    if (g == NULL) {\n","        printf(\"Error allocating memory in createGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    g->matrix = NULL;\n","    g->numVertices = 0;\n","    g->nodesToLabel = NULL;\n","    g->labelsCardinalities = NULL;\n","    g->degrees = NULL;\n","    g->labelToNodes = NULL;\n","    return g;\n","}\n","\n","Graph* readGraph(char* path) {\n","    int src, target, srcLabel, targetLabel;\n","    Graph* g = createGraph();\n","\n","    FILE* f = fopen(path, \"r\");\n","    if (f == NULL) {\n","        printf(\"Error opening file\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    char line[128];\n","    fgets(line, sizeof(line), f);\n","    sscanf(line, \"%*s%*s%*s%d\", &g->numVertices);\n","    fgets(line, sizeof(line), f); // skip the header\n","\n","    initGraphGPU(g);\n","\n","    while (fgets(line, sizeof(line), f)) {\n","        sscanf(line, \"%d,%d,%d,%d\", &src, &target, &srcLabel, &targetLabel);\n","        addEdge(g, src, target);\n","        setLabel(g, src, srcLabel);\n","        setLabel(g, target, targetLabel);\n","    }\n","\n","    fclose(f);\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        g->labelToNodes[label] = (int*)realloc(g->labelToNodes[label], g->labelsCardinalities[label] * sizeof(int));\n","    }\n","\n","    return g;\n","}\n","\n","void printGraph(Graph* g) {\n","    for (int i = 0; i < g->numVertices; i++) {\n","        for (int j = 0; j < g->numVertices; j++) {\n","            printf(\"%d \", g->matrix[i * g->numVertices + j]);\n","        }\n","        printf(\"\\tVertex %d, label %d, degree %d\\n\", i, g->nodesToLabel[i], g->degrees[i]);\n","    }\n","\n","    printf(\"\\nCardinalities\\n\");\n","    for (int i = 0; i < LABELS; i++) {\n","        printf(\"Label %d: %d\\n\", i, g->labelsCardinalities[i]);\n","    }\n","\n","    for(int i = 0; i < LABELS; i++) {\n","       printf(\"\\nLabel %d\\n\", i);\n","       for(int j = 0; j < g->labelsCardinalities[i]; j++) {\n","           printf(\"%d \", g->labelToNodes[i][j]);\n","       }\n","    }\n","}\n","\n","void freeGraph(Graph* g) {\n","    for(int i = 0; i < LABELS; i++) {\n","        free(g->labelToNodes[i]);\n","    }\n","    free(g->labelToNodes);\n","    free(g->matrix);\n","    free(g->nodesToLabel);\n","    free(g->labelsCardinalities);\n","    free(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","Graph* graphGPU(Graph* h_g) {\n","    Graph* d_g = createGraph();\n","\n","    size_t sizeMatrix = h_g->numVertices * h_g->numVertices * sizeof(int);\n","    size_t size1 = h_g->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    d_g->numVertices = h_g->numVertices;\n","    d_g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));   // it is a vector on host wich contains pointers to vectors on device\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->matrix, sizeMatrix));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->nodesToLabel, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->labelsCardinalities, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->degrees, size1));\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->labelToNodes[label], h_g->labelsCardinalities[label] * sizeof(int))); // each vector on device has a size equal to the cardinality of the label\n","    }\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->matrix, h_g->matrix, sizeMatrix, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->nodesToLabel, h_g->nodesToLabel, size1, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->labelsCardinalities, h_g->labelsCardinalities, size2, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->degrees, h_g->degrees, size1, cudaMemcpyHostToDevice));\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_g->labelToNodes[label], h_g->labelToNodes[label], h_g->labelsCardinalities[label] * sizeof(int), cudaMemcpyHostToDevice));\n","    }\n","\n","    return d_g;\n","}\n","\n","void freeGraphGPU(Graph* g) {\n","    for(int label = 0; label < LABELS; label++) {\n","        cudaFree(g->labelToNodes[label]);\n","    }\n","    free(g->labelToNodes);\n","    cudaFree(g->matrix);\n","    cudaFree(g->nodesToLabel);\n","    cudaFree(g->labelsCardinalities);\n","    cudaFree(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","/***** STATE FUNCTIONS *****/\n","State* createStateGPU(Graph* g1, Graph* g2, cudaStream_t* streams) {\n","    State* s = (State*)malloc(sizeof(State));\n","\n","    if (s == NULL) {\n","        printf(\"Error allocating memory in createStateGPU\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    size_t size1 = g1->numVertices * sizeof(int);\n","    size_t size2 = g2->numVertices * sizeof(int);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->mapping1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->mapping2, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T2, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T1_out, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T2_out, size2));\n","\n","    int gridSize1 = (g1->numVertices + blockSize - 1) / blockSize;\n","    int gridSize2 = (g2->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream1>>>(s->mapping1, g1->numVertices, -1);\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream2>>>(s->T1, g1->numVertices, -1);\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream3>>>(s->T1_out, g1->numVertices, 1);\n","\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream1>>>(s->mapping2, g2->numVertices, -1);\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream2>>>(s->T2, g2->numVertices, -1);\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream3>>>(s->T2_out, g2->numVertices, 1);\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","    cudaStreamSynchronize(stream3);\n","\n","    return s;\n","}\n","\n","void freeStateGPU(State* s) {\n","    cudaFree(s->mapping1);\n","    cudaFree(s->mapping2);\n","    cudaFree(s->T1);\n","    cudaFree(s->T2);\n","    cudaFree(s->T1_out);\n","    cudaFree(s->T2_out);\n","    free(s);\n","    s = NULL;\n","}\n","\n","void printState(State* s, int numVertices) {\n","    printf(\"Mapping 1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping1[i]);\n","    }\n","\n","    printf(\"\\nMapping 2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping2[i]);\n","    }\n","\n","    printf(\"\\nT1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1[i]);\n","    }\n","\n","    printf(\"\\nT2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2[i]);\n","    }\n","\n","    printf(\"\\nT1_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1_out[i]);\n","    }\n","\n","    printf(\"\\nT2_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2_out[i]);\n","    }\n","}\n","\n","__global__ void updateStateKernel(int* matrix, int V, int* mapping, int* T, int* T_out, int node1, int node2) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( idx < V) {\n","        if(idx == node1) {\n","            mapping[idx] = node2;\n","        }\n","\n","        __syncthreads();\n","\n","        if(matrix[node1 * V + idx] == 1 && mapping[idx] == -1) {\n","            T[idx] = 1;\n","            T_out[idx] = -1;\n","        }\n","\n","        __syncthreads();\n","\n","        if(idx == node1) {\n","            T[idx] = -1;\n","            T_out[idx] = -1;\n","        }\n","    }\n","}\n","\n","void updateStateGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int candidate, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","    updateStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_g1->numVertices, d_state->mapping1, d_state->T1, d_state->T1_out, node, candidate);\n","\n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    updateStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, d_g2->numVertices, d_state->mapping2, d_state->T2, d_state->T2_out, candidate, node);\n","}\n","\n","__global__ void restoreStateKernel(int* matrix, int V, int node, int* T, int* T_out, int offset, int usage, int* mapping) {  // offset introduced because of the two streams\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx >= V) {\n","        return;\n","    }\n","\n","    int isAdded = 0;\n","\n","    if(matrix[node * V + idx] == 1) {\n","\n","        if(usage == 1) {\n","\n","            if(constMem[offset + idx] != -1) {  // constMem contains mapping1 if offset is 0, mapping2 if offset is V\n","                atomicExch(&T[node], 1);\n","                isAdded = 1;\n","            }\n","            else {\n","                int hasCoveredNeighbor = 0;\n","                for(int adjVertex2 = 0; adjVertex2 < V; adjVertex2++) {\n","                    if(matrix[idx * V + adjVertex2] == 1 && constMem[offset + adjVertex2] != -1) {   // constMem contains mapping1 if offset is 0, mapping2 if offset is V\n","                        hasCoveredNeighbor = 1;\n","                        break;\n","                    }\n","                }\n","\n","                if(hasCoveredNeighbor == 0) {\n","                    T[idx] = -1;\n","                    T_out[idx] = 1;\n","                }\n","            }\n","\n","        } else {\n","\n","            if(mapping[idx] != -1) { \n","                atomicExch(&T[node], 1);\n","                isAdded = 1;\n","            }\n","            else {\n","                int hasCoveredNeighbor = 0;\n","                for(int adjVertex2 = 0; adjVertex2 < V; adjVertex2++) {\n","                    if(matrix[idx * V + adjVertex2] == 1 && mapping[adjVertex2] != -1) {   \n","                        hasCoveredNeighbor = 1;\n","                        break;\n","                    }\n","                }\n","\n","                if(hasCoveredNeighbor == 0) {\n","                    T[idx] = -1;\n","                    T_out[idx] = 1;\n","                }\n","            }\n","\n","        }\n","    }\n","\n","    if(isAdded == 0) {\n","        atomicExch(&T_out[node], 1);\n","    }\n","}\n","\n","void restoreStateGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int candidate, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    int value = -1;\n","\n","    int useConstMem = d_g1->numVertices + d_g2->numVertices < MAXCONSTMEM;\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_state->mapping1 + node, &value, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_state->mapping2 + candidate, &value, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping1, size1, 0, cudaMemcpyDeviceToDevice, stream1));\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping2, size2, size1, cudaMemcpyDeviceToDevice, stream2));\n","    }\n","    \n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_g1->numVertices, node, d_state->T1, d_state->T1_out, 0, useConstMem, d_state->mapping1);\n","    \n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, d_g2->numVertices, candidate, d_state->T2, d_state->T2_out, d_g1->numVertices, useConstMem, d_state->mapping2);\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","}\n","\n","/***** VF2++ FUNCTIONS *****/\n","__global__ void equalKernel(int* arr1, int* arr2, int size, int* ret) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx < size) {\n","        if (arr1[idx] != arr2[idx]) {\n","            *ret = 0;\n","        }\n","    }\n","}\n","\n","int compare(const void* a, const void* b) {\n","    return (*(int*)a - *(int*)b);\n","}\n","\n","bool checkGraphPropertiesGPU(Graph* h_g1, Graph* h_g2, Graph* d_g1, Graph* d_g2, cudaStream_t* streams) {\n","    if (h_g1->numVertices != h_g2->numVertices || h_g1->numVertices == 0 || h_g2->numVertices == 0) {\n","        return false;\n","    }\n","\n","    int h_ret1 = 1, h_ret2 = 1;\n","    int* d_ret1, *d_ret2;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    // first check: the cardinalities of the labels must be the same\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_ret1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_ret1, &h_ret1, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int gridSize1 = (LABELS + blockSize - 1) / blockSize;\n","    equalKernel<<<gridSize1, blockSize, 0, stream1>>>(d_g1->labelsCardinalities, d_g2->labelsCardinalities, LABELS, d_ret1);\n","\n","    // second check: the sequence of the degrees must be the same\n","    int size = h_g1->numVertices;\n","    int *d_tmp1, *d_tmp2;\n","\n","    int* tmp1 = (int*)malloc(size * sizeof(int));\n","    int* tmp2 = (int*)malloc(size * sizeof(int));\n","\n","    memcpy(tmp1, h_g1->degrees, size * sizeof(int));\n","    memcpy(tmp2, h_g2->degrees, size * sizeof(int));\n","    \n","    qsort(tmp1, size, sizeof(int), compare);\n","    qsort(tmp2, size, sizeof(int), compare);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_tmp1, size * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_tmp2, size * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_ret2, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_tmp1, tmp1, size * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_tmp2, tmp2, size * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_ret2, &h_ret2, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","   \n","    int gridSize2 = (size + blockSize - 1) / blockSize;\n","    equalKernel<<<gridSize2, blockSize, 0, stream2>>>(d_tmp1, d_tmp2, size, d_ret2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_ret1, d_ret1, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_ret2, d_ret2, sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    cudaFree(d_ret1);\n","    cudaFree(d_ret2);\n","    cudaFree(d_tmp2);\n","    cudaFree(d_tmp1);\n","    \n","    free(tmp1);\n","    free(tmp2);\n","\n","    return h_ret1 && h_ret2;\n","}\n","\n","__global__ void bfsKernel(int* matrix, int V, int* levels, int* d_done, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    __shared__ int s_done;\n","    extern __shared__ int s_levels[];\n","\n","    if(threadIdx.x == 0) {\n","        s_done = 0;\n","    }\n","\n","    // it copies the whole levels array in shared memory\n","    // example: thread 0 loads levels[0] and levels[4] if blockSize is 4\n","    for(int i = threadIdx.x; i < V; i += blockDim.x) {\n","      s_levels[i] = levels[i];\n","    }\n","\n","    __syncthreads();\n","\n","    // levels is used as visited too\n","    if(idx < V && s_levels[idx] == depth) {    // it blocks all thread with size greater than V and all threads not at the current depth\n","\n","        for(int adjVertex = 0; adjVertex < V; adjVertex++) {\n","            if(matrix[idx * V + adjVertex] == 1 && s_levels[adjVertex] == -1) {\n","                levels[adjVertex] = depth + 1;  // depth is a \"constant\" so no need to use atomicExch\n","                s_done = 1;\n","            }\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0 && s_done) {\n","        *d_done = 1;\n","    }\n","}\n","\n","__global__ void maxRarityConstMemKernel(int V, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = constMem[d_nodesToLabel[idx]];    // constMem contains labelRarity\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityConstMemFilterKernel(int V, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(constMem[d_nodesToLabel[idx]] != *d_maxRarity) {     // constMem contains labelRarity\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void maxDegreeKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_degrees[idx];\n","    }\n","    else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxDegree, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxDegreeFilterKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_degrees[idx] != *d_maxDegree) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void findNodeKernel(int V, int* is_good, int* d_node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if(idx < V && is_good[idx]) {\n","        atomicExch(d_node, idx);    // we don't know which thread will be the first to find the node but are equivalent\n","    }\n","}\n","\n","void findRootGPU(Graph* d_g, int* d_root, int* d_is_good, int* d_maxRarity, int* d_maxDegree, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    int h_maxRarity = INF, h_maxDegree = -INF;\n","    int gridSize = (d_g->numVertices + blockSize - 1) / blockSize;\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_g->numVertices, 1);\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxRarity, &h_maxRarity, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxDegree, &h_maxDegree, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_g->labelsCardinalities, size2, 0, cudaMemcpyDeviceToDevice, stream2)); // constMem contains labelRarity (d_g1->labelsCardinalities)\n","\n","    size_t sharedMemSize = blockSize * sizeof(int); // each block has a shared memory of size blockSize\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    maxRarityConstMemKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","    maxRarityConstMemFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","    maxDegreeKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","    maxDegreeFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","    findNodeKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_is_good, d_root);\n","}\n","\n","__global__ void findLevelNodesKernel(int* levels, int depth, int* levelNodes, int V, int* levelSize) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    __shared__ int s_levelSize;\n","\n","    if(threadIdx.x == 0) {\n","        s_levelSize = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < V && levels[idx] == depth) {\n","        atomicAdd(&s_levelSize, 1);\n","        levelNodes[idx] = 1;\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {\n","        atomicAdd(levelSize, s_levelSize);\n","    }\n","}\n","\n","__global__ void initBfsKernel(int* levels, int V, int* root, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    \n","    if(idx < V) {\n","        if(idx == *root) {\n","            levels[idx] = depth;\n","        }\n","        else {\n","            levels[idx] = -1;\n","        }\n","    }\n","}\n","\n","int* orderingGPU(Graph* d_g1, Graph* d_g2, cudaStream_t* streams, Graph* h_g1) {\n","    // findRootGPU\n","    int *d_root, *d_is_good, *d_maxRarity, *d_maxDegree;\n","\n","    // BFS\n","    int* d_levels, *d_done;\n","    int* h_done;\n","    int depth = 0;\n","\n","    // findLevelNodesKernel\n","    int *d_levelNodes, *d_levelSize;\n","\n","    // processDepth\n","    int *d_V1Unordered, *d_labelRarity, *d_connectivityG1, *d_maxConnectivity;  // d_root is reused as d_nextNode\n","    int* order = (int*)malloc(d_g1->numVertices * sizeof(int));   // order of the nodes of g1\n","    int order_index = 0;\n","\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_root, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_is_good, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxDegree, sizeof(int)));\n","    \n","    findRootGPU(d_g1, d_root, d_is_good, d_maxRarity, d_maxDegree, streams);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levels, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_done, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_done, sizeof(int))); // pinned memory for faster host-device communication\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelNodes, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelSize, sizeof(int)));\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_V1Unordered, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelRarity, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_connectivityG1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxConnectivity, sizeof(int)));\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream2>>>(d_V1Unordered, d_g1->numVertices, -1); // stream2 empty because already sync in findRootGPU\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_labelRarity, d_g1->labelsCardinalities, size2, cudaMemcpyDeviceToDevice, stream3));\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_connectivityG1, 0, size1, stream4));\n","\n","    initBfsKernel<<<gridSize, blockSize, 0, stream1>>>(d_levels, d_g1->numVertices, d_root, depth); // executed after findRootGPU because of same stream\n","    \n","    size_t sharedMemSize = size1;\n","\n","    do {\n","        *h_done = 0;\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_done, 0, sizeof(int), stream1));\n","\n","        bfsKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g1->matrix, d_g1->numVertices, d_levels, d_done, depth);\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_done, d_done, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","        depth++;\n","       \n","        cudaStreamSynchronize(stream1);\n","    } while(*h_done);\n","  \n","    cudaStreamSynchronize(stream2); // bfs at stream1 is implicitly synchronized \n","    cudaStreamSynchronize(stream3);\n","    cudaStreamSynchronize(stream4);\n","    \n","    for (int d = 0; d < depth; d++) {\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_levelNodes, 0, size1, stream2));\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_levelSize, 0, sizeof(int), stream1));\n","\n","        cudaStreamSynchronize(stream2);\n","        \n","        findLevelNodesKernel<<<gridSize, blockSize, 0, stream1>>>(d_levels, d, d_levelNodes, d_g1->numVertices, d_levelSize);\n","        \n","        processDepthGPU(d_g1, order, &order_index, d_connectivityG1, d_labelRarity, d_V1Unordered, d_levelNodes, d_levelSize, \n","                        d_is_good, d_maxRarity, d_maxDegree, d_maxConnectivity, d_root, streams, h_g1);\n","    }\n","\n","    // findRootGPU\n","    cudaFree(d_root);\n","    cudaFree(d_is_good);\n","    cudaFree(d_maxRarity);\n","    cudaFree(d_maxDegree);\n","\n","    //BFS\n","    cudaFreeHost(h_done);\n","    cudaFree(d_done);\n","    cudaFree(d_levels);\n","    \n","    // findLevelNodesKernel\n","    cudaFree(d_levelNodes);\n","    cudaFree(d_levelSize);\n","\n","    // processDepth\n","    cudaFree(d_V1Unordered);\n","    cudaFree(d_labelRarity);\n","    cudaFree(d_connectivityG1);\n","    cudaFree(d_maxConnectivity);\n","\n","    return order;\n","}\n","\n","__global__ void maxConnectivityKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {         // d_is_good already contains the information about the nodes of the current level\n","        int conn = d_connectivityG1[idx];\n","        s_data[threadIdx.x] = conn;\n","    } else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxConnectivity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxConnectivityFilterKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_connectivityG1[idx] != *d_maxConnectivity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void unorderedFilterKernel(int* d_is_good, int* d_V1Unordered, int* d_levelNodes, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= V) {\n","        return;\n","    }\n","\n","    if(d_levelNodes[idx]) {\n","        if(d_V1Unordered[idx] == 1) {\n","            d_is_good[idx] = 0;\n","        }\n","    } else {\n","        d_is_good[idx] = 0;\n","    }\n","}\n","\n","__global__ void updateConnKernel(int* matrix, int V, int* connectivity, int node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(matrix[node * V + idx] == 1) {\n","            connectivity[idx]++;\n","        }\n","    }\n","}\n","\n","// we need to update labelRarity so we cannot use constMem in order to avoid overhead\n","__global__ void maxRarityKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_labelRarity[d_nodesToLabel[idx]];\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityFilterKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_labelRarity[d_nodesToLabel[idx]] != *d_maxRarity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","void processDepthGPU(Graph* d_g, int* order, int* order_index, int* d_connectivityG1, int* d_labelRarity, int* d_V1Unordered, int* d_levelNodes,\n","                    int* d_levelSize, int* d_is_good, int* d_maxRarity, int* d_maxDegree, int* d_maxConnectivity, int* d_nextNode, cudaStream_t* streams,\n","                    Graph* h_g) {\n","    \n","    size_t size1 = d_g->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    int h_levelSize;\n","    int* h_nodesToLabel = h_g->nodesToLabel;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_levelSize, d_levelSize, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","    // pinned memory for faster host-device communication\n","    int *h_maxRarity, *h_maxDegree, *h_maxConnectivity, *h_nextNode; \n","    int* h_labelRarity, *h_V1Unordered;\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxDegree, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxConnectivity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_nextNode, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_labelRarity, size2));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_V1Unordered, size1));       \n","\n","    *h_maxRarity = INF; *h_maxDegree = -INF; *h_maxConnectivity = -INF;\n","\n","    int gridSize = (d_g->numVertices + blockSize - 1) / blockSize;\n","    size_t sharedMemSize = blockSize * sizeof(int);\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    while(h_levelSize > 0) {\n","        initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_g->numVertices, 1);\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxRarity, h_maxRarity, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxDegree, h_maxDegree, sizeof(int), cudaMemcpyHostToDevice, stream3));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxConnectivity, h_maxConnectivity, sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","        cudaStreamSynchronize(stream4);\n","\n","        unorderedFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_V1Unordered, d_levelNodes, d_g->numVertices);\n","        maxConnectivityKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_connectivityG1, d_maxConnectivity, d_is_good, d_g->numVertices);\n","        maxConnectivityFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_connectivityG1, d_maxConnectivity, d_is_good, d_g->numVertices);\n","        maxDegreeKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);   \n","        maxDegreeFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","        maxRarityKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_labelRarity, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","        maxRarityFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_labelRarity, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","        findNodeKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_is_good, d_nextNode);\n","        \n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_nextNode, d_nextNode, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_labelRarity, d_labelRarity, size2, cudaMemcpyDeviceToHost, stream2));        \n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_V1Unordered, d_V1Unordered, size1, cudaMemcpyDeviceToHost, stream3)); \n","        \n","        cudaStreamSynchronize(stream1);\n","        \n","        updateConnKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->matrix, d_g->numVertices, d_connectivityG1, *h_nextNode);\n","        order[(*order_index)++] = *h_nextNode;\n","        h_levelSize--;\n","\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","\n","        h_labelRarity[h_nodesToLabel[*h_nextNode]]--;\n","        h_V1Unordered[*h_nextNode] = 1;\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_labelRarity, h_labelRarity, size2, cudaMemcpyHostToDevice, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_V1Unordered, h_V1Unordered, size1, cudaMemcpyHostToDevice, stream3));\n","\n","        cudaStreamSynchronize(stream1);\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","    }\n","\n","    cudaFreeHost(h_maxRarity);\n","    cudaFreeHost(h_maxDegree);\n","    cudaFreeHost(h_maxConnectivity);\n","    cudaFreeHost(h_nextNode);\n","    cudaFreeHost(h_labelRarity);\n","    cudaFreeHost(h_V1Unordered);\n","}\n","\n","__global__ void findCoveredNeighborsKernel(int* matrix1, int* mapping1, int node, int* coveredNeighbors, int* coveredNeighborsSize,\n","                                            int numVertices) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= numVertices)\n","        return;\n","\n","    if(matrix1[node * numVertices + idx] == 1 && mapping1[idx] != -1) {\n","        int index = atomicAdd(coveredNeighborsSize, 1);\n","        coveredNeighbors[index] = idx;\n","    }\n","}\n","\n","__global__ void findCandidatesKernel(int g1_label, int maxSizeCandidates, int* g2_vertexList, int g1_degree, int* g2_degrees, int* T2_out, \n","                                    int* mapping2, int* candidates, int* candidateSize, int g2_numVertices, int* commonNodes, int* g2_matrix, \n","                                    int* g2_nodesToLabel, int offset, int useConstMem, int* coveredNeighbors, int* mapping1) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(constMemVar == 0) {  // constMemVar contains coveredNeighborsSize\n","\n","        if(idx < maxSizeCandidates) {\n","            int vertex = g2_vertexList[idx];  // g2_labelToNodes[label]\n","\n","            if(g2_degrees[vertex] == g1_degree && T2_out[vertex] == 1 && mapping2[vertex] == -1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = vertex;\n","            }\n","        }\n","    }\n","    else {\n","\n","        if(idx < g2_numVertices) {\n","           commonNodes[idx] = 1;\n","\n","            if(useConstMem) {\n","                \n","                for (int i = 0; i < constMemVar; i++) {\n","                    int nbrG1 = constMem[i];        // constMem contains coveredNeighbors with offset 0\n","                    int mappedG2 = constMem[offset + nbrG1]; // constMem contains mapping1 with offset degrees[node]\n","                    if (g2_matrix[mappedG2 * g2_numVertices + idx] == 0) {\n","                        commonNodes[idx] = 0;\n","                    }\n","                }\n","\n","            } else {\n","\n","                for (int i = 0; i < constMemVar; i++) {\n","                    int nbrG1 = coveredNeighbors[i];\n","                    int mappedG2 = mapping1[nbrG1];\n","                    if (g2_matrix[mappedG2 * g2_numVertices + idx] == 0) {\n","                        commonNodes[idx] = 0;\n","                    }\n","                }\n","\n","            }\n","\n","            if (commonNodes[idx] && mapping2[idx] != -1) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_degrees[idx] != g1_degree) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_nodesToLabel[idx] != g1_label) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] == 1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = idx;\n","            }\n","        }\n","    }\n","}\n","\n","int* findCandidatesGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int* sizeCandidates, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    size_t neighSize = h_g1->degrees[node] * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","\n","    int useConstMem = d_g1->numVertices + h_g1->degrees[node] < MAXCONSTMEM;\n","\n","    // writes on constMem must be sequential\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping1, size1, neighSize, cudaMemcpyDeviceToDevice, stream2));    // neighSize is the offset\n","    }\n","    \n","    int* d_coveredNeighbors, *d_coveredNeighborsSize;\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighbors, neighSize));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighborsSize, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_coveredNeighborsSize, 0, sizeof(int), stream1));\n","    \n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findCoveredNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_state->mapping1, node, d_coveredNeighbors, \n","                                d_coveredNeighborsSize, d_g1->numVertices);\n","\n","    int g1_label = h_g1->nodesToLabel[node];\n","    int g1_degree = h_g1->degrees[node];\n","    int maxSizeCandidates = h_g2->labelsCardinalities[g1_label];\n","    \n","    int* d_candidates, *d_candidateSize, *d_commonNodes;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidates, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidateSize, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_commonNodes, size2));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_candidateSize, 0, sizeof(int), stream2));\n","    \n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    \n","    cudaStreamSynchronize(stream2); // transfer of constMem in the stream2 must be finished before the second write on constMem\n","\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_coveredNeighbors, neighSize, 0, cudaMemcpyDeviceToDevice, stream1));   // it is queued before the kernel call on stream1 so implicit synchronization\n","    }\n","    \n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMemVar, d_coveredNeighborsSize, sizeof(int), 0, cudaMemcpyDeviceToDevice, stream1)); // constMemVar contains coveredNeighborsSize\n","\n","    findCandidatesKernel<<<gridSize, blockSize, 0, stream1>>>(g1_label, maxSizeCandidates, d_g2->labelToNodes[g1_label], g1_degree, \n","                            d_g2->degrees, d_state->T2_out, d_state->mapping2, d_candidates, d_candidateSize, d_g2->numVertices, \n","                            d_commonNodes, d_g2->matrix, d_g2->nodesToLabel, h_g1->degrees[node], useConstMem, d_coveredNeighbors, d_state->mapping1);\n","\n","    int* candidates = (int*)malloc(maxSizeCandidates * sizeof(int));\n","    \n","    cudaStreamSynchronize(stream1);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(candidates, d_candidates, maxSizeCandidates * sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(sizeCandidates, d_candidateSize, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    cudaFree(d_coveredNeighbors);\n","    cudaFree(d_coveredNeighborsSize);\n","\n","    cudaFree(d_candidates);\n","    cudaFree(d_candidateSize);\n","    cudaFree(d_commonNodes);\n","\n","    return candidates;\n","}\n","\n","\n","void vf2ppGPU(Graph* d_g1, Graph* d_g2, State* d_state, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    if (!checkGraphPropertiesGPU(h_g1, h_g2, d_g1, d_g2, streams)) {\n","        return;\n","    }\n","\n","    int* order = orderingGPU(d_g1, d_g2, streams, h_g1);\n","\n","     //printf(\"Order:\\t\");\n","     //for(int i = 0; i < h_g1->numVertices; i++) {\n","     //   printf(\"%d \", order[i]);\n","     //}\n","     //printf(\"\\n\");\n","\n","    int sizeCandidates = 0;\n","    int* candidates = findCandidatesGPU(d_g1, d_g2, d_state, order[0], &sizeCandidates, h_g1, h_g2, streams);\n","\n","    StackNode* stack = createStack();\n","    Info* info = createInfo(candidates, sizeCandidates, order[0]);\n","    push(&stack, info);\n","\n","    int matchingNode = 1;\n","    while (!isStackEmpty(stack)) {\n","        Info* info = peek(&stack);\n","        bool isMatch = false;\n","\n","        // printInfo(info);\n","\n","        for(int i = info->candidateIndex; i < info->sizeCandidates; i++) {\n","            int candidate = info->candidates[i];\n","            info->candidateIndex = i + 1;\n","\n","            int ret = cutISOGPU(d_g1, d_g2, d_state, info->vertex, candidate, h_g1, h_g2, streams);\n","\n","            // printf(\"CutISO: %d\\n\", ret);\n","            // printf(\"\\n\");\n","\n","            if(!ret) {\n","                // printf(\"\\nMatch %d -> %d\\n\", info->vertex, candidate);\n","\n","                updateStateGPU(d_g1, d_g2, d_state, info->vertex, candidate, streams);\n","\n","                if(matchingNode >= d_g1->numVertices) {\n","                    freeStack(stack);\n","                    free(order);\n","                    printf(\"Graphs are isomorphic\\n\");\n","                    cudaStreamSynchronize(streams[0]);  // wait for updates on the state\n","                    cudaStreamSynchronize(streams[1]);\n","                    return;\n","                }\n","\n","                cudaStreamSynchronize(streams[0]);\n","                cudaStreamSynchronize(streams[1]);\n","\n","                candidates = findCandidatesGPU(d_g1, d_g2, d_state, order[matchingNode], &sizeCandidates, h_g1, h_g2, streams);\n","                Info* info = createInfo(candidates, sizeCandidates, order[matchingNode]);\n","                push(&stack, info);\n","                matchingNode++;\n","                isMatch = true;\n","                break;\n","            }\n","        }\n","\n","        // no more candidates\n","        if(!isMatch) {\n","            Info* tmp = pop(&stack);\n","            freeInfo(tmp);\n","            matchingNode--;\n","\n","            // backtracking\n","            if(!isStackEmpty(stack)) {\n","                Info* prevInfo = peek(&stack);\n","                int candidate = prevInfo->candidates[prevInfo->candidateIndex - 1];\n","                restoreStateGPU(d_g1, d_g2, d_state, prevInfo->vertex, candidate, streams);\n","            }\n","        }\n","    }\n","    free(order);\n","    freeStack(stack);   \n","}\n","\n","__global__ void findNeighborsKernel(int* matrix, int node, int* neighbors, int* size, int numVertices) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < numVertices) {\n","        if(matrix[node * numVertices + idx] == 1) {\n","            int index = atomicAdd(size, 1);\n","            neighbors[index] = idx;\n","        }\n","    }\n","}\n","\n","__global__ void checkLabelsKernel(int* neighbors1, int nbrSize1, int nbrSize2, int* labelsNbr, int numVertices,\n","    int* g1_nodesToLabel, int* d_result, int offset, int usage, int* neighbors2, int* g2_nodesToLabel) {   // idx is the id of the thread in the grid\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < nbrSize1) {\n","        int nbr1 = neighbors1[idx];\n","        int labelNbr1 = g1_nodesToLabel[nbr1];\n","        bool found = false;\n","\n","        if(usage) {\n","\n","            for(int i = 0; i < nbrSize2; i++) {\n","                int nbr2 = constMem[i];     // constMem contains the neighbors2 with offset 0\n","                if(labelNbr1 == constMem[offset + nbr2]) {  // constMem contains the g2_nodesToLabel with offset degrees[node]\n","                    found = true;\n","                    labelsNbr[labelNbr1] = 1;\n","                    break;\n","                }\n","            }\n","\n","        } else {\n","\n","            for(int i = 0; i < nbrSize2; i++) {\n","                int nbr2 = neighbors2[i];\n","                if(labelNbr1 == g2_nodesToLabel[nbr2]) {\n","                    found = true;\n","                    labelsNbr[labelNbr1] = 1;\n","                    break;\n","                }\n","            }\n","        }\n","\n","        if(!found) {\n","            *d_result = 0;   // d_result is initialized to 1 by default\n","        }\n","    }\n","}\n","\n","__global__ void findNodesOfLabelKernel(int* neighbors, int* g_nodesToLabel, int label, int maxSize, int* size, int* nodes) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < maxSize) {\n","        int vertex = neighbors[idx];\n","\n","        if(g_nodesToLabel[vertex] == label) {\n","            int index = atomicAdd(size, 1);   // atomicAdd returns the index respect to global memory (so can't be used shared memory)\n","            nodes[index] = vertex;\n","        }\n","    }\n","}\n","\n","__global__ void intersectionCountKernel(int* nodes, int* size, int* stateSet, int* count) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;    // idx is the id of the thread in the grid among all threads\n","    __shared__ int localCount;  // each block has own localCount variable (shared memory)\n","\n","    if(threadIdx.x == 0) {   // only one thread in the block initializes the localCount\n","        localCount = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < *size) {\n","        int vertex = nodes[idx];\n","\n","        if(stateSet[vertex] == 1) {\n","            atomicAdd(&localCount, 1);\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {  // only the thread with id 0 of each block updates the global size\n","        atomicAdd(count, localCount);\n","    }\n","}\n","\n","bool cutISOGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node1, int node2, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","    cudaStream_t stream5 = streams[4];\n","    cudaStream_t stream6 = streams[5];\n","\n","    int nbrSize1 = h_g1->degrees[node1];\n","    int nbrSize2 = h_g2->degrees[node2];\n","\n","    size_t nbrSize1_bytes = nbrSize1 * sizeof(int);\n","    size_t nbrSize2_bytes = nbrSize2 * sizeof(int);\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    size_t labelSize = LABELS * sizeof(int);\n","\n","    int useConstMem = d_g2->numVertices + nbrSize2 < MAXCONSTMEM;\n","\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_g2->nodesToLabel, size2, nbrSize2_bytes, cudaMemcpyDeviceToDevice, stream3)); // constMem contains g2_nodesToLabel\n","    }\n","\n","    int* d_neighbors1, *d_neighbors2;\n","    int* d_nbrSize1, *d_nbrSize2;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors2, nbrSize2_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize2, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize2, 0, sizeof(int), stream2));\n","   \n","    int gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, node2, d_neighbors2, d_nbrSize2, d_g2->numVertices);\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors1, nbrSize1_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize1, sizeof(int)));\n","    \n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize1, 0, sizeof(int), stream1));\n","    \n","    gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, node1, d_neighbors1, d_nbrSize1, d_g1->numVertices);\n","\n","    int* d_labelsNbr;\n","    int* d_result;\n","    int result = 1;                               \n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelsNbr, labelSize));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_result, sizeof(int)));\n","    \n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_labelsNbr, 0, labelSize, stream4));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_result, &result, sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","    gridSize = (nbrSize1 + blockSize - 1) / blockSize;\n","\n","    cudaStreamSynchronize(stream3); // the first write on constMem must be finished before the second write on constMem\n","    \n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_neighbors2, nbrSize2_bytes, 0, cudaMemcpyDeviceToDevice, stream2)); // constMem contains neighbors2. it is queued after findNeighborsKernel on stream2 \n","    }\n","\n","    cudaStreamSynchronize(stream4);\n","    cudaStreamSynchronize(stream2);\n","\n","    checkLabelsKernel<<<gridSize, blockSize, 0, stream1>>>(d_neighbors1, nbrSize1, nbrSize2, d_labelsNbr, d_g1->numVertices, \n","                        d_g1->nodesToLabel, d_result, nbrSize2, useConstMem, d_neighbors2, d_g2->nodesToLabel);\n","\n","    int* labelsNbr = (int*)malloc(labelSize);\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&result, d_result, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(labelsNbr, d_labelsNbr, labelSize, cudaMemcpyDeviceToHost, stream1));\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    if(result == 0) {\n","        cudaFree(d_neighbors1);\n","        cudaFree(d_neighbors2);\n","        cudaFree(d_nbrSize1);\n","        cudaFree(d_nbrSize2);\n","        cudaFree(d_labelsNbr);\n","        cudaFree(d_result);\n","        free(labelsNbr);\n","        return true;\n","    }\n","\n","    // pinned memory for faster host-device communication\n","    int *h_size1, *h_size2;\n","    int *h_count1, *h_count2, *h_count3, *h_count4;\n","    int *d_count1, *d_count2, *d_count3, *d_count4;\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_size1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_size2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count4, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count4, sizeof(int)));\n","\n","    int *d_nodes_g1, *d_size_g1;\n","    int *d_nodes_g2, *d_size_g2;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g1, nbrSize1_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g2, nbrSize2_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g2, sizeof(int)));\n","\n","    int gridSize1 = (nbrSize1 + blockSize - 1) / blockSize;\n","    int gridSize2 = (nbrSize2 + blockSize - 1) / blockSize;\n","   \n","    bool ret = false;\n","    for(int label = 0; label < LABELS; label++) {\n","        if(labelsNbr[label] == 1) {\n","\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g1, 0, sizeof(int), stream1));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g2, 0, sizeof(int), stream2));\n","\n","            findNodesOfLabelKernel<<<gridSize1, blockSize, 0, stream1>>>(d_neighbors1, d_g1->nodesToLabel, label, nbrSize1, d_size_g1, d_nodes_g1);\n","            findNodesOfLabelKernel<<<gridSize2, blockSize, 0, stream2>>>(d_neighbors2, d_g2->nodesToLabel, label, nbrSize2, d_size_g2, d_nodes_g2);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_size1, d_size_g1, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_size2, d_size_g2, sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count1, 0, sizeof(int), stream3));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count2, 0, sizeof(int), stream4));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count3, 0, sizeof(int), stream5));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count4, 0, sizeof(int), stream6));\n","\n","            cudaStreamSynchronize(stream1);\n","            cudaStreamSynchronize(stream2);\n","\n","            gridSize = (*h_size1 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream3>>>(d_nodes_g1, d_size_g1, d_state->T1, d_count1);\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream5>>>(d_nodes_g1, d_size_g1, d_state->T1_out, d_count3);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count1, d_count1, sizeof(int), cudaMemcpyDeviceToHost, stream3));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count3, d_count3, sizeof(int), cudaMemcpyDeviceToHost, stream5));\n","            \n","            gridSize = (*h_size2 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream4>>>(d_nodes_g2, d_size_g2, d_state->T2, d_count2);\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream6>>>(d_nodes_g2, d_size_g2, d_state->T2_out, d_count4);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count2, d_count2, sizeof(int), cudaMemcpyDeviceToHost, stream4));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count4, d_count4, sizeof(int), cudaMemcpyDeviceToHost, stream6));\n","           \n","            cudaStreamSynchronize(stream3);\n","            cudaStreamSynchronize(stream4);\n","            cudaStreamSynchronize(stream5);\n","            cudaStreamSynchronize(stream6);\n","\n","            if(*h_count1 != *h_count2 || *h_count3 != *h_count4) {\n","                ret = true;\n","                break;\n","            }\n","        }\n","    }\n","\n","    cudaFree(d_neighbors1);\n","    cudaFree(d_neighbors2);\n","    cudaFree(d_nbrSize1);\n","    cudaFree(d_nbrSize2);\n","    cudaFree(d_labelsNbr);\n","    cudaFree(d_result);\n","    free(labelsNbr);\n","\n","    cudaFreeHost(h_size1);\n","    cudaFreeHost(h_size2);\n","    cudaFreeHost(h_count1);\n","    cudaFreeHost(h_count2);\n","    cudaFreeHost(h_count3);\n","    cudaFreeHost(h_count4);\n","    \n","    cudaFree(d_count1);\n","    cudaFree(d_count2);\n","    cudaFree(d_count3);\n","    cudaFree(d_count4);\n","\n","    cudaFree(d_nodes_g1);\n","    cudaFree(d_size_g1);\n","    cudaFree(d_nodes_g2);\n","    cudaFree(d_size_g2);\n","\n","    return ret;\n","}\n","\n","/***** STACK FUNCTIONS *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex) {\n","    Info* info = (Info*)malloc(sizeof(Info));\n","    if (info == NULL) {\n","        printf(\"Error allocating memory in createInfo\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    info->vertex = vertex;\n","    info->candidates = (int*)realloc(candidates, sizeCandidates * sizeof(int));\n","    info->sizeCandidates = sizeCandidates;\n","    info->candidateIndex = 0;\n","    return info;\n","}\n","\n","StackNode* createStackNode(Info* info) {\n","    StackNode* node = (StackNode*)malloc(sizeof(StackNode));\n","    if (node == NULL) {\n","        printf(\"Error allocating memory in createStackNode\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    node->info = info;\n","    node->next = NULL;\n","    return node;\n","}\n","\n","void push(StackNode** top, Info* info) {\n","    StackNode* node = createStackNode(info);\n","    node->next = *top;\n","    *top = node;\n","}\n","\n","Info* pop(StackNode** top) {\n","    if (isStackEmpty(*top)) {\n","        printf(\"Stack is empty, cannot pop\\n\");\n","        return NULL;\n","    }\n","    StackNode* node = *top;\n","    Info* info = node->info;\n","    *top = node->next;\n","    free(node);\n","    return info;\n","}\n","\n","bool isStackEmpty(StackNode* top) {\n","    return top == NULL;\n","}\n","\n","void freeStack(StackNode* top) {\n","    while (!isStackEmpty(top)) {\n","        StackNode* node = top;\n","        top = top->next;\n","        freeInfo(node->info);\n","        free(node);\n","    }\n","}\n","\n","void printStack(StackNode* top) {\n","    StackNode* current = top;\n","    while (current != NULL) {\n","        printInfo(current->info);\n","        current = current->next;\n","    }\n","}\n","\n","void printInfo(Info* info) {\n","    printf(\"\\nVertex: %d\\n\", info->vertex);\n","    printf(\"Index seen: %d\\n\", info->candidateIndex);\n","    printf(\"Candidates: \");\n","    for (int i = 0; i < info->sizeCandidates; i++) {\n","        printf(\"%d \", info->candidates[i]);\n","    }\n","    printf(\"\\n\");\n","}\n","\n","void freeInfo(Info* info) {\n","    free(info->candidates);\n","    free(info);\n","}\n","\n","StackNode* createStack() {\n","    return NULL;\n","}\n","\n","Info* peek(StackNode** top) {\n","    return (*top)->info;\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
