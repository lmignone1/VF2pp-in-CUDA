{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25510,"status":"ok","timestamp":1721040086692,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"OVKsPm9V3gQL","outputId":"f24c01b7-ff4a-4757-a3c4-8e79462e03bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","os.chdir(\"drive/Othercomputers/pc/VF2pp-in-CUDA/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1720434007641,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"_IbccJNAcjIa","outputId":"aaf4d3fd-4a6c-4807-fc50-c9a6cb0d82ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm -f src/lib/stack.o src/lib/queue.o src/lib/graph.o src/lib/state.o src/vf2pp_sequential.o vf2pp_sequential\n"]}],"source":["os.listdir()\n","!make clean"]},{"cell_type":"markdown","metadata":{"id":"h498tzxnwDyi"},"source":["CUDA SETUP"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158582,"status":"ok","timestamp":1721040245271,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"rbh20Z5utmFQ","outputId":"5d060cba-a2ea-4e61-ed1a-5a8ae59af0ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nvcc4jupyter\n","  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n","Installing collected packages: nvcc4jupyter\n","Successfully installed nvcc4jupyter-1.2.1\n","Collecting pycuda\n","  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytools>=2011.2 (from pycuda)\n","  Downloading pytools-2024.1.8-py3-none-any.whl (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Collecting mako (from pycuda)\n","  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.2 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n","Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n","  Downloading siphash24-1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n","Building wheels for collected packages: pycuda\n","  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661204 sha256=953fe5009c67072366cb6eff4fd0bbc258adb80a2114b0234a8ed463b685e5ec\n","  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n","Successfully built pycuda\n","Installing collected packages: appdirs, siphash24, mako, pytools, pycuda\n","Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.8 siphash24-1.6\n"]}],"source":["!pip install nvcc4jupyter\n","!pip install pycuda"]},{"cell_type":"markdown","metadata":{"id":"8_P-CD0hwSmY"},"source":["GPU TYPE"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":599,"status":"ok","timestamp":1721040245861,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"ntofwbc0tEKS","outputId":"5d94fdf9-b6b2-4929-ddc7-46b0a1973c4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Jul 15 10:44:04 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   49C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp8oehhmhx\".\n"]}],"source":["!nvidia-smi\n","!nvcc --version\n","%load_ext nvcc4jupyter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1720436857013,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"KfQ4iKK_p0hY","outputId":"67de1cce-ea1d-4a9f-e6a6-9b17047575aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 device(s) found.\n","Device #0: Tesla T4\n"," Compute Capability: 7.5\n"," Total Memory: 14 GB\n"]}],"source":["import pycuda.driver as drv\n","import pycuda.autoinit\n","drv.init()\n","print(\"%d device(s) found.\" % drv.Device.count())\n","for i in range(drv.Device.count()):\n","  dev = drv.Device(i)\n","  print(\"Device #%d: %s\" % (i, dev.name()))\n","  print(\" Compute Capability: %d.%d\" % dev.compute_capability())\n","  print(\" Total Memory: %s GB\" % (dev.total_memory() // (1024 * 1024 * 1024)))"]},{"cell_type":"markdown","metadata":{"id":"8X72on3Cwj7C"},"source":["GPU INFO"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1796,"status":"ok","timestamp":1721043130771,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"u9x2_pq4wwyI","outputId":"2b483e5f-4f73-4759-d15c-4260249dd14d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device number: 0\n","  Device name: Tesla T4\n","  Compute capability: 7.5\n","\n","  Clock Rate: 1590000 kHz\n","  Total SMs: 40 \n","  Shared Memory Per SM: 65536 bytes\n","  Registers Per SM: 65536 32-bit\n","  Max threads per SM: 1024\n","  L2 Cache Size: 4194304 bytes\n","  Total Global Memory: 15835660288 bytes\n","  Memory Clock Rate: 5001000 kHz\n","\n","  Max threads per block: 1024\n","  Max threads in X-dimension of block: 1024\n","  Max threads in Y-dimension of block: 1024\n","  Max threads in Z-dimension of block: 64\n","\n","  Max blocks in X-dimension of grid: 2147483647\n","  Max blocks in Y-dimension of grid: 65535\n","  Max blocks in Z-dimension of grid: 65535\n","\n","  Shared Memory Per Block: 49152 bytes\n","  Registers Per Block: 65536 32-bit\n","  Warp size: 32\n","\n"," Constant Memory: 65536\n","\n"]}],"source":["%%cuda\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","\n","void deviceQuery()\n","{\n","  cudaDeviceProp prop;\n","  int nDevices=0, i;\n","  cudaError_t ierr;\n","\n","  ierr = cudaGetDeviceCount(&nDevices);\n","  if (ierr != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierr)); }\n","\n","\n","\n","  for( i = 0; i < nDevices; ++i )\n","  {\n","     ierr = cudaGetDeviceProperties(&prop, i);\n","     printf(\"Device number: %d\\n\", i);\n","     printf(\"  Device name: %s\\n\", prop.name);\n","     printf(\"  Compute capability: %d.%d\\n\\n\", prop.major, prop.minor);\n","\n","     printf(\"  Clock Rate: %d kHz\\n\", prop.clockRate);\n","     printf(\"  Total SMs: %d \\n\", prop.multiProcessorCount);\n","     printf(\"  Shared Memory Per SM: %lu bytes\\n\", prop.sharedMemPerMultiprocessor);\n","     printf(\"  Registers Per SM: %d 32-bit\\n\", prop.regsPerMultiprocessor);\n","     printf(\"  Max threads per SM: %d\\n\", prop.maxThreadsPerMultiProcessor);\n","     printf(\"  L2 Cache Size: %d bytes\\n\", prop.l2CacheSize);\n","     printf(\"  Total Global Memory: %lu bytes\\n\", prop.totalGlobalMem);\n","     printf(\"  Memory Clock Rate: %d kHz\\n\\n\", prop.memoryClockRate);\n","\n","\n","     printf(\"  Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n","     printf(\"  Max threads in X-dimension of block: %d\\n\", prop.maxThreadsDim[0]);\n","     printf(\"  Max threads in Y-dimension of block: %d\\n\", prop.maxThreadsDim[1]);\n","     printf(\"  Max threads in Z-dimension of block: %d\\n\\n\", prop.maxThreadsDim[2]);\n","\n","     printf(\"  Max blocks in X-dimension of grid: %d\\n\", prop.maxGridSize[0]);\n","     printf(\"  Max blocks in Y-dimension of grid: %d\\n\", prop.maxGridSize[1]);\n","     printf(\"  Max blocks in Z-dimension of grid: %d\\n\\n\", prop.maxGridSize[2]);\n","\n","     printf(\"  Shared Memory Per Block: %lu bytes\\n\", prop.sharedMemPerBlock);\n","     printf(\"  Registers Per Block: %d 32-bit\\n\", prop.regsPerBlock);\n","     printf(\"  Warp size: %d\\n\\n\", prop.warpSize);\n","\n","     printf(\" Constant Memory: %d\\n\", prop.totalConstMem);\n","\n","  }\n","}\n","\n","int main() {\n","    deviceQuery();\n","}"]},{"cell_type":"code","source":["!nvcc -Xptxas=\"-v\" -o vf2_paralell temp.cu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JW4DaHbTCWIZ","executionInfo":{"status":"ok","timestamp":1721044223050,"user_tz":-120,"elapsed":4195,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"}},"outputId":"f87d2591-a5e8-4d5f-f455-8147ae1c1ed8"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01m\u001b[0m\u001b[01mtemp.cu(101)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n","      Graph* h_g1 = readGraph(\"data/graph_query_10000.csv\");\n","                              ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mtemp.cu(102)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n","      Graph* h_g2 = readGraph(\"data/graph_target_10000.csv\");\n","                              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mtemp.cu(101)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n","      Graph* h_g1 = readGraph(\"data/graph_query_10000.csv\");\n","                              ^\n","\n","\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","\u001b[01m\u001b[0m\u001b[01mtemp.cu(102)\u001b[0m: \u001b[01;35mwarning\u001b[0m #2464-D: conversion from a string literal to \"char *\" is deprecated\n","      Graph* h_g2 = readGraph(\"data/graph_target_10000.csv\");\n","                              ^\n","\n","\u001b[01m\u001b[0m\u001b[01mtemp.cu(1402)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"size1\"\u001b[0m was declared but never referenced\n","      size_t size1 = d_g1->numVertices * sizeof(int);\n","             ^\n","\n","ptxas info    : 0 bytes gmem, 64004 bytes cmem[3]\n","ptxas info    : Compiling entry function '_Z23intersectionCountKernelPiS_S_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z23intersectionCountKernelPiS_S_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 7 registers, 16 bytes smem, 352 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z22findNodesOfLabelKernelPiS_iiS_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z22findNodesOfLabelKernelPiS_iiS_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 10 registers, 360 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z17checkLabelsKernelPiiiS_iS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z17checkLabelsKernelPiiiS_iS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 372 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z19findNeighborsKernelPiiS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z19findNeighborsKernelPiiS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 10 registers, 356 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z20findCandidatesKerneliiPiiS_S_S_S_S_iS_S_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z20findCandidatesKerneliiPiiS_S_S_S_S_iS_S_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 17 registers, 420 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z26findCoveredNeighborsKernelPiS_iS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z26findCoveredNeighborsKernelPiS_iS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 10 registers, 364 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z21maxRarityFilterKerneliPiS_S_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z21maxRarityFilterKerneliPiS_S_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 8 registers, 360 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z15maxRarityKerneliPiS_S_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z15maxRarityKerneliPiS_S_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 8 registers, 360 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z16updateConnKernelPiiS_i' for 'sm_52'\n","ptxas info    : Function properties for _Z16updateConnKernelPiiS_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 348 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z21unorderedFilterKernelPiS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z21unorderedFilterKernelPiS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 348 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z27maxConnectivityFilterKernelPiS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z27maxConnectivityFilterKernelPiS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 348 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z21maxConnectivityKernelPiS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z21maxConnectivityKernelPiS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 348 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z13initBfsKernelPiiS_i' for 'sm_52'\n","ptxas info    : Function properties for _Z13initBfsKernelPiiS_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 7 registers, 348 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z20findLevelNodesKernelPiiS_iS_' for 'sm_52'\n","ptxas info    : Function properties for _Z20findLevelNodesKernelPiiS_iS_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 7 registers, 16 bytes smem, 360 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z14findNodeKerneliPiS_' for 'sm_52'\n","ptxas info    : Function properties for _Z14findNodeKerneliPiS_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 4 registers, 344 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z21maxDegreeFilterKerneliPiS_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z21maxDegreeFilterKerneliPiS_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 352 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z15maxDegreeKerneliPiS_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z15maxDegreeKerneliPiS_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 352 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z29maxRarityConstMemFilterKerneliPiS_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z29maxRarityConstMemFilterKerneliPiS_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 8 registers, 352 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z23maxRarityConstMemKerneliPiS_S_' for 'sm_52'\n","ptxas info    : Function properties for _Z23maxRarityConstMemKerneliPiS_S_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 7 registers, 352 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z9bfsKernelPiiS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z9bfsKernelPiiS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 12 registers, 16 bytes smem, 356 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z11equalKernelPiS_iS_' for 'sm_52'\n","ptxas info    : Function properties for _Z11equalKernelPiS_iS_\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 352 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z18restoreStateKernelPiiiS_S_i' for 'sm_52'\n","ptxas info    : Function properties for _Z18restoreStateKernelPiiiS_S_i\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 9 registers, 356 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z17updateStateKernelPiiS_S_S_ii' for 'sm_52'\n","ptxas info    : Function properties for _Z17updateStateKernelPiiS_S_S_ii\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 10 registers, 368 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z16initMatrixKernelPiii' for 'sm_52'\n","ptxas info    : Function properties for _Z16initMatrixKernelPiii\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 6 registers, 336 bytes cmem[0]\n","ptxas info    : Compiling entry function '_Z15initArrayKernelPiii' for 'sm_52'\n","ptxas info    : Function properties for _Z15initArrayKernelPiii\n","    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n","ptxas info    : Used 4 registers, 336 bytes cmem[0]\n"]}]},{"cell_type":"code","execution_count":45,"metadata":{"id":"QNGy3BpeqZGs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721042982189,"user_tz":-120,"elapsed":41616,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"}},"outputId":"3cd0f93e-53c6-4ab4-a617-d679da368c27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Order:\t243 4925 4874 73 2106 64 3814 2919 6975 293 3944 4814 4446 7745 6790 3926 319 1943 4646 5971 6748 4182 2224 2338 391 466 3261 3795 4463 3933 6196 1430 5 4015 948 173 7547 2177 3057 4142 5558 5467 7114 7321 4405 5901 6942 5063 2343 4179 2287 3548 832 2303 4112 681 5926 1531 2957 105 1502 7905 7749 2167 4370 3880 4847 7459 4323 3813 3514 1732 4438 2168 2186 5275 4143 620 3586 7718 2971 2435 4017 6817 4821 7865 2200 5316 2607 2956 3297 7935 6147 6187 7674 1461 2035 2883 7897 3033 1253 1441 3506 3492 5494 5413 4353 2939 4741 7449 4166 4055 6205 4431 6438 2089 6632 5908 2575 7596 6188 6768 5380 1405 601 409 3383 446 6254 4192 509 2049 406 1287 4332 1395 6070 7478 282 3718 2856 2697 588 6635 5882 7376 4900 3757 4719 3006 6972 3336 1140 6662 7569 7889 2823 3370 6808 3966 6931 2962 5312 4314 4557 4430 2727 2110 917 2408 6329 420 4950 4414 6403 2346 3320 541 3040 1589 631 6945 2757 6248 1571 7139 3895 361 6365 7064 3699 5823 3724 520 6759 5462 2709 6263 7907 6508 2263 2132 1556 3945 6352 4959 1477 5025 1896 6073 2837 634 1111 7069 5555 4801 5122 4058 7525 4555 1576 4445 5433 4539 5487 6382 3196 1926 1780 3379 48 4492 7355 894 1594 3972 7998 7264 7965 6338 2355 7838 3020 7024 6066 2802 5571 821 42 3363 4870 7395 3062 4435 6450 2430 6701 1769 5720 3529 228 4596 2991 5328 1993 939 291 7052 6393 3823 7189 2483 7653 7800 1610 712 1050 5329 4104 6402 4771 6746 6739 6623 1163 7542 2731 2344 2051 905 6337 1584 1493 826 672 7009 485 4749 4533 4710 7528 4724 2078 6940 3061 873 5568 668 4736 4570 7201 6117 2708 275 3646 7165 4520 2109 1328 5430 1417 3177 5208 702 6721 3959 5135 4854 6653 1515 7090 3762 6586 636 6032 3616 4857 2650 5807 3983 2796 1555 4931 3153 5692 729 4200 7756 4734 7235 4601 4518 1845 4238 4877 5250 7813 6025 3698 5674 7357 7857 3284 4723 529 7993 6458 5923 504 5833 7544 2060 4267 4962 7717 4237 6604 965 2906 6691 4297 3747 4590 2 1942 2329 7891 2750 7391 6678 4331 3214 4707 987 1839 5377 2776 1629 5973 1437 1093 7468 2119 1828 7397 5873 4141 493 6088 2631 1784 192 7650 7856 1178 2135 4046 132 298 4938 1153 4606 6620 5265 5038 7628 4367 183 1915 2369 4502 4743 1807 6621 680 4764 717 3715 2082 5071 1543 4649 3060 7424 7404 5443 5758 6060 3410 2308 5977 7617 5241 4965 1307 267 3236 6314 3759 4527 3773 2670 336 2194 7873 2380 5957 3246 4536 1049 3787 5183 4774 2638 6223 7533 5026 4792 3194 6900 2620 3260 2271 1115 753 4650 4504 1876 2053 5303 402 1677 7669 1865 328 4918 1331 5315 926 4288 6030 4403 1258 7299 7059 1271 640 6576 4892 6629 4342 2706 4290 1254 3689 5755 4897 6603 6211 3275 2171 2268 4226 425 2169 7594 4574 2911 3140 5290 7415 2935 885 5967 1771 7639 4834 6505 4825 5499 2862 7985 1858 3235 7529 3470 507 5125 4799 6538 38 5718 7428 3804 5374 7694 6908 6106 224 5070 6800 6788 2634 3830 1156 746 3170 2529 7420 5189 2873 1321 1300 5870 5868 2558 2547 1924 1597 6343 1177 26 7210 4086 2923 5010 5746 2157 5542 1626 5617 4593 5043 3728 3293 6043 902 1737 5902 211 1234 2554 1342 968 2836 764 6468 294 5045 6606 4103 3590 7118 744 597 2943 7814 3398 3985 7250 957 754 3511 5350 7324 1724 2767 3318 3558 3175 4619 2316 4010 2897 5267 2870 2780 7771 6278 4044 284 1041 7291 511 7551 537 1973 6028 2765 2924 6565 2144 4915 320 6224 5319 131 7374 268 1237 2234 613 1911 2284 5590 3749 5768 3900 6276 5594 1447 1946 7545 1781 4014 6913 475 7208 7944 3931 3861 5530 3731 4078 440 6689 4469 5024 7036 6325 6802 4701 6720 6574 4949 6017 622 3733 7431 4704 5570 3001 1969 1064 165 2848 1108 3119 2567 1894 3761 3600 1702 2513 4462 5969 4057 1504 4302 7735 7828 7105 4076 2250 7554 3515 6617 7515 4879 3189 4629 1215 1496 999 6280 2636 938 7006 7115 1283 2330 44 3287 5455 7576 6172 3351 5088 7506 5001 362 5636 6168 3432 3359 7316 7463 7244 1029 2657 1800 5960 1647 1011 5460 7681 4234 3858 6771 4999 3943 1344 1455 3587 479 6832 4914 290 1369 135 3628 1336 14 7148 1138 6406 3115 2012 7353 7621 1291 2161 3002 1991 1704 6772 300 3853 1220 7778 5927 4043 4817 1846 4209 5511 2090 233 7311 3331 3528 4898 7358 1059 890 6615 3197 3402 4971 5794 2831 4128 7313 6738 4451 1900 396 924 3936 5151 4935 271 2496 6806 2423 5586 3679 6711 1706 1072 2760 7038 7132 1678 757 4528 6677 4263 6308 3162 4919 4364 1079 6303 1881 1824 2604 6115 685 5040 4172 1539 6376 66 5273 1090 6097 5302 1884 4809 4498 3122 2348 3690 4042 7383 5531 3836 1826 4180 4259 7500 1027 7812 1002 1961 3597 5281 4388 1966 2007 468 5108 4815 4194 4800 1783 1801 6014 6412 4470 1927 4117 2501 1534 2979 698 7015 4449 80 4406 3300 1585 4610 5933 7332 1144 1095 5361 1171 7895 2002 3603 6933 6559 7319 5627 205 5363 6274 4077 2684 3744 5152 6217 5230 5044 718 7579 1188 7992 2608 7786 7599 1958 6504 2063 7435 4936 5515 7593 2655 819 6763 1620 4933 5142 6849 913 1596 4223 6886 3980 6447 7187 756 1798 2834 5055 3831 592 5724 5034 2479 1207 6173 3174 5845 5293 3921 498 53 5212 6996 4041 7513 7203 7939 323 481 7425 186 7925 6983 1850 6442 7890 5660 1145 5016 3301 7443 6960 1964 133 768 6926 7976 6011 6605 7719 6268 3249 1131 3567 1311 7330 3619 188 2039 2256 1276 3730 2388 2143 1413 7284 7278 1537 88 1255 2147 7541 3611 7867 162 2512 2950 1669 3964 6730 1932 6006 2013 7436 3130 1037 2645 7738 3440 2465 6076 3325 11 6702 4038 994 7190 1608 370 3660 4366 7302 5773 247 4119 4735 2052 6313 3554 5759 598 5641 7308 5941 3592 4491 4444 5392 2294 3113 1550 2432 2824 7056 5709 3416 7230 2091 2056 7648 7768 1068 5498 2854 4561 3508 1444 4905 7304 5439 5575 1898 4783 3971 4393 32 1752 5320 3234 6704 3100 4035 7793 5625 4114 2044 1863 6891 7386 3430 263 3627 1503 5689 1152 6272 2445 781 1817 197 2877 6897 6628 5160 372 4341 3667 5437 147 2766 450 7492 2030 6517 4372 2903 2068 3420 2025 1561 4875 6864 1199 3735 100 1326 2478 7526 4682 3353 3876 4573 7508 5182 7904 7394 2648 3745 3665 1175 4920 4967 7790 5445 4474 769 4810 2540 2304 5551 3589 5729 2471 3296 916 1785 2008 4882 771 3021 2444 4222 7774 2671 3908 666 4598 7879 3168 5482 1794 2022 1368 3580 3463 5009 3859 285 7268 3215 7629 452 6599 1741 7791 6822 1725 1410 5313 2360 6277 865 1047 2658 6247 45 6542 1388 7182 4779 1382 6390 5597 1189 857 7370 405 5258 818 1028 7924 5994 6497 5086 5687 7632 3138 219 3450 7906 2288 5562 3692 7373 2963 1406 4137 2596 7421 4396 617 443 7344 6903 5871 6854 359 4033 2537 3509 3786 5858 3459 2394 5217 6984 1386 7943 4509 278 7633 2077 7936 7659 6860 7249 2816 5095 4708 6642 1749 1617 5970 6354 2712 3226 5784 5878 5399 1360 1293 7995 2173 3879 4201 5968 578 2047 6609 7566 3593 2968 6466 4550 1262 5480 7782 54 4737 6871 101 3885 4987 940 6444 723 2105 6289 4116 7486 1809 2422 6110 5100 6035 2764 2322 6144 3647 2711 6820 5839 4917 3164 7903 6140 1020 5896 5236 1030 4844 4957 811 1684 2004 7439 7915 1520 7155 1297 5298 6423 1708 6495 3829 7075 2325 190 741 4325 1513 5200 1478 2320 2630 6927 4625 5762 4524 3678 5639 1530 7792 773 6648 1330 5547 5131 2249 5703 4891 2810 1362 334 797 3308 4820 2617 4345 971 3481 2929 6708 3571 1312 1351 4410 3542 50 3922 3549 6783 7272 1265 3717 4096 7295 7564 4003 7327 1779 1694 794 5892 785 286 1261 3334 2508 4174 5554 4822 5243 6572 4482 5822 4132 4634 3969 3960 2153 2717 778 324 4351 5899 506 346 5857 6182 1700 5282 4478 2428 7874 7839 7972 6229 4247 4908 4409 2230 180 1209 6777 3848 6334 2473 2976 1159 523 7310 3371 5676 5765 3850 3013 261 6440 4494 3523 6273 6579 4757 5830 4280 383 4304 367 1495 6305 2202 1296 5280 662 6397 5528 4228 7678 4687 2736 7093 5607 1909 804 639 7481 6023 3228 2704 5825 5115 2103 6448 6930 3904 1861 3266 4027 6846 6040 1082 5655 2915 4049 5057 4212 5824 4805 1848 4022 5188 3919 5263 7298 7347 6072 5444 7751 4542 1956 747 4399 3248 1854 2640 7136 5507 2784 5576 2413 5238 34 1107 1105 6469 1560 7377 6837 1158 2446 3358 2902 2525 7290 3108 7198 1414 4837 2942 3468 1224 3504 3760 1790 4667 351 2996 4846 4434 2517 7000 1836 301 4553 1683 6866 3335 4199 3886 7217 3213 7265 5881 6722 3360 382 2981 3283 5354 7881 6036 1486 3317 6976 4806 2509 3797 217 2326 1333 5237 5505 5155 1716 3315 2453 3682 6199 1551 5520 576 1552 3651 1526 3677 3705 5648 6221 6861 5423 7294 546 2572 5713 2092 7898 4361 1428 4526 5195 4991 7044 2891 4970 281 5872 1748 3716 5985 2852 3652 1 6923 587 6561 1429 1219 1354 1662 1376 2118 1719 5632 4665 4866 7046 4511 7610 4972 2532 3493 3092 7141 2463 1913 4515 3495 5989 7351 2846 1290 3963 6719 3016 780 993 3947 5883 6827 2076 1736 1587 2855 4307 2379 477 7543 3114 3219 1061 4496 6986 5012 4286 2523 6065 5193 3695 5544 656 6592 7953 3925 1089 3870 6512 602 6536 6496 2190 736 3105 1400 5417 5234 6809 3686 7523 371 2701 6758 1357 7253 4268 3683 4147 6761 7061 5593 1528 3707 951 1475 1639 6553 1142 3205 2863 3872 4485 1535 5344 2719 3708 3011 352 377 7955 3039 210 2564 5711 3129 7720 985 1743 2559 5239 84 1985 934 6845 3777 2904 1705 5792 125 6675 433 4379 2452 5886 4081 7027 3697 3822 5809 1753 5982 6298 532 2859 4204 1474 5339 1309 2470 453 4512 6556 496 7335 7163 303 6339 590 6718 6195 784 4802 1580 203 2037 4488 4680 393 6712 1391 6857 3856 6137 7279 6614 4924 3750 6805 2133 5092 7849 3442 7779 4615 2464 7772 6333 6647 3663 6296 6554 1663 1345 6797 2574 1259 3439 5652 3725 6850 2665 7381 535 5174 2886 7475 1697 1986 3494 2782 5252 6977 3244 7450 7709 257 4118 4489 3961 4569 623 7444 2024 3668 6967 4763 936 2321 6537 4867 5785 5318 2116 1006 1246 1491 1812 720 3693 604 6489 7312 5634 2152 633 1588 3704 3875 2462 6085 7760 4196 6725 2718 6369 7451 7471 2778 5966 6563 7832 3524 3995 7699 326 6357 3570 2944 675 1274 5801 5910 4912 7834 2351 2243 304 2447 313 6584 4105 1359 6519 4864 1065 872 1691 4594 6506 5180 4884 424 517 6998 3890 5244 6294 7512 4376 6047 4298 2364 2589 3427 6552 7973 3472 3352 5128 5272 2952 3343 1516 273 7242 893 2726 7131 7125 7356 2058 2331 1721 3116 108 3518 2899 5117 4362 6240 6346 3202 1119 2667 2832 134 292 2872 2775 758 533 3954 7966 216 2195 3274 4624 7212 6602 5726 1403 4778 4319 6928 5458 2549 5978 3710 7777 701 6852 2682 1015 1270 279 410 5156 4681 4217 6007 6128 7663 3837 654 7130 824 4317 3748 4762 7447 3997 2753 154 897 5843 2239 112 1692 1338 2505 2639 5322 6042 1278 5638 5076 4868 4004 6511 5301 2290 6385 6034 385 7111 7833 4229 1651 3179 7652 7418 909 3816 686 6446 1318 2149 454 2588 4490 5213 2806 2916 565 4677 3332 3525 439 3435 2621 5981 663 4986 5132 4207 5506 4272 3843 1860 3610 6818 6435 6938 4101 3028 7326 7237 1343 7354 5852 5310 3054 2451 2324 6426 2905 3207 6191 6387 7888 1746 3685 1606 3825 2429 5645 3635 6141 1099 4599 3897 6596 570 3473 4567 6514 6249 2678 394 6660 1656 36 4668 7232 4329 7151 4500 5419 4966 2225 1792 7246 331 4266 6935 1218 6304 3423 4355 2887 5861 3694 5671 459 3034 585 3397 5358 7757 6985 4178 4477 3098 3144 5429 7179 6807 6231 3254 3962 1306 1039 5053 6671 5489 1038 6992 5651 4359 2773 1063 6821 4835 6775 5112 5731 3424 5581 3117 853 524 5268 1882 7417 6460 3294 3819 6101 7145 4162 4368 2628 6020 2801 4937 960 6680 6750 2892 7196 6639 952 2277 1937 5407 7703 3187 7581 3438 255 3148 1319 1925 6693 3344 6079 2633 4638 43 5582 593 6752 6880 3183 5287 2629 4523 4452 418 4183 674 935 2980 4091 567 1621 1923 1690 6177 4712 6429 167 5248 5454 652 3461 6421 6139 6670 3007 1857 6919 6688 7952 5649 1665 482 7822 5929 5903 4605 3106 2556 5663 6901 7045 5373 1631 6987 5457 5261 5046 5991 7608 5550 1933 5894 3203 895 4881 6155 4258 2096 401 1289 235 7962 5425 1664 3605 4125 3178 3599 306 2995 7913 7455 4262 1799 82 144 6646 6052 4974 4059 4023 814 2377 6637 6153 6443 5215 3380 6631 6363 6024 467 4161 309 5924 6905 5608 5472 825 4988 6922 2937 7673 1574 5775 5766 7916 3078 3595 3484 7837 3375 5728 6521 7725 3337 6541 4517 3551 7932 4922 1880 6687 4513 2415 246 3774 7128 2417 3 3903 4299 3000 1155 4878 2416 5951 5335 1367 956 1810 6779 6836 7276 3953 4647 2075 6627 3827 2254 1653 841 4910 1988 1996 5286 4048 3988 725 1372 5578 5811 4848 1907 2560 6694 2610 5300 4514 6037 368 5752 7228 4129 6778 2187 3741 3714 5522 1280 6714 6740 4193 4466 861 805 7981 4281 792 2248 451 643 3950 2083 69 550 3803 4711 2458 5850 1871 1563 6059 6543 5512 7252 7240 7940 5566 7530 196 3894 3156 7552 1583 1600 7441 5051 3142 6616 5209 4563 3742 7339 4030 6242 4618 653 7798 7292 5880 1660 7994 655 5893 881 245 716 844 5090 7318 7275 6760 2533 6293 5559 5015 1097 903 70 1110 7588 2100 5110 4321 2121 6431 6573 3928 1842 5934 3482 3994 766 4170 7982 7504 2818 500 1527 4716 7708 7225 1514 2729 1085 2582 752 1686 2561 4282 2747 1990 7584 6684 1192 5925 4389 238 6493 728 3441 2994 7258 1827 1033 4662 5999 4790 164 6022 4670 3860 98 5589 2600 2605 7818 5990 3738 1114 4036 5311 7646 7615 6176 1335 2725 23 4327 876 6432 3019 7483 4458 6858 7748 5620 6000 1710 1071 4315 6651 140 1553 4589 3956 2969 6546 1098 3182 1133 2285 445 2489 7263 4829 2302 2354 7548 2771 1897 6810 6208 1295 6475 182 7026 899 4412 3185 1347 7453 1361 102 3401 4187 7047 3290 200 5496 946 2597 7497 7349 3584 6755 3820 2253 1442 1435 7614 8 1680 5659 5604 5513 3666 817 900 2627 117 2267 2450 6548 7199 4070 6145 2974 7521 5005 1878 5477 3658 6465 6472 7087 1458 7788 7557 5563 4923 514 6781 7305 1086 4556 60 3052 966 5802 2045 3222 931 2511 1383 2867 6055 3940 4906 5059 1847 7482 2656 5584 7178 4954 1356 3345 6198 4034 7979 76 984 5723 4685 7763 4943 3147 7607 1524 7205 430 1377 1490 1176 5598 842 2336 1987 2272 4693 530 7758 1439 2375 2841 1796 4072 2059 843 5841 6373 1558 2728 5468 4715 2721 3613 3458 6391 6265 6600 3085 4961 2992 991 2983 6583 5876 800 6358 4221 380 910 5806 4652 7501 288 2975 7034 1837 3176 5745 7407 4862 3128 6091 563 2552 6476 1431 7918 6762 2476 364 3299 2920 2356 3270 253 3650 7784 7477 5107 4722 7289 6949 6102 6392 1536 1506 492 3826 3489 2158 796 6162 4146 310 2949 5048 5204 6163 1103 5945 6558 3557 851 2880 6699 5367 2261 2668 1402 5704 6622 1436 109 3729 287 2393 7677 5412 2125 1615 3169 4099 1241 2642 163 4842 5577 854 7171 7452 1487 3126 6676 2205 5245 7413 314 2664 3541 5242 3454 1723 198 7243 2584 7380 912 1046 6842 978 4529 7675 3031 3798 2661 5702 7560 495 3930 6048 7963 4381 2159 4501 5075 1116 4631 1471 556 1718 5147 3218 6204 3565 4768 5600 7008 1286 6731 480 6238 7018 2840 4213 1727 7360 4398 4538 5708 106 3631 4824 5082 1586 4459 448 5816 3133 4705 2084 5247 172 2502 4577 1808 3181 3737 2578 4990 5751 3457 3687 3675 1473 3377 4544 4295 7002 5962 5887 7089 7691 683 3338 3389 1025 7288 722 5365 6414 7043 4380 41 1950 2522 2433 1717 627 338 7362 3408 5747 3805 4664 4776 5269 2142 1628 7727 830 1173 4441 770 3790 3210 1568 5514 802 2402 5705 6228 6785 4387 515 5783 7077 325 7255 7375 1775 5681 7570 5855 3017 7403 5366 6848 4198 4580 4586 1962 1004 3661 7572 412 5484 4745 5119 7273 2193 745 1070 3543 5085 3121 20 863 3216 3223 1954 7825 434 2042 713 6413 1184 6474 5390 4795 159 2777 6118 318 4746 115 107 4627 4636 1483 2690 6791 4765 5540 6745 3227 5409 611 7902 258 3143 4363 6123 2893 619 4080 2086 5795 35 1117 1191 7984 5387 3644 6285 2487 1210 7409 2482 5814 5623 158 250 1931 4495 4558 4640 572 4401 305 6077 4206 4904 6607 3779 2720 2683 4748 3640 7303 7571 4270 3414 7949 880 1813 2454 6264 2966 1464 1625 1778 5278 1489 3497 4429 5932 322 5210 2041 4902 5979 4447 5447 5106 4691 169 793 2790 145 179 2907 6974 4903 2520 7861 6958 7664 2931 2196 3081 4018 6831 1180 3676 7887 4692 2026 1853 4067 4384 5712 4952 1712 4784 6503 870 542 6713 827 1776 2563 2231 2738 3329 6113 4721 1042 4151 3857 5338 354 3217 5324 4850 2071 628 733 212 4583 1843 7432 6963 1789 1844 4233 1397 3056 5343 647 2742 5027 6991 5918 7527 1523 4642 2881 2676 3743 3428 5453 4408 168 5226 242 7809 2129 5414 3791 4548 6581 2624 7004 2166 7946 1298 2546 4139 1734 7855 2409 1452 6350 2297 3727 1867 6099 6485 4827 7618 4476 502 2550 7642 1272 7585 1901 5408 1381 2293 5535 3688 1872 5984 6461 6114 2120 3099 3303 2735 963 6411 7950 3629 6564 4546 1498 5753 411 5254 7920 7081 3239 4225 1069 119 6302 2072 552 812 3231 6971 7019 7957 3775 4607 7423 1682 1451 1544 2548 5862 4407 7496 5353 715 341 7454 608 3756 4360 2334 1820 6225 5495 1075 7733 5199 7743 4066 7392 4985 1511 2460 2291 1840 2894 4155 7080 4916 6356 719 687 4208 5163 4340 5998 5552 2280 3091 387 3927 2960 3276 1859 7634 5748 7028 3835 2586 4065 5837 3077 1100 248 4309 1591 5917 528 2744 6492 1714 3263 5186 7603 1622 7880 7602 7712 4694 2491 3476 945 7707 760 7519 4819 7274 342 7446 3488 2459 1162 5093 3800 4063 1146 151 6853 1755 1396 7685 6585 3639 3120 1317 6529 835 5060 7457 7753 3074 554 6061 5330 2292 3051 7796 3049 79 3221 4130 6830 3555 1941 4330 1375 7117 1044 3655 7926 4744 4483 1240 1619 1279 3291 6551 1953 1080 1136 2967 3496 7074 5948 7847 4730 4073 5669 3093 6855 2961 6167 6105 2940 55 6220 280 6029 2398 4742 2614 5340 3630 3037 272 7666 3411 750 6309 7700 5432 3123 4678 6580 7670 3041 3946 569 2716 836 7871 5438 5786 1940 3036 1101 5913 740 2220 4236 7858 1238 4831 2498 7433 879 4358 5605 2936 4024 104 7875 5628 1250 6039 5800 2888 732 124 244 6074 6703 1081 6591 5954 2696 5682 5393 6044 5332 2970 6103 1077 7207 3042 2101 1268 908 941 942 7509 7239 2978 6342 483 3103 7810 7104 711 4481 1305 7575 2663 1722 709 6959 918 7868 833 2349 7851 6956 2107 5166 5371 1787 6383 6859 234 6290 5404 199 5089 3828 7739 7016 3912 813 5716 3501 3934 3838 584 5685 2733 6415 6295 6318 4519 5764 536 6892 5066 6134 6130 6863 4184 3807 5740 6430 2643 5525 5255 3241 1418 6068 7817 9 2201 4669 5000 1906 3507 7181 3539 1782 5105 7948 4303 6206 6291 3818 65 6316 7173 7300 5698 7412 2581 6156 7122 1204 5073 1048 6299 2884 7869 5129 337 3479 5295 6287 7066 6215 5308 2238 4975 4796 5345 315 6669 1466 3067 5721 4984 2653 501 5422 3072 6567 929 855 5860 742 5262 4157 783 6348 3625 4402 6109 1434 3709 5615 5995 4019 2027 5679 4395 1247 1292 1420 3641 1060 5742 5844 7416 625 5836 7204 5549 989 1976 7660 56 6950 251 1624 150 6921 3612 5064 6706 7285 7283 2623 1313 2425 1421 1877 6937 6724 4717 6010 3632 1040 5257 521 4264 5307 7023 4244 7123 7462 456 7215 3238 1885 3585 7600 2436 3268 6910 7037 820 2807 6932 2551 4791 1735 7910 6754 7761 1182 4397 2609 3433 1256 6757 1768 6275 67 5813 6904 5067 7737 3643 4054 457 2269 3101 1604 7213 3059 3986 7317 7033 5706 2242 7129 3386 2779 2381 7206 3865 4890 5397 1687 6473 3474 332 6214 7826 7657 555 538 1756 3851 5158 7516 6672 3984 1208 671 404 3368 7695 5516 4656 6867 149 2999 6075 3722 4510 3436 3445 7538 6625 3055 648 4241 6003 7661 472 5483 2495 2583 6433 6839 3264 3302 2281 3863 2571 5251 5490 649 1601 7789 3163 7549 7053 3713 1578 7872 734 6266 4439 5950 274 959 599 6872 3906 4738 7158 5294 2358 4276 220 4424 7801 1875 2176 5231 1521 6051 3546 1655 6594 143 3808 1891 7765 1157 4929 1132 3245 692 6568 1607 7806 862 6888 136 7472 4425 2838 6973 5150 5834 579 5780 161 1123 2307 4804 4982 4651 3703 505 6132 2055 87 7987 3477 2973 6733 6823 3251 944 1316 6120 5675 976 3462 1609 7537 3087 7341 175 7518 1401 7440 1223 6663 4612 7168 4356 3932 3987 386 3009 7359 5164 2932 1165 1549 1982 5610 7884 7345 7590 4940 2296 4617 4946 2054 2715 4752 7524 591 7701 2365 2737 497 1960 2569 2048 1399 846 6824 7042 395 5647 7020 2215 4978 166 4859 1654 4698 4851 6336 1094 5754 5421 3935 5124 6961 7012 670 982 7780 6952 3289 1206 6437 2740 1804 1668 226 6946 146 4635 958 4620 3456 4261 3004 3636 1091 678 3443 669 4171 4455 4751 7503 4337 3027 5078 4980 2622 6464 24 4095 3664 7062 6657 2431 5296 7231 3151 961 2803 4231 2791 1517 7382 3090 1323 4436 4326 6717 7958 1251 2210 3346 4005 3073 5953 5629 992 6158 4758 5974 2722 5143 4785 6401 7824 1484 5683 3881 3467 7430 699 3706 3970 1052 6544 7613 3278 6789 706 3560 4969 4124 2299 4386 3562 447 5613 2948 5560 5428 3721 3781 2418 4100 3381 19 6636 4833 3257 6462 1670 6112 2874 7773 7787 1003 3499 2441 7954 2439 4324 7977 5074 3799 6279 2099 436 5370 2003 829 1409 4611 7172 7456 6969 4456 6811 1467 3788 1139 7641 2566 2649 1243 4294 5214 5493 3125 7682 6236 1202 3522 6422 6372 834 2352 4158 6331 2255 6054 1303 4293 4860 4037 7005 5541 2361 7489 6792 2734 2015 3490 1602 2259 4658 7140 4164 2426 2235 4173 3871 801 0 3026 3388 3083 2150 6257 2227 5949 5510 4050 7251 7679 526 5856 7988 2797 327 5637 1866 6080 2912 5733 5788 1127 7815 4243 3255 379 6981 6004 5914 6138 1200 3901 3526 5626 4149 6459 7842 6907 4772 5410 2811 3355 2178 6454 3620 1217 232 6523 3700 568 1632 3330 7236 4271 5567 1341 1438 594 1346 3998 6644 2134 2781 283 5181 3322 1126 1481 1106 1636 6645 7460 6349 3405 3191 7702 6420 111 3018 1967 2396 1564 1908 5333 6064 1058 2080 3626 1125 6245 5451 697 1000 7021 3385 5246 7110 4595 5061 6770 7997 7491 1143 765 72 5963 2247 6041 659 1510 889 2698 1949 2542 3832 2619 1252 2819 3429 2466 5848 184 2073 665 759 6887 1390 6146 6965 2845 4020 28 3534 5936 6381 3030 6619 142 4310 6618 2246 2900 5791 4927 3765 911 5502 3755 3594 4609 5646 6766 6332 6995 1387 236 160 3614 7912 3866 1398 561 4885 3608 2216 7860 5006 5772 7686 4260 2172 3937 3561 3328 1611 2844 2412 4442 355 1166 5348 4540 130 5930 1593 6259 4009 6654 2410 2315 94 3842 3696 1959 7990 249 3449 499 5309 691 5233 7017 3948 123 6751 6092 2175 3702 397 3089 148 3250 6192 5111 3313 7668 4224 464 6246 2705 6664 1698 7209 1984 1830 6129 3311 7901 6005 1666 1570 1161 5536 5778 5631 3366 193 1598 6152 2017 1612 7211 202 5317 4729 5509 416 5810 849 1339 6328 7254 2751 7135 2300 5763 6882 7797 5517 3465 5207 262 583 2183 4560 6053 5321 3873 5463 2938 4767 7490 3475 2635 2825 6870 581 3566 7369 5041 3776 1092 378 5622 1767 6094 4467 7226 2232 508 121 7883 1917 2136 2646 739 7577 2363 77 1427 933 6157 4249 4725 4565 6227 6773 544 2395 1715 3899 6906 2241 4165 6234 7923 4148 4292 5137 7138 7769 5478 7937 6862 2347 2067 4126 5761 776 339 5148 3273 5190 2440 2827 7494 276 3941 5666 3598 5585 1179 6786 400 2686 2357 7174 1699 5922 4653 5803 3674 1905 7626 6838 2544 3448 704 1497 2587 6749 7807 5169 7400 2400 7180 491 4349 1257 3979 6457 2783 3165 99 5680 7870 3112 2117 1112 4427 366 6484 7469 868 6169 998 1051 4064 4464 4889 4782 4026 6482 7054 667 4932 2233 4564 2808 1955 5583 1566 5279 864 7100 7101 3464 3915 2986 6095 1936 2965 4789 2384 7398 6491 5869 187 5986 3145 2651 954 1228 3417 7296 4284 3237 5218 6161 5019 1685 5179 5036 5928 7193 6258 3253 6471 5694 93 3909 7331 1167 3032 1971 969 4979 4133 4769 46 1818 3111 3512 5065 6226 6098 1053 389 1886 7325 4960 2804 6239 3240 6324 1758 3172 1018 7933 6233 2598 616 7882 1681 213 2864 2179 7470 7658 4661 1833 694 5177 4503 5828 4807 5964 4968 1892 7257 3989 7007 7094 4443 3063 6547 2141 5240 4404 1364 5799 2800 1423 7732 3434 7715 6165 373 2298 7133 7820 2521 2543 5750 503 3978 6427 3208 7563 1874 296 7261 632 4852 3025 78 6306 2273 3559 6499 2406 1738 5618 3753 5326 5767 4301 6742 5642 1970 2860 2930 7216 4992 118 6362 5569 1883 5854 3076 1275 7233 4587 1322 1928 3029 6405 2519 6638 5988 682 3659 5008 3204 516 612 3364 5730 494 7803 7522 4880 7595 7795 7297 7214 892 673 5684 1747 6419 607 7794 1938 4552 5232 4411 403 3784 4195 4311 6885 2455 3602 2861 384 1688 5907 2097 7762 5384 1087 4706 4094 1374 4828 414 3958 3579 6982 6271 1212 2785 5524 2383 6193 1043 2245 1109 5014 3321 6993 7041 2480 5588 1675 5884 3097 6311 1695 3809 5673 1141 5098 7960 6734 871 4285 914 4543 1017 4754 6948 4823 2516 6641 2763 357 7448 949 1529 345 5351 4893 113 4028 2817 4419 6767 5084 7227 7287 2472 4797 7550 915 5123 3902 2407 375 5749 6812 7124 5050 4602 5635 6681 363 358 7134 2913 1149 6994 6250 7113 7338 6666 1823 2181 2755 4781 5789 688 4154 6184 330 2820 7645 3768 6665 7202 2079 2074 1509 7473 6202 3166 6700 7968 1888 1067 1476 5609 49 7200 986 5396 7919 5889 1841 1197 1340 580 2922 2258 1764 858 1062 356 4246 3265 2580 7804 1542 4453 1592 5534 4188 7754 465 4109 7055 4798 5154 990 3634 2822 6307 5897 3771 5068 266 4997 2499 7974 6341 7647 4475 586 6834 7040 6327 4616 1284 4981 6569 7767 6452 3295 455 208 1366 4232 7885 807 5864 7680 4941 5959 6185 6371 4581 3778 6436 3048 5031 6378 6879 2211 5291 1353 4308 5658 7534 4525 2562 4826 5678 2192 1134 5661 6404 3889 1463 7461 1903 6063 4111 980 1895 189 3740 4786 4532 7340 5283 4074 230 4068 62 321 312 4948 6119 6695 4568 2504 4135 6300 5035 4856 852 679 3957 2680 7741 4873 786 6445 6915 3637 1196 6386 3070 1216 7092 705 964 1829 3212 1422 6869 4189 3949 4780 7333 1249 2244 4052 7900 3259 6540 5020 4163 6968 5205 3193 2534 2809 1661 5452 1994 6595 86 2221 743 3466 6481 6127 551 6002 7343 365 2000 1232 2815 6686 626 1869 85 4334 5734 2391 6990 91 7334 3623 2043 845 1577 6142 3974 2229 2040 1045 2001 83 4416 850 4695 847 2982 2749 6856 4277 4928 2946 1649 3399 6847 7836 4343 7088 2477 564 2847 3403 955 3141 5134 1310 1731 3043 6947 1150 422 3225 6877 1754 5369 4623 1034 2276 2301 4803 5771 5017 4413 1084 6330 1214 7025 7532 3394 6031 5736 7574 3973 6601 5420 2599 6819 7853 2019 3154 7367 6917 5077 1890 5394 7854 1977 4551 5219 1449 767 27 302 7656 7112 241 2114 97 2122 5033 4461 967 6232 2023 4082 4279 867 4720 559 5052 7277 6322 1652 5355 6624 5701 487 3305 6216 2490 5256 5937 7078 413 6634 7819 4418 3821 2984 2565 5102 3478 3517 7147 1129 6374 6803 4250 6833 5002 2510 1914 3258 6434 25 1730 5722 1168 4176 2474 4887 5405 7980 1887 7102 3262 3527 4239 1151 5818 5285 7048 4853 4168 6171 5203 5599 6649 6796 2917 2951 5686 4092 7328 7723 1788 5403 3135 2050 6203 6894 4377 1472 5983 3425 1930 5259 3577 7405 1120 763 3005 6281 1644 2257 223 2339 3874 1032 7555 7480 1703 3190 5668 7692 7665 2693 2713 1870 4535 6874 2754 3802 7010 239 6013 774 71 2652 6347 6150 259 6943 6715 3796 7561 1407 6954 7465 2795 5523 4219 7390 5476 3035 7934 4608 6697 7766 5693 5362 4562 307 4872 6531 3232 7245 3723 5347 6131 2759 7154 1128 1154 152 4977 3447 5710 6925 5840 589 1203 1921 3581 2341 4845 3149 7116 1389 5619 3887 1007 6151 264 4256 7323 7001 2350 2385 5508 3916 7320 2198 614 3923 4437 1849 1187 5449 3491 6270 2443 1797 2475 6222 333 1031 5614 3884 3726 7697 2295 2419 6562 1024 6124 1679 2492 877 2606 2997 2955 3316 7746 1979 979 2020 4505 5796 6912 4728 2741 7580 7821 5688 6873 6396 6212 3662 1501 5469 1233 390 7408 478 5448 2793 6050 3282 3192 2515 3564 207 790 6083 6673 1981 6261 2213 3669 1066 806 3022 2353 540 6801 1646 2085 6658 4888 6737 7770 7638 5341 875 7971 2207 5336 3573 5162 4203 886 5194 1174 1459 1567 7445 4506 2909 13 3469 1952 5672 6125 270 6159 7947 4420 6682 1263 2163 297 4071 2305 7039 7930 6815 1522 755 947 2739 5741 676 1055 5501 1194 1922 7672 5334 4571 6321 4254 7706 6955 5557 6439 5385 137 1957 6661 317 7364 6408 901 1334 7144 4976 7755 907 6884 6814 7108 4369 1277 1572 7578 2033 6941 6284 2031 650 5656 2366 3058 7662 127 329 1963 7909 1113 5149 7082 7811 2625 2204 7322 2088 7220 787 574 7057 3533 4516 6301 3409 308 4051 5677 7625 6916 1856 4134 3538 6480 4942 7070 6813 4056 6829 4144 677 3553 7438 5874 1379 6650 3104 7234 7067 557 6015 340 6170 661 7651 5879 5691 7342 7315 4415 2170 89 7271 7191 7830 441 3609 7978 3576 4659 2507 7162 18 350 4465 3124 4032 5276 4911 6463 1640 2789 1729 6288 3670 595 562 1035 4739 2139 6046 6840 5058 4531 7938 2104 4549 6082 2703 6999 1559 7928 7223 6286 983 2798 6980 7329 3485 3256 5545 1862 4227 6692 3739 4579 5797 1479 3751 1096 2140 3812 2535 6149 1562 2327 5770 6590 5744 1998 6394 6640 7065 6545 2373 463 4350 1658 5975 3086 7783 906 2420 3158 3920 2531 822 3010 3395 6243 1273 6667 1968 1456 6045 2401 1614 3437 4603 577 1815 1446 7195 1352 7775 1972 5331 6315 2659 3069 4671 7785 2335 5270 6698 5621 6418 5650 7619 5441 3201 4537 6179 6997 6989 4484 560 7073 7346 7567 1019 840 2526 4296 888 4591 7609 1948 2313 3758 7911 1358 5029 7582 6108 1711 5427 3656 2113 866 5185 6210 5719 6345 7161 547 3968 353 155 6944 5993 5167 7649 566 3483 5915 1540 1001 68 1773 4274 7013 4775 5564 904 15 81 2062 2032 6183 4316 4098 6398 6513 651 1760 3319 1054 1482 4352 7917 4747 3280 4191 2889 2087 7479 2148 2137 5616 2925 7644 7546 5697 7427 2038 3552 5114 4895 5173 7411 3046 700 4869 4572 2993 7293 3186 5030 5727 4214 1557 3118 779 6526 3220 3582 5548 3530 7050 3378 5492 6509 7520 7604 5725 6237 5079 4016 7535 1244 2093 4211 5526 4507 1650 4688 7591 7282 657 4812 4641 5486 2601 2005 1733 3531 1186 1193 5931 6951 484 7029 4953 6515 7458 2485 7540 6375 5965 4578 874 1332 6575 6148 1696 932 1634 7724 4278 782 1912 610 7003 437 1541 2674 3047 4830 7587 7716 4956 2382 7437 856 7378 5707 4934 2568 6019 2895 6918 1795 7799 6549 714 3917 2112 7841 6377 3719 5141 2154 1266 2660 5946 7721 4926 3653 7510 2988 7893 7286 4338 2692 126 4633 2918 7414 2212 1693 5805 4473 3312 3545 6016 5815 5323 218 4621 2699 6571 7186 5987 7643 4382 4683 548 1533 191 1102 3127 3412 695 1236 3487 1213 3071 7389 1365 1599 1939 4753 4031 3867 5739 489 6828 6878 2262 972 3400 3160 3084 635 6978 5574 4675 638 3891 4849 2011 4508 7573 3918 5196 5288 5565 157 1445 2449 2306 4083 5406 1851 637 525 2851 222 6267 518 3516 7899 7314 925 2842 2278 5624 5518 1299 3536 7049 4676 3180 2203 1130 7419 1248 6424 6655 6164 5028 5382 1137 2933 1518 6335 1239 6081 1022 3840 5342 4773 3563 4740 4755 5459 4021 5640 7886 575 4663 5146 7103 6340 6936 2647 5662 4480 1864 7684 5504 4861 2115 5202 7177 7986 3376 4993 3421 2591 7969 798 3498 3349 6086 7184 7908 2632 7485 6716 6379 1302 4613 2959 4289 7159 3206 3993 2185 6527 5175 3139 4521 5819 6608 1519 12 4896 5947 5037 3544 37 344 2457 3094 5388 4865 3157 5023 5980 3596 1371 2427 4811 4069 2748 7399 1327 4841 3845 2317 7188 2794 4320 40 7730 5145 6317 3392 2197 7309 3806 4322 1432 2958 7705 5072 3990 4328 2724 5521 2184 3654 5376 4269 3372 2500 2162 269 896 5126 2484 6069 3913 4375 4391 260 2945 7689 1264 1642 7556 5774 4883 6256 5049 7583 549 6843 6186 256 4175 2018 6500 7307 7683 3591 6532 4989 2813 1014 6428 4600 1638 1811 1462 3783 4645 1227 5760 3184 1426 3080 1012 6957 1394 2672 2102 4215 7164 2707 3131 2266 5595 2283 4336 1135 3357 2275 5277 4674 204 5223 348 5573 2866 2743 2226 1282 2830 1443 7835 7565 444 3044 3601 2585 2772 7931 5184 772 114 2688 4084 5314 921 3537 3519 7999 2539 937 7840 376 2434 3951 7107 5911 7371 5096 4684 1198 6200 170 1416 1657 5958 2057 52 7442 2787 2333 61 6902 4964 4858 3877 4305 3767 6524 5527 3419 7035 4159 4727 6453 5109 3065 5743 349 3286 6865 6626 295 6370 6727 2878 3012 882 6705 2953 3844 1348 4479 4423 4497 1380 6560 4450 1757 2687 174 2337 5942 6323 7143 2662 421 1118 7846 3815 6966 1765 3252 5352 727 1073 5790 791 1355 3907 6096 7507 596 5229 2615 6107 2545 5442 4628 1190 721 5939 1349 7690 7687 5909 2332 1935 641 5592 7764 432 5004 3938 7194 3155 3617 2222 2371 2573 2805 5221 3161 1951 5434 388 5827 3407 6587 6795 973 2494 5056 4673 6194 6690 4006 7156 6197 5113 3023 2095 4733 6351 5737 2094 1770 6753 799 1582 7379 4973 7270 6764 6741 1315 4534 4433 2219 1221 4235 7260 3981 2199 1633 4248 5475 1500 4153 4186 7866 996 6787 5211 2839 2274 5172 5080 3977 6876 2342 838 5305 6776 6090 2536 2947 1301 4654 392 1412 3453 1433 4087 4604 1759 6794 6395 6962 5735 2404 3211 4759 2901 5228 177 2456 7562 5612 2389 6611 7959 5402 7368 6924 7605 6835 6668 7109 751 7845 7060 419 2593 4197 4110 2310 3198 4378 7072 7970 460 4732 6709 3924 808 974 6262 4622 4251 5890 438 7781 2921 1350 5997 3976 3050 5533 7597 4958 4471 3929 2914 2871 4210 429 5820 2065 7722 1329 7927 5961 2576 316 4793 7467 7084 7484 5488 3167 762 3982 3746 5440 6953 2323 1324 1777 1816 473 2374 6610 4761 2541 3824 7306 5198 5021 7085 5042 4655 7589 4090 828 2208 7829 777 2926 428 5411 3387 4951 3810 3134 7474 5938 5738 5357 4994 461 5013 928 1944 5503 6659 1980 5867 5996 6122 6868 2421 7740 1021 360 3833 6477 1320 4713 1873 5865 3307 1919 1393 1078 2618 3413 761 1057 919 3782 6696 2006 129 206 1009 2188 1147 7167 7192 1122 1579 2879 6533 6368 629 6297 103 6793 975 3910 708 3578 5603 7262 5161 6253 927 5249 5804 1231 5197 5220 5118 3766 58 1195 3673 6652 7598 571 522 5717 4808 1548 5153 6189 1786 1384 2611 5543 2328 3568 5187 2362 1465 3607 1036 3471 4522 3883 3892 2998 5940 624 5808 6260 3045 6555 7385 816 1603 6384 4813 690 5904 3088 1674 7843 4592 6732 2570 6269 3096 2792 7808 7366 4460 5426 2237 3015 6057 5227 1772 3356 5047 2882 3898 1635 3905 6416 5018 3793 1590 5436 3532 426 2669 185 3754 7259 4454 707 887 5464 6494 1370 1613 5216 2530 3996 2835 7711 2130 2746 471 5327 3393 7623 3811 3502 731 7611 7106 5260 2702 1201 4008 3242 3396 4630 4714 4731 1076 5601 5664 1016 4787 4011 1545 2214 605 2146 6219 1720 470 3285 2602 513 1026 2260 4939 3333 1507 2908 693 2954 6049 374 4836 4127 2524 1740 5539 227 6685 1739 122 3732 4913 343 2786 7698 1918 5606 1411 5519 5643 3390 7864 5039 289 4252 1074 3347 4679 4718 4088 4559 2673 2270 6707 5700 1488 859 3224 5849 2685 1468 3583 5654 3942 139 5372 7502 3391 7975 7539 1947 831 1512 458 6613 7466 3622 5657 5461 4150 5696 2865 7592 1832 3298 2123 7848 1762 2251 2745 1425 4794 1852 2826 553 3455 646 4097 5665 860 4136 6380 2180 4013 2828 5644 6898 5826 6241 2896 1424 6674 2756 2990 4818 7476 7914 3064 5389 7823 4644 2289 4344 1805 7422 4152 6255 5812 7175 7152 7922 2442 2164 2403 3109 795 5875 6180 2557 810 3110 2626 7627 5081 3680 3864 3340 7011 4385 4855 3209 7710 7863 3575 7384 7499 2367 5139 7894 1546 7281 252 4273 6881 5386 1211 4079 4487 29 3053 7802 4777 3150 3422 7671 31 4432 1902 6451 7229 4002 7945 6425 4145 618 3374 2592 5842 1575 2898 4756 449 6143 4335 7076 2700 3734 2590 1581 6353 5798 3200 5121 3604 4907 5011 7426 3382 417 4703 5235 7401 7348 5481 1419 621 1659 4955 6914 6456 1742 1978 221 1554 2010 4089 4029 2390 2145 5157 3846 347 5972 4185 950 7956 6539 3649 1392 5859 2641 5383 2833 2311 2752 214 2849 2279 3460 4576 1802 1997 3684 3855 1373 6899 5561 5201 1023 4115 6251 3711 3384 2595 435 878 7731 6417 2579 5943 7068 1834 3342 5003 7734 6175 3132 1480 724 6033 7098 4657 4690 1855 4400 7410 3326 3269 7729 1288 4876 4062 3939 195 615 138 6190 3955 2070 3681 6181 5325 2843 1667 5222 6875 4300 1230 1169 7142 1838 710 3350 4106 265 5470 2309 3327 1225 3102 7146 4354 4257 3306 3574 5416 4760 5192 2927 3288 5776 6929 1929 1916 5891 5431 1325 3404 5103 7063 3038 4220 7630 476 2723 4108 3834 4726 3003 6310 1121 1975 1825 1819 5829 7301 30 6355 2411 5732 7635 5630 7352 16 5378 4085 1569 5054 6520 4839 1803 6133 240 51 5400 6001 2689 7742 6135 5356 2493 6735 3992 5466 7498 2206 7983 6844 7464 5465 469 7137 5094 4666 5885 3324 3314 2910 2359 1965 5136 128 156 1547 7031 7363 7336 6889 6467 2857 2124 3615 2876 5529 3817 5976 1088 5170 5756 3137 5832 7393 7079 3152 1806 3008 1385 5916 510 3310 5851 4648 2488 7844 658 4702 1992 1641 1170 2223 4545 6111 2666 3789 1172 5715 1750 6920 5895 2964 7640 2577 7637 3720 5611 660 1083 5912 6364 3452 3967 3341 1148 5138 5935 5144 6483 1104 898 7996 6104 4347 2858 4025 4840 209 4138 644 4177 7892 2695 2282 5346 21 2240 573 2217 3272 4660 3348 6067 5546 2252 10 997 4584 4554 7402 7736 6213 6612 231 1920 2397 3136 1763 1285 2812 527 7831 1904 6736 95 737 5591 7714 110 22 5178 2191 5130 7014 7517 5921 3014 7877 7553 6534 5835 2156 684 2028 4945 2064 869 5364 981 7 2768 3279 3862 6174 6939 6252 3657 7964 3304 6100 6630 2345 1728 3712 1242 6154 2774 5670 789 2538 6589 4788 923 6487 1457 7693 7488 6084 153 5667 6895 1713 5099 4696 4816 582 1791 3173 1595 4265 5532 3952 1532 5368 5097 5777 3281 3869 4686 3847 7337 1260 6282 6883 1314 6009 6071 7852 3547 4597 7655 5381 7183 4190 7218 7704 6 2314 600 726 4585 2853 2469 2387 2972 4060 642 4218 6679 3068 2016 6018 423 4 7248 3066 689 3107 7487 7726 3736 1494 6320 3361 6774 3672 7120 75 4156 2151 5821 6207 4582 3520 4283 2312 5191 2890 920 630 7744 6166 2603 1744 970 3896 6851 1269 4541 1999 6816 4169 2528 3780 4643 4075 5297 4357 5266 7951 7083 90 6683 2677 1454 486 1181 3446 3618 519 4901 7558 4995 2036 7850 6160 5401 194 6516 1492 6784 3243 3267 1124 3415 4547 7185 7030 2126 7256 6121 5104 606 3882 6744 277 3556 3794 1995 3764 7365 4457 6359 4040 2160 703 7942 1008 1889 7169 7776 4700 6490 6344 5171 6528 4899 788 63 2934 1160 2985 4242 6988 5596 299 4255 2174 3550 2829 1281 5572 5120 1573 2613 5271 6026 1673 6056 5007 7728 39 2681 5793 1308 6577 4422 543 4921 1726 1448 7601 4421 4996 6470 2437 3233 1525 1205 1627 4045 369 7616 2654 4632 4240 6400 7096 6502 1538 7238 7941 3199 3535 5491 4107 738 5956 1226 1831 7224 735 7071 2111 4374 7170 1185 5418 7127 6409 4766 2009 5284 1814 823 120 1630 5450 3513 1616 5176 4140 2612 1485 4843 3406 5695 7921 4371 6765 7620 2236 96 2644 6209 7247 4894 2405 7032 181 3770 6012 3195 488 6488 5337 5159 4275 4230 1164 7396 4998 4983 5757 5831 171 6326 7022 4120 5538 6893 1934 5888 399 749 5435 2506 442 7429 4392 381 3521 2108 4575 6804 7859 4205 4626 2710 4639 6726 215 988 7388 1440 4428 6728 6909 7121 2637 5955 2189 603 1983 2875 2034 7361 1469 2218 3079 7493 3914 5253 4417 3146 4566 5299 2679 5225 6093 4709 2762 1304 2138 5062 2438 4012 2209 6367 2264 1183 2758 201 5264 5415 5992 1618 4253 3671 5168 2319 4394 2486 1267 1645 5952 6283 5395 2340 7967 6557 3588 6518 6410 2399 254 696 6116 7495 7241 1707 6890 6178 1689 4113 5133 4697 3569 1229 4346 1408 6136 6058 1222 1470 6479 1709 7197 7269 4093 803 2770 1989 1879 1835 7531 4689 1010 3763 7387 3841 5479 3503 3638 7219 4053 3839 4061 335 5580 4944 2228 5274 237 5289 4614 1766 431 6388 4339 3633 5863 7929 2987 7160 1363 3991 2691 3792 5032 3354 1672 1453 7667 2714 1499 2497 6582 1605 5769 995 2555 7747 398 5866 7568 2977 1245 6455 5474 4216 5905 5900 5781 6235 1761 7511 4121 3323 3572 2081 7372 4122 2392 4440 4202 2414 7149 4672 3540 5633 5165 7166 2869 2448 3975 6747 7612 4373 531 4245 815 2730 6038 1623 3868 1701 5853 3606 883 884 5379 3444 3500 2769 6593 3362 5653 7713 839 1774 5306 7876 141 1821 3701 6292 3769 7157 7126 3785 977 6578 3911 2127 6798 2286 5349 6826 1643 1676 6449 5944 7221 415 5779 229 1945 3854 4383 943 4468 474 1378 7606 1404 311 5846 2069 5877 5206 848 953 33 748 922 6501 7267 1868 2046 2376 7536 7119 176 6896 6633 5446 2941 57 7051 2675 3229 5602 609 2732 5920 2518 407 7222 3365 5898 6825 3367 6244 2131 5456 4306 1056 3893 5537 2928 3480 7816 1450 4318 1793 3095 4426 6087 1910 5699 775 7153 4832 6530 5116 5919 3247 6841 5838 3277 1565 2821 6126 534 5485 4863 2514 4750 1013 2318 4333 930 5101 512 7586 6319 6486 2128 7688 7280 5817 2885 6979 7654 3230 5690 3510 7058 408 5140 4312 6729 4039 4007 3075 1751 7099 3159 6062 2467 2761 4499 1508 4047 3965 891 59 5359 427 4770 2061 6361 3418 7805 2021 6597 4699 5360 4365 3171 7097 462 5083 4637 4123 5375 6021 5587 2527 4886 3369 6201 2694 2868 4871 2066 3999 3451 5391 7350 2029 4102 3648 4909 7622 539 6399 7676 3888 6510 5091 7514 4838 2372 2616 2182 7505 7631 5292 6478 6588 1745 545 6312 1974 4947 116 6970 1005 6535 4530 1822 4167 7961 837 47 4448 2386 664 5473 7896 3024 3621 3642 5782 6780 5906 3505 5304 1648 7878 4313 2014 7991 7989 4493 6441 6389 6769 7406 6078 3849 6525 7827 2594 5847 1337 7696 7862 5398 3878 7624 74 4390 6710 7750 2461 4963 2989 5556 3772 2788 1671 5087 6027 3691 5069 6782 3801 809 7091 3373 1893 2503 7086 3624 6656 5579 4001 5471 5787 5022 225 178 2098 5497 490 1415 1235 3426 2850 92 3486 3339 1505 5714 6570 2481 4588 2368 7434 5127 2370 3292 7095 6566 4472 6008 4287 962 3082 6507 17 2424 7559 4000 6911 4486 4930 7150 558 6360 6598 3852 2165 3309 6089 4348 3431 3188 6799 7752 2265 1899 730 5500 2155 6723 3645 6934 7636 4291 4131 1460 2378 6522 4160 5224 5424 6230 1637 6643 3271 6743 6407 4181 6550 2799 7176 2468 1294 6498 2814 2553 7759 6366 7266 6756 645 5553 6218 3752 6964 \n","Graphs are isomorphic\n","\n"]}],"source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <stdbool.h>\n","#include <cuda_runtime.h>\n","#include <limits.h>\n","#include <string.h>\n","\n","#define FILENAME_QUERY \"data/graph_query_8000.csv\"\n","#define FILENAME_TARGET \"data/graph_target_8000.csv\"\n","#define STREAMS 6\n","#define LABELS 10\n","#define INF 99999\n","\n","int blockSize;\n","__constant__ int constMem[16000];\n","__constant__ int constMemVar;\n","\n","#define CUDA_CHECK_ERROR(err)           \\\n","    if (err != cudaSuccess) {            \\\n","        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n","        printf(\"Error in file: %s, line: %i\\n\", __FILE__, __LINE__); \\\n","        exit(EXIT_FAILURE);              \\\n","    }\n","\n","/***** STRUCTS *****/\n","typedef struct {\n","    int* matrix;\n","    int numVertices;\n","    int* nodesToLabel;\n","    int** labelToNodes;\n","    int* labelsCardinalities;\n","    int* degrees;\n","} Graph;\n","\n","typedef struct {\n","    int *mapping1;  // mapping from query to target\n","    int *mapping2;  // mapping from target to query\n","    int *T1;        // Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are neighbors of nodes that are.\n","    int *T2;\n","    int* T1_out;     //Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti. Cioe nodi che non sono in mapping e non sono vicini di nodi coperti\n","    int* T2_out;\n","} State;\n","\n","typedef struct {\n","    int vertex;\n","    int* candidates;\n","    int sizeCandidates;\n","    int candidateIndex;\n","} Info;\n","\n","typedef struct StackNode {\n","    Info* info;\n","    struct StackNode* next;\n","} StackNode;\n","\n","/***** GRAPH PROTOTYPES *****/\n","void initGraphGPU(Graph*);\n","Graph* createGraph();\n","void addEdge(Graph*, int, int);\n","Graph* readGraph(char*);\n","void printGraph(Graph*);\n","void freeGraph(Graph*);\n","void setLabel(Graph*, int, int);\n","\n","Graph* graphGPU(Graph*);\n","void freeGraphGPU(Graph*);\n","\n","/***** STATE PROTOTYPES *****/\n","State* createStateGPU(Graph*, Graph*, cudaStream_t*);\n","void freeStateGPU(State*);\n","void printState(State*, int);\n","void updateStateGPU(Graph*, Graph*, State*, int, int, cudaStream_t*);\n","void restoreStateGPU(Graph*, Graph*, State*, int, int, cudaStream_t*);\n","\n","/***** VF2++ PROTOTYPES *****/\n","void vf2ppGPU(Graph*, Graph*, State*, Graph*, Graph*, cudaStream_t*);\n","bool checkGraphPropertiesGPU(Graph*, Graph*, Graph*, Graph*, cudaStream_t*);\n","int compare(const void*, const void*);\n","int* orderingGPU(Graph*, Graph*, cudaStream_t*, Graph*);\n","void findRootGPU(Graph*, int*, int*, int*, int*, cudaStream_t*);\n","void processDepthGPU(Graph*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, cudaStream_t*, Graph*);\n","int* findCandidatesGPU(Graph*, Graph*, State*, int, int*, Graph*, Graph*, cudaStream_t*);\n","bool cutISOGPU(Graph*, Graph*, State*, int, int, Graph*, Graph*, cudaStream_t*);\n","\n","/***** STACK PROTOTYPES *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex);\n","StackNode* createStackNode(Info*);\n","void push(StackNode**, Info*);\n","Info* pop(StackNode**);\n","bool isStackEmpty(StackNode*);\n","void freeStack(StackNode*);\n","void printStack(StackNode*);\n","void printInfo(Info*);\n","void freeInfo(Info*);\n","StackNode* createStack();\n","Info* peek(StackNode**);\n","\n","int main() {\n","    blockSize = 256;\n","\n","    Graph* h_g1 = readGraph(FILENAME_QUERY);\n","    Graph* h_g2 = readGraph(FILENAME_TARGET);\n","    Graph* d_g1 = graphGPU(h_g1);\n","    Graph* d_g2 = graphGPU(h_g2);\n","\n","    cudaStream_t streams[STREAMS];\n","\n","    for(int i = 0; i < STREAMS; i++) {\n","        cudaStreamCreate(&streams[i]);\n","    }\n","\n","    State* d_state = createStateGPU(d_g1, d_g2, streams);\n","\n","    vf2ppGPU(d_g1, d_g2, d_state, h_g1, h_g2, streams);\n","\n","    int* mapping1 = (int*)malloc(d_g1->numVertices * sizeof(int));\n","\n","    if(mapping1 == NULL) {\n","        printf(\"Error allocating memory in main\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(mapping1, d_state->mapping1, d_g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    printf(\"Mapping\\n\");\n","    for(int i = 0; i < d_g1->numVertices; i++) {\n","        printf(\"%d -> %d\\n\", i, mapping1[i]);\n","    }\n","\n","    for(int i = 0; i < STREAMS; i++) {\n","        cudaStreamDestroy(streams[i]);\n","    }\n","\n","    freeGraph(h_g1);\n","    freeGraph(h_g2);\n","    freeGraphGPU(d_g1);\n","    freeGraphGPU(d_g2);\n","\n","    freeStateGPU(d_state);\n","\n","    return EXIT_SUCCESS;\n","}\n","\n","__global__ void initArrayKernel(int* d_array, int size, int value) {\n","    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","    if (idx < size) {\n","        d_array[idx] = value;\n","    }\n","}\n","\n","__global__ void initMatrixKernel(int* d_matrix, int V, int value) {\n","    int col = threadIdx.y + blockIdx.y * blockDim.y;\n","    int row = threadIdx.x + blockIdx.x * blockDim.x;\n","\n","    if (row < V && col < V) {\n","        d_matrix[row * V + col] = value;\n","    }\n","}\n","\n","/***** GRAPH FUNCTIONS *****/\n","void initGraphGPU(Graph* g) {\n","    g->matrix = (int*)malloc(g->numVertices * g->numVertices * sizeof(int));\n","    g->nodesToLabel = (int*)malloc(g->numVertices * sizeof(int));\n","    g->labelsCardinalities = (int*)malloc(LABELS * sizeof(int));\n","    g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));\n","    g->degrees = (int*)malloc(g->numVertices * sizeof(int));\n","\n","    if (g->nodesToLabel == NULL || g->labelsCardinalities == NULL || g->labelToNodes == NULL || g->degrees == NULL || g->matrix == NULL) {\n","        printf(\"Error allocating memory in initGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int *d_nodesToLabel, *d_degrees, *d_matrix;\n","\n","    cudaStream_t stream1, stream2, stream3;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","    cudaStreamCreate(&stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodesToLabel, g->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_degrees, g->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix, g->numVertices * g->numVertices * sizeof(int)));\n","\n","    int gridSize = (g->numVertices + blockSize - 1) / blockSize;\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_nodesToLabel, g->numVertices, -1);\n","    initArrayKernel<<<gridSize, blockSize, 0, stream2>>>(d_degrees, g->numVertices, 0);\n","\n","    int gridSizeX = (g->numVertices + blockSize - 1) / blockSize;\n","    int gridSizeY = (g->numVertices + blockSize - 1) / blockSize;\n","    dim3 gridSizeM(gridSizeX, gridSizeY);\n","\n","    dim3 blockSizeM(blockSize, blockSize);\n","    initMatrixKernel<<<gridSizeM, blockSizeM, 0, stream3>>>(d_matrix, g->numVertices, 0);\n","\n","    for (int label = 0; label < LABELS; label++) {\n","        g->labelsCardinalities[label] = 0;\n","        g->labelToNodes[label] = (int*)malloc(g->numVertices * sizeof(int));\n","    }\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","    cudaStreamSynchronize(stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->nodesToLabel, d_nodesToLabel, g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->degrees, d_degrees, g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->matrix, d_matrix, g->numVertices * g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream1);\n","    cudaStreamDestroy(stream2);\n","    cudaStreamDestroy(stream3);\n","\n","    cudaFree(d_nodesToLabel);\n","    cudaFree(d_degrees);\n","    cudaFree(d_matrix);\n","}\n","\n","void setLabel(Graph* g, int node, int label) {\n","    if (g->nodesToLabel[node] == -1) {\n","        g->nodesToLabel[node] = label;\n","        g->labelsCardinalities[label]++;\n","        g->labelToNodes[label][g->labelsCardinalities[label] - 1] = node;\n","    }\n","}\n","\n","void addEdge(Graph* g, int src, int target) {\n","    g->matrix[src * g->numVertices + target] = 1;\n","    g->matrix[target * g->numVertices + src] = 1;\n","    g->degrees[src]++;\n","    g->degrees[target]++;\n","}\n","\n","Graph* createGraph() {\n","    Graph* g = (Graph*)malloc(sizeof(Graph));\n","\n","    if (g == NULL) {\n","        printf(\"Error allocating memory in createGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    g->matrix = NULL;\n","    g->numVertices = 0;\n","    g->nodesToLabel = NULL;\n","    g->labelsCardinalities = NULL;\n","    g->degrees = NULL;\n","    g->labelToNodes = NULL;\n","    return g;\n","}\n","\n","Graph* readGraph(char* path) {\n","    int src, target, srcLabel, targetLabel;\n","    Graph* g = createGraph();\n","\n","    FILE* f = fopen(path, \"r\");\n","    if (f == NULL) {\n","        printf(\"Error opening file\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    char line[128];\n","    fgets(line, sizeof(line), f);\n","    sscanf(line, \"%*s%*s%*s%d\", &g->numVertices);\n","    fgets(line, sizeof(line), f); // skip the header\n","\n","    initGraphGPU(g);\n","\n","    while (fgets(line, sizeof(line), f)) {\n","        sscanf(line, \"%d,%d,%d,%d\", &src, &target, &srcLabel, &targetLabel);\n","        addEdge(g, src, target);\n","        setLabel(g, src, srcLabel);\n","        setLabel(g, target, targetLabel);\n","    }\n","\n","    fclose(f);\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        g->labelToNodes[label] = (int*)realloc(g->labelToNodes[label], g->labelsCardinalities[label] * sizeof(int));\n","    }\n","\n","    return g;\n","}\n","\n","void printGraph(Graph* g) {\n","    for (int i = 0; i < g->numVertices; i++) {\n","        for (int j = 0; j < g->numVertices; j++) {\n","            printf(\"%d \", g->matrix[i * g->numVertices + j]);\n","        }\n","        printf(\"\\tVertex %d, label %d, degree %d\\n\", i, g->nodesToLabel[i], g->degrees[i]);\n","    }\n","\n","    printf(\"\\nCardinalities\\n\");\n","    for (int i = 0; i < LABELS; i++) {\n","        printf(\"Label %d: %d\\n\", i, g->labelsCardinalities[i]);\n","    }\n","\n","    for(int i = 0; i < LABELS; i++) {\n","       printf(\"\\nLabel %d\\n\", i);\n","       for(int j = 0; j < g->labelsCardinalities[i]; j++) {\n","           printf(\"%d \", g->labelToNodes[i][j]);\n","       }\n","    }\n","}\n","\n","void freeGraph(Graph* g) {\n","    for(int i = 0; i < LABELS; i++) {\n","        free(g->labelToNodes[i]);\n","    }\n","    free(g->labelToNodes);\n","    free(g->matrix);\n","    free(g->nodesToLabel);\n","    free(g->labelsCardinalities);\n","    free(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","Graph* graphGPU(Graph* h_g) {\n","    Graph* d_g = createGraph();\n","\n","    size_t sizeMatrix = h_g->numVertices * h_g->numVertices * sizeof(int);\n","    size_t size1 = h_g->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    d_g->numVertices = h_g->numVertices;\n","    d_g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));   // it is a vector on host wich contains pointers to vectors on device\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->matrix, sizeMatrix));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->nodesToLabel, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->labelsCardinalities, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->degrees, size1));\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->labelToNodes[label], h_g->labelsCardinalities[label] * sizeof(int))); // each vector on device has a size equal to the cardinality of the label\n","    }\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->matrix, h_g->matrix, sizeMatrix, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->nodesToLabel, h_g->nodesToLabel, size1, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->labelsCardinalities, h_g->labelsCardinalities, size2, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->degrees, h_g->degrees, size1, cudaMemcpyHostToDevice));\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_g->labelToNodes[label], h_g->labelToNodes[label], h_g->labelsCardinalities[label] * sizeof(int), cudaMemcpyHostToDevice));\n","    }\n","\n","    return d_g;\n","}\n","\n","void freeGraphGPU(Graph* g) {\n","    for(int label = 0; label < LABELS; label++) {\n","        cudaFree(g->labelToNodes[label]);\n","    }\n","    free(g->labelToNodes);\n","    cudaFree(g->matrix);\n","    cudaFree(g->nodesToLabel);\n","    cudaFree(g->labelsCardinalities);\n","    cudaFree(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","/***** STATE FUNCTIONS *****/\n","State* createStateGPU(Graph* g1, Graph* g2, cudaStream_t* streams) {\n","    State* s = (State*)malloc(sizeof(State));\n","\n","    if (s == NULL) {\n","        printf(\"Error allocating memory in createStateGPU\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    size_t size1 = g1->numVertices * sizeof(int);\n","    size_t size2 = g2->numVertices * sizeof(int);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->mapping1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->mapping2, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T2, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T1_out, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T2_out, size2));\n","\n","    int gridSize1 = (g1->numVertices + blockSize - 1) / blockSize;\n","    int gridSize2 = (g2->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream1>>>(s->mapping1, g1->numVertices, -1);\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream2>>>(s->T1, g1->numVertices, -1);\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream3>>>(s->T1_out, g1->numVertices, 1);\n","\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream1>>>(s->mapping2, g2->numVertices, -1);\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream2>>>(s->T2, g2->numVertices, -1);\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream3>>>(s->T2_out, g2->numVertices, 1);\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","    cudaStreamSynchronize(stream3);\n","\n","    return s;\n","}\n","\n","void freeStateGPU(State* s) {\n","    cudaFree(s->mapping1);\n","    cudaFree(s->mapping2);\n","    cudaFree(s->T1);\n","    cudaFree(s->T2);\n","    cudaFree(s->T1_out);\n","    cudaFree(s->T2_out);\n","    free(s);\n","    s = NULL;\n","}\n","\n","void printState(State* s, int numVertices) {\n","    printf(\"Mapping 1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping1[i]);\n","    }\n","\n","    printf(\"\\nMapping 2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping2[i]);\n","    }\n","\n","    printf(\"\\nT1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1[i]);\n","    }\n","\n","    printf(\"\\nT2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2[i]);\n","    }\n","\n","    printf(\"\\nT1_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1_out[i]);\n","    }\n","\n","    printf(\"\\nT2_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2_out[i]);\n","    }\n","}\n","\n","__global__ void updateStateKernel(int* matrix, int V, int* mapping, int* T, int* T_out, int node1, int node2) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( idx < V) {\n","        if(idx == node1) {\n","            mapping[idx] = node2;\n","        }\n","\n","        __syncthreads();\n","\n","        if(matrix[node1 * V + idx] == 1 && mapping[idx] == -1) {\n","            T[idx] = 1;\n","            T_out[idx] = -1;\n","        }\n","\n","        __syncthreads();\n","\n","        if(idx == node1) {\n","            T[idx] = -1;\n","            T_out[idx] = -1;\n","        }\n","    }\n","}\n","\n","void updateStateGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int candidate, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","    updateStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_g1->numVertices, d_state->mapping1, d_state->T1, d_state->T1_out, node, candidate);\n","\n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    updateStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, d_g2->numVertices, d_state->mapping2, d_state->T2, d_state->T2_out, candidate, node);\n","}\n","\n","__global__ void restoreStateKernel(int* matrix, int V, int node, int* T, int* T_out, int offset) {  // offset introduced because of the two streams\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx >= V) {\n","        return;\n","    }\n","\n","    int isAdded = 0;\n","\n","    if(matrix[node * V + idx] == 1) {\n","\n","        if(constMem[offset + idx] != -1) {  // constMem contains mapping1 if offset is 0, mapping2 if offset is V\n","            atomicExch(&T[node], 1);\n","            isAdded = 1;\n","        }\n","        else {\n","            int hasCoveredNeighbor = 0;\n","            for(int adjVertex2 = 0; adjVertex2 < V; adjVertex2++) {\n","                if(matrix[idx * V + adjVertex2] == 1 && constMem[offset + adjVertex2] != -1) {   // constMem contains mapping1 if offset is 0, mapping2 if offset is V\n","                    hasCoveredNeighbor = 1;\n","                    break;\n","                }\n","            }\n","\n","            if(hasCoveredNeighbor == 0) {\n","                T[idx] = -1;\n","                T_out[idx] = 1;\n","            }\n","        }\n","    }\n","\n","    if(isAdded == 0) {\n","        atomicExch(&T_out[node], 1);\n","    }\n","}\n","\n","void restoreStateGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int candidate, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    int value = -1;\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_state->mapping1 + node, &value, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_state->mapping2 + candidate, &value, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping1, size1, 0, cudaMemcpyDeviceToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping2, size2, size1, cudaMemcpyDeviceToDevice, stream2));\n","\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_g1->numVertices, node, d_state->T1, d_state->T1_out, 0);\n","\n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, d_g2->numVertices, candidate, d_state->T2, d_state->T2_out, d_g1->numVertices);\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","}\n","\n","/***** VF2++ FUNCTIONS *****/\n","__global__ void equalKernel(int* arr1, int* arr2, int size, int* ret) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx < size) {\n","        if (arr1[idx] != arr2[idx]) {\n","            *ret = 0;\n","        }\n","    }\n","}\n","\n","int compare(const void* a, const void* b) {\n","    return (*(int*)a - *(int*)b);\n","}\n","\n","bool checkGraphPropertiesGPU(Graph* h_g1, Graph* h_g2, Graph* d_g1, Graph* d_g2, cudaStream_t* streams) {\n","    if (h_g1->numVertices != h_g2->numVertices || h_g1->numVertices == 0 || h_g2->numVertices == 0) {\n","        return false;\n","    }\n","\n","    int h_ret1 = 1, h_ret2 = 1;\n","    int* d_ret1, *d_ret2;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    // first check: the cardinalities of the labels must be the same\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_ret1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_ret1, &h_ret1, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int gridSize1 = (LABELS + blockSize - 1) / blockSize;\n","    equalKernel<<<gridSize1, blockSize, 0, stream1>>>(d_g1->labelsCardinalities, d_g2->labelsCardinalities, LABELS, d_ret1);\n","\n","    // second check: the sequence of the degrees must be the same\n","    int size = h_g1->numVertices;\n","    int *d_tmp1, *d_tmp2;\n","\n","    int* tmp1 = (int*)malloc(size * sizeof(int));\n","    int* tmp2 = (int*)malloc(size * sizeof(int));\n","\n","    memcpy(tmp1, h_g1->degrees, size * sizeof(int));\n","    memcpy(tmp2, h_g2->degrees, size * sizeof(int));\n","\n","    qsort(tmp1, size, sizeof(int), compare);\n","    qsort(tmp2, size, sizeof(int), compare);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_tmp1, size * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_tmp2, size * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_ret2, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_tmp1, tmp1, size * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_tmp2, tmp2, size * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_ret2, &h_ret2, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    int gridSize2 = (size + blockSize - 1) / blockSize;\n","    equalKernel<<<gridSize2, blockSize, 0, stream2>>>(d_tmp1, d_tmp2, size, d_ret2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_ret1, d_ret1, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_ret2, d_ret2, sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    cudaFree(d_ret1);\n","    cudaFree(d_ret2);\n","    cudaFree(d_tmp2);\n","    cudaFree(d_tmp1);\n","\n","    free(tmp1);\n","    free(tmp2);\n","\n","    return h_ret1 && h_ret2;\n","}\n","\n","__global__ void bfsKernel(int* matrix, int V, int* levels, int* d_done, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    __shared__ int s_done;\n","    extern __shared__ int s_levels[];\n","\n","    if(threadIdx.x == 0) {\n","        s_done = 0;\n","    }\n","\n","    // it copies the whole levels array in shared memory\n","    // example: thread 0 loads levels[0] and levels[4] if blockSize is 4\n","    for(int i = threadIdx.x; i < V; i += blockDim.x) {\n","      s_levels[i] = levels[i];\n","    }\n","\n","    __syncthreads();\n","\n","    // levels is used as visited too\n","    if(idx < V && s_levels[idx] == depth) {    // it blocks all thread with size greater than V and all threads not at the current depth\n","\n","        for(int adjVertex = 0; adjVertex < V; adjVertex++) {\n","            if(matrix[idx * V + adjVertex] == 1 && s_levels[adjVertex] == -1) {\n","                atomicExch(&levels[adjVertex], depth + 1);\n","                s_done = 1;\n","            }\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {\n","        atomicExch(d_done, s_done);\n","    }\n","}\n","\n","__global__ void maxRarityConstMemKernel(int V, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = constMem[d_nodesToLabel[idx]];    // constMem contains labelRarity\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityConstMemFilterKernel(int V, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(constMem[d_nodesToLabel[idx]] != *d_maxRarity) {     // constMem contains labelRarity\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void maxDegreeKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_degrees[idx];\n","    }\n","    else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxDegree, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxDegreeFilterKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_degrees[idx] != *d_maxDegree) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void findNodeKernel(int V, int* is_good, int* d_node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if(idx < V && is_good[idx]) {\n","        atomicExch(d_node, idx);\n","    }\n","}\n","\n","void findRootGPU(Graph* d_g, int* d_root, int* d_is_good, int* d_maxRarity, int* d_maxDegree, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    int h_maxRarity = INF, h_maxDegree = -INF;\n","    int gridSize = (d_g->numVertices + blockSize - 1) / blockSize;\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_g->numVertices, 1);\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxRarity, &h_maxRarity, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxDegree, &h_maxDegree, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_g->labelsCardinalities, size2, 0, cudaMemcpyDeviceToDevice, stream2)); // constMem contains labelRarity (d_g1->labelsCardinalities)\n","\n","    size_t sharedMemSize = blockSize * sizeof(int); // each block has a shared memory of size blockSize\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    maxRarityConstMemKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","    maxRarityConstMemFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","    maxDegreeKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","    maxDegreeFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","    findNodeKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_is_good, d_root);\n","}\n","\n","__global__ void findLevelNodesKernel(int* levels, int depth, int* levelNodes, int V, int* levelSize) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    __shared__ int s_levelSize;\n","\n","    if(threadIdx.x == 0) {\n","        s_levelSize = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < V && levels[idx] == depth) {\n","        atomicAdd(&s_levelSize, 1);\n","        levelNodes[idx] = 1;\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {\n","        atomicAdd(levelSize, s_levelSize);\n","    }\n","}\n","\n","__global__ void initBfsKernel(int* levels, int V, int* root, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(idx == *root) {\n","            levels[idx] = depth;\n","        }\n","        else {\n","            levels[idx] = -1;\n","        }\n","    }\n","}\n","\n","// __global__ void printArrayKernel(int* arr, int V) {\n","//     int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","//     if( idx == 0)\n","//       for(int i = 0; i < V; i++)\n","//         printf(\"%d \", arr[i]);\n","// }\n","\n","// __global__ void printVarKernel(int* d) {\n","//     printf(\" .%d. \", *d);\n","// }\n","\n","int* orderingGPU(Graph* d_g1, Graph* d_g2, cudaStream_t* streams, Graph* h_g1) {\n","    // findRootGPU\n","    int *d_root, *d_is_good, *d_maxRarity, *d_maxDegree;\n","\n","    // BFS\n","    int* d_levels, *d_done;\n","    int* h_done;\n","    int depth = 0;\n","\n","    // findLevelNodesKernel\n","    int *d_levelNodes, *d_levelSize;\n","\n","    // processDepth\n","    int *d_V1Unordered, *d_labelRarity, *d_connectivityG1, *d_maxConnectivity;  // d_root is reused as d_nextNode\n","    int* order = (int*)malloc(d_g1->numVertices * sizeof(int));   // order of the nodes of g1\n","    int order_index = 0;\n","\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_root, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_is_good, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxDegree, sizeof(int)));\n","\n","    findRootGPU(d_g1, d_root, d_is_good, d_maxRarity, d_maxDegree, streams);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levels, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_done, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_done, sizeof(int))); // pinned memory for faster host-device communication\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelNodes, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelSize, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_V1Unordered, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelRarity, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_connectivityG1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxConnectivity, sizeof(int)));\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream2>>>(d_V1Unordered, d_g1->numVertices, -1); // stream2 empty because already sync in findRootGPU\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_labelRarity, d_g1->labelsCardinalities, size2, cudaMemcpyDeviceToDevice, stream3));\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_connectivityG1, 0, size1, stream4));\n","\n","    initBfsKernel<<<gridSize, blockSize, 0, stream1>>>(d_levels, d_g1->numVertices, d_root, depth); // executed after findRootGPU because of same stream\n","\n","    size_t sharedMemSize = size1;\n","\n","    do {\n","        *h_done = 0;\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_done, 0, sizeof(int), stream1));\n","\n","        bfsKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g1->matrix, d_g1->numVertices, d_levels, d_done, depth);\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_done, d_done, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","        depth++;\n","\n","        cudaStreamSynchronize(stream1);\n","    } while(*h_done);\n","\n","    cudaStreamSynchronize(stream2); // bfs at stream1 is implicitly synchronized\n","    cudaStreamSynchronize(stream3);\n","    cudaStreamSynchronize(stream4);\n","\n","    for (int d = 0; d < depth; d++) {\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_levelNodes, 0, size1, stream2));\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_levelSize, 0, sizeof(int), stream1));\n","\n","        cudaStreamSynchronize(stream2);\n","\n","        findLevelNodesKernel<<<gridSize, blockSize, 0, stream1>>>(d_levels, d, d_levelNodes, d_g1->numVertices, d_levelSize);\n","\n","        processDepthGPU(d_g1, order, &order_index, d_connectivityG1, d_labelRarity, d_V1Unordered, d_levelNodes, d_levelSize,\n","                        d_is_good, d_maxRarity, d_maxDegree, d_maxConnectivity, d_root, streams, h_g1);\n","    }\n","\n","    // findRootGPU\n","    cudaFree(d_root);\n","    cudaFree(d_is_good);\n","    cudaFree(d_maxRarity);\n","    cudaFree(d_maxDegree);\n","\n","    //BFS\n","    cudaFreeHost(h_done);\n","    cudaFree(d_done);\n","    cudaFree(d_levels);\n","\n","    // findLevelNodesKernel\n","    cudaFree(d_levelNodes);\n","    cudaFree(d_levelSize);\n","\n","    // processDepth\n","    cudaFree(d_V1Unordered);\n","    cudaFree(d_labelRarity);\n","    cudaFree(d_connectivityG1);\n","    cudaFree(d_maxConnectivity);\n","\n","    return order;\n","}\n","\n","__global__ void maxConnectivityKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {         // d_is_good already contains the information about the nodes of the current level\n","        int conn = d_connectivityG1[idx];\n","        s_data[threadIdx.x] = conn;\n","    } else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxConnectivity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxConnectivityFilterKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_connectivityG1[idx] != *d_maxConnectivity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void unorderedFilterKernel(int* d_is_good, int* d_V1Unordered, int* d_levelNodes, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= V) {\n","        return;\n","    }\n","\n","    if(d_levelNodes[idx]) {\n","        if(d_V1Unordered[idx] == 1) {\n","            d_is_good[idx] = 0;\n","        }\n","    } else {\n","        d_is_good[idx] = 0;\n","    }\n","}\n","\n","__global__ void updateConnKernel(int* matrix, int V, int* connectivity, int node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(matrix[node * V + idx] == 1) {\n","            atomicAdd(&connectivity[idx], 1);\n","        }\n","    }\n","}\n","\n","// we need to update labelRarity so we cannot use constMem in order to avoid overhead\n","__global__ void maxRarityKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_labelRarity[d_nodesToLabel[idx]];\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityFilterKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_labelRarity[d_nodesToLabel[idx]] != *d_maxRarity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","void processDepthGPU(Graph* d_g, int* order, int* order_index, int* d_connectivityG1, int* d_labelRarity, int* d_V1Unordered, int* d_levelNodes,\n","                    int* d_levelSize, int* d_is_good, int* d_maxRarity, int* d_maxDegree, int* d_maxConnectivity, int* d_nextNode, cudaStream_t* streams,\n","                    Graph* h_g) {\n","\n","    size_t size1 = d_g->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    int h_levelSize;\n","    int* h_nodesToLabel = h_g->nodesToLabel;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_levelSize, d_levelSize, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","    // pinned memory for faster host-device communication\n","    int *h_maxRarity, *h_maxDegree, *h_maxConnectivity, *h_nextNode;\n","    int* h_labelRarity, *h_V1Unordered;\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxDegree, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxConnectivity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_nextNode, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_labelRarity, size2));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_V1Unordered, size1));\n","\n","    *h_maxRarity = INF; *h_maxDegree = -INF; *h_maxConnectivity = -INF;\n","\n","    int gridSize = (d_g->numVertices + blockSize - 1) / blockSize;\n","    size_t sharedMemSize = blockSize * sizeof(int);\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    while(h_levelSize > 0) {\n","        initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_g->numVertices, 1);\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxRarity, h_maxRarity, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxDegree, h_maxDegree, sizeof(int), cudaMemcpyHostToDevice, stream3));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxConnectivity, h_maxConnectivity, sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","        cudaStreamSynchronize(stream4);\n","\n","        unorderedFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_V1Unordered, d_levelNodes, d_g->numVertices);\n","        maxConnectivityKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_connectivityG1, d_maxConnectivity, d_is_good, d_g->numVertices);\n","        maxConnectivityFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_connectivityG1, d_maxConnectivity, d_is_good, d_g->numVertices);\n","        maxDegreeKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","        maxDegreeFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","        maxRarityKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_labelRarity, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","        maxRarityFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_labelRarity, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","        findNodeKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_is_good, d_nextNode);\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_nextNode, d_nextNode, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_labelRarity, d_labelRarity, size2, cudaMemcpyDeviceToHost, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_V1Unordered, d_V1Unordered, size1, cudaMemcpyDeviceToHost, stream3));\n","\n","        cudaStreamSynchronize(stream1);\n","\n","        updateConnKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->matrix, d_g->numVertices, d_connectivityG1, *h_nextNode);\n","        order[(*order_index)++] = *h_nextNode;\n","        h_levelSize--;\n","\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","\n","        h_labelRarity[h_nodesToLabel[*h_nextNode]]--;\n","        h_V1Unordered[*h_nextNode] = 1;\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_labelRarity, h_labelRarity, size2, cudaMemcpyHostToDevice, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_V1Unordered, h_V1Unordered, size1, cudaMemcpyHostToDevice, stream3));\n","\n","        cudaStreamSynchronize(stream1);\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","    }\n","\n","    cudaFreeHost(h_maxRarity);\n","    cudaFreeHost(h_maxDegree);\n","    cudaFreeHost(h_maxConnectivity);\n","    cudaFreeHost(h_nextNode);\n","    cudaFreeHost(h_labelRarity);\n","    cudaFreeHost(h_V1Unordered);\n","}\n","\n","__global__ void findCoveredNeighborsKernel(int* matrix1, int* mapping1, int node, int* coveredNeighbors, int* coveredNeighborsSize,\n","                                            int numVertices) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= numVertices)\n","        return;\n","\n","    if(matrix1[node * numVertices + idx] == 1 && mapping1[idx] != -1) {\n","        int index = atomicAdd(coveredNeighborsSize, 1);\n","        coveredNeighbors[index] = idx;\n","    }\n","}\n","\n","__global__ void findCandidatesKernel(int g1_label, int maxSizeCandidates, int* g2_vertexList, int g1_degree, int* g2_degrees, int* T2_out,\n","                                    int* mapping2, int* candidates, int* candidateSize, int g2_numVertices, int* commonNodes, int* g2_matrix,\n","                                    int* g2_nodesToLabel, int offset) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(constMemVar == 0) {  // constMemVar contains coveredNeighborsSize\n","\n","        if(idx < maxSizeCandidates) {\n","            int vertex = g2_vertexList[idx];  // g2_labelToNodes[label]\n","\n","            if(g2_degrees[vertex] == g1_degree && T2_out[vertex] == 1 && mapping2[vertex] == -1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = vertex;\n","            }\n","        }\n","    }\n","    else {\n","\n","        if(idx < g2_numVertices) {\n","           commonNodes[idx] = 1;\n","\n","            for (int i = 0; i < constMemVar; i++) {\n","                int nbrG1 = constMem[i];        // constMem contains coveredNeighbors with offset 0\n","                int mappedG2 = constMem[offset + nbrG1]; // constMem contains mapping1 with offset degrees[node]\n","                if (g2_matrix[mappedG2 * g2_numVertices + idx] == 0) {\n","                    commonNodes[idx] = 0;\n","                }\n","            }\n","\n","            if (commonNodes[idx] && mapping2[idx] != -1) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_degrees[idx] != g1_degree) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_nodesToLabel[idx] != g1_label) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] == 1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = idx;\n","            }\n","        }\n","    }\n","}\n","\n","int* findCandidatesGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int* sizeCandidates, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    size_t neighSize = h_g1->degrees[node] * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","\n","    // writes on constMem must be sequential\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping1, size1, neighSize, cudaMemcpyDeviceToDevice, stream2));    // neighSize is the offset\n","\n","    int* d_coveredNeighbors, *d_coveredNeighborsSize;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighbors, neighSize));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighborsSize, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_coveredNeighborsSize, 0, sizeof(int), stream1));\n","\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findCoveredNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_state->mapping1, node, d_coveredNeighbors,\n","                                d_coveredNeighborsSize, d_g1->numVertices);\n","\n","    int g1_label = h_g1->nodesToLabel[node];\n","    int g1_degree = h_g1->degrees[node];\n","    int maxSizeCandidates = h_g2->labelsCardinalities[g1_label];\n","\n","    int* d_candidates, *d_candidateSize, *d_commonNodes;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidates, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidateSize, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_commonNodes, size2));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_candidateSize, 0, sizeof(int), stream2));\n","\n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStreamSynchronize(stream2); // transfer of constMem in the stream2 must be finished before the second write on constMem\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_coveredNeighbors, neighSize, 0, cudaMemcpyDeviceToDevice, stream1));   // it is queued before the kernel call on stream1 so implicit synchronization\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMemVar, d_coveredNeighborsSize, sizeof(int), 0, cudaMemcpyDeviceToDevice, stream1)); // constMemVar contains coveredNeighborsSize\n","\n","    findCandidatesKernel<<<gridSize, blockSize, 0, stream1>>>(g1_label, maxSizeCandidates, d_g2->labelToNodes[g1_label], g1_degree,\n","                            d_g2->degrees, d_state->T2_out, d_state->mapping2, d_candidates, d_candidateSize, d_g2->numVertices,\n","                            d_commonNodes, d_g2->matrix, d_g2->nodesToLabel, h_g1->degrees[node]);\n","\n","    int* candidates = (int*)malloc(maxSizeCandidates * sizeof(int));\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(candidates, d_candidates, maxSizeCandidates * sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(sizeCandidates, d_candidateSize, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    cudaFree(d_coveredNeighbors);\n","    cudaFree(d_coveredNeighborsSize);\n","\n","    cudaFree(d_candidates);\n","    cudaFree(d_candidateSize);\n","    cudaFree(d_commonNodes);\n","\n","    return candidates;\n","}\n","\n","\n","void vf2ppGPU(Graph* d_g1, Graph* d_g2, State* d_state, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    if (!checkGraphPropertiesGPU(h_g1, h_g2, d_g1, d_g2, streams)) {\n","        return;\n","    }\n","\n","    int* order = orderingGPU(d_g1, d_g2, streams, h_g1);\n","\n","     //printf(\"Order:\\t\");\n","     //for(int i = 0; i < h_g1->numVertices; i++) {\n","     //   printf(\"%d \", order[i]);\n","     //}\n","     //printf(\"\\n\");\n","\n","    int sizeCandidates = 0;\n","    int* candidates = findCandidatesGPU(d_g1, d_g2, d_state, order[0], &sizeCandidates, h_g1, h_g2, streams);\n","\n","    StackNode* stack = createStack();\n","    Info* info = createInfo(candidates, sizeCandidates, order[0]);\n","    push(&stack, info);\n","\n","    int matchingNode = 1;\n","    while (!isStackEmpty(stack)) {\n","        Info* info = peek(&stack);\n","        bool isMatch = false;\n","\n","        // printInfo(info);\n","\n","        for(int i = info->candidateIndex; i < info->sizeCandidates; i++) {\n","            int candidate = info->candidates[i];\n","            info->candidateIndex = i + 1;\n","\n","            int ret = cutISOGPU(d_g1, d_g2, d_state, info->vertex, candidate, h_g1, h_g2, streams);\n","\n","            // printf(\"CutISO: %d\\n\", ret);\n","            // printf(\"\\n\");\n","\n","            if(!ret) {\n","                // printf(\"\\nMatch %d -> %d\\n\", info->vertex, candidate);\n","\n","                updateStateGPU(d_g1, d_g2, d_state, info->vertex, candidate, streams);\n","\n","                if(matchingNode >= d_g1->numVertices) {\n","                    freeStack(stack);\n","                    free(order);\n","                    printf(\"Graphs are isomorphic\\n\");\n","                    cudaStreamSynchronize(streams[0]);  // wait for updates on the state\n","                    cudaStreamSynchronize(streams[1]);\n","                    return;\n","                }\n","\n","                cudaStreamSynchronize(streams[0]);\n","                cudaStreamSynchronize(streams[1]);\n","\n","                candidates = findCandidatesGPU(d_g1, d_g2, d_state, order[matchingNode], &sizeCandidates, h_g1, h_g2, streams);\n","                Info* info = createInfo(candidates, sizeCandidates, order[matchingNode]);\n","                push(&stack, info);\n","                matchingNode++;\n","                isMatch = true;\n","                break;\n","            }\n","        }\n","\n","        // no more candidates\n","        if(!isMatch) {\n","            Info* tmp = pop(&stack);\n","            freeInfo(tmp);\n","            matchingNode--;\n","\n","            // backtracking\n","            if(!isStackEmpty(stack)) {\n","                Info* prevInfo = peek(&stack);\n","                int candidate = prevInfo->candidates[prevInfo->candidateIndex - 1];\n","                restoreStateGPU(d_g1, d_g2, d_state, prevInfo->vertex, candidate, streams);\n","            }\n","        }\n","    }\n","    free(order);\n","    freeStack(stack);\n","}\n","\n","__global__ void findNeighborsKernel(int* matrix, int node, int* neighbors, int* size, int numVertices) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < numVertices) {\n","        if(matrix[node * numVertices + idx] == 1) {\n","            int index = atomicAdd(size, 1);\n","            neighbors[index] = idx;\n","        }\n","    }\n","}\n","\n","__global__ void checkLabelsKernel(int* neighbors1, int nbrSize1, int nbrSize2, int* labelsNbr, int numVertices,\n","    int* g1_nodesToLabel, int* d_result, int offset) {   // idx is the id of the thread in the grid\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < nbrSize1) {\n","        int nbr1 = neighbors1[idx];\n","        int labelNbr1 = g1_nodesToLabel[nbr1];\n","        bool found = false;\n","\n","        for(int i = 0; i < nbrSize2; i++) {\n","            int nbr2 = constMem[i];     // constMem contains the neighbors2 with offset 0\n","            if(labelNbr1 == constMem[offset + nbr2]) {  // constMem contains the g2_nodesToLabel with offset degrees[node]\n","                found = true;\n","                labelsNbr[labelNbr1] = 1;\n","                break;\n","            }\n","        }\n","\n","        if(!found) {\n","            atomicExch(d_result, 0);   // d_result is initialized to 1 by default\n","        }\n","    }\n","}\n","\n","__global__ void findNodesOfLabelKernel(int* neighbors, int* g_nodesToLabel, int label, int maxSize, int* size, int* nodes) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < maxSize) {\n","        int vertex = neighbors[idx];\n","\n","        if(g_nodesToLabel[vertex] == label) {\n","            int index = atomicAdd(size, 1);   // atomicAdd returns the index respect to global memory (so can't be used shared memory)\n","            nodes[index] = vertex;\n","        }\n","    }\n","}\n","\n","__global__ void intersectionCountKernel(int* nodes, int* size, int* stateSet, int* count) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;    // idx is the id of the thread in the grid among all threads\n","    __shared__ int localCount;  // each block has own localCount variable (shared memory)\n","\n","    if(threadIdx.x == 0) {   // only one thread in the block initializes the localCount\n","        localCount = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < *size) {\n","        int vertex = nodes[idx];\n","\n","        if(stateSet[vertex] == 1) {\n","            atomicAdd(&localCount, 1);\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {  // only the thread with id 0 of each block updates the global size\n","        atomicAdd(count, localCount);\n","    }\n","}\n","\n","bool cutISOGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node1, int node2, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","    cudaStream_t stream5 = streams[4];\n","    cudaStream_t stream6 = streams[5];\n","\n","    int nbrSize1 = h_g1->degrees[node1];\n","    int nbrSize2 = h_g2->degrees[node2];\n","\n","    size_t nbrSize1_bytes = nbrSize1 * sizeof(int);\n","    size_t nbrSize2_bytes = nbrSize2 * sizeof(int);\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    size_t labelSize = LABELS * sizeof(int);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_g2->nodesToLabel, size2, nbrSize2_bytes, cudaMemcpyDeviceToDevice, stream3)); // constMem contains g2_nodesToLabel\n","\n","    int* d_neighbors1, *d_neighbors2;\n","    int* d_nbrSize1, *d_nbrSize2;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors2, nbrSize2_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize2, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize2, 0, sizeof(int), stream2));\n","\n","    int gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, node2, d_neighbors2, d_nbrSize2, d_g2->numVertices);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors1, nbrSize1_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize1, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize1, 0, sizeof(int), stream1));\n","\n","    gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, node1, d_neighbors1, d_nbrSize1, d_g1->numVertices);\n","\n","    int* d_labelsNbr;\n","    int* d_result;\n","    int result = 1;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelsNbr, labelSize));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_result, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_labelsNbr, 0, labelSize, stream4));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_result, &result, sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","    gridSize = (nbrSize1 + blockSize - 1) / blockSize;\n","\n","    cudaStreamSynchronize(stream3); // the first write on constMem must be finished before the second write on constMem\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_neighbors2, nbrSize2_bytes, 0, cudaMemcpyDeviceToDevice, stream2)); // constMem contains neighbors2. it is queued after findNeighborsKernel on stream2\n","\n","    cudaStreamSynchronize(stream4);\n","    cudaStreamSynchronize(stream2);\n","\n","    checkLabelsKernel<<<gridSize, blockSize, 0, stream1>>>(d_neighbors1, nbrSize1, nbrSize2, d_labelsNbr, d_g1->numVertices,\n","                        d_g1->nodesToLabel, d_result, nbrSize2);\n","\n","    int* labelsNbr = (int*)malloc(labelSize);\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&result, d_result, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(labelsNbr, d_labelsNbr, labelSize, cudaMemcpyDeviceToHost, stream1));\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    if(result == 0) {\n","        cudaFree(d_neighbors1);\n","        cudaFree(d_neighbors2);\n","        cudaFree(d_nbrSize1);\n","        cudaFree(d_nbrSize2);\n","        cudaFree(d_labelsNbr);\n","        cudaFree(d_result);\n","        free(labelsNbr);\n","        return true;\n","    }\n","\n","    // pinned memory for faster host-device communication\n","    int *h_size1, *h_size2;\n","    int *h_count1, *h_count2, *h_count3, *h_count4;\n","    int *d_count1, *d_count2, *d_count3, *d_count4;\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_size1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_size2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count4, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count4, sizeof(int)));\n","\n","    int *d_nodes_g1, *d_size_g1;\n","    int *d_nodes_g2, *d_size_g2;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g1, nbrSize1_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g2, nbrSize2_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g2, sizeof(int)));\n","\n","    int gridSize1 = (nbrSize1 + blockSize - 1) / blockSize;\n","    int gridSize2 = (nbrSize2 + blockSize - 1) / blockSize;\n","\n","    bool ret = false;\n","    for(int label = 0; label < LABELS; label++) {\n","        if(labelsNbr[label] == 1) {\n","\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g1, 0, sizeof(int), stream1));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g2, 0, sizeof(int), stream2));\n","\n","            findNodesOfLabelKernel<<<gridSize1, blockSize, 0, stream1>>>(d_neighbors1, d_g1->nodesToLabel, label, nbrSize1, d_size_g1, d_nodes_g1);\n","            findNodesOfLabelKernel<<<gridSize2, blockSize, 0, stream2>>>(d_neighbors2, d_g2->nodesToLabel, label, nbrSize2, d_size_g2, d_nodes_g2);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_size1, d_size_g1, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_size2, d_size_g2, sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count1, 0, sizeof(int), stream3));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count2, 0, sizeof(int), stream4));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count3, 0, sizeof(int), stream5));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count4, 0, sizeof(int), stream6));\n","\n","            cudaStreamSynchronize(stream1);\n","            cudaStreamSynchronize(stream2);\n","\n","            gridSize = (*h_size1 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream3>>>(d_nodes_g1, d_size_g1, d_state->T1, d_count1);\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream5>>>(d_nodes_g1, d_size_g1, d_state->T1_out, d_count3);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count1, d_count1, sizeof(int), cudaMemcpyDeviceToHost, stream3));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count3, d_count3, sizeof(int), cudaMemcpyDeviceToHost, stream5));\n","\n","            gridSize = (*h_size2 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream4>>>(d_nodes_g2, d_size_g2, d_state->T2, d_count2);\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream6>>>(d_nodes_g2, d_size_g2, d_state->T2_out, d_count4);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count2, d_count2, sizeof(int), cudaMemcpyDeviceToHost, stream4));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count4, d_count4, sizeof(int), cudaMemcpyDeviceToHost, stream6));\n","\n","            cudaStreamSynchronize(stream3);\n","            cudaStreamSynchronize(stream4);\n","            cudaStreamSynchronize(stream5);\n","            cudaStreamSynchronize(stream6);\n","\n","            if(*h_count1 != *h_count2 || *h_count3 != *h_count4) {\n","                ret = true;\n","                break;\n","            }\n","        }\n","    }\n","\n","    cudaFree(d_neighbors1);\n","    cudaFree(d_neighbors2);\n","    cudaFree(d_nbrSize1);\n","    cudaFree(d_nbrSize2);\n","    cudaFree(d_labelsNbr);\n","    cudaFree(d_result);\n","    free(labelsNbr);\n","\n","    cudaFreeHost(h_size1);\n","    cudaFreeHost(h_size2);\n","    cudaFreeHost(h_count1);\n","    cudaFreeHost(h_count2);\n","    cudaFreeHost(h_count3);\n","    cudaFreeHost(h_count4);\n","\n","    cudaFree(d_count1);\n","    cudaFree(d_count2);\n","    cudaFree(d_count3);\n","    cudaFree(d_count4);\n","\n","    cudaFree(d_nodes_g1);\n","    cudaFree(d_size_g1);\n","    cudaFree(d_nodes_g2);\n","    cudaFree(d_size_g2);\n","\n","    return ret;\n","}\n","\n","/***** STACK FUNCTIONS *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex) {\n","    Info* info = (Info*)malloc(sizeof(Info));\n","    if (info == NULL) {\n","        printf(\"Error allocating memory in createInfo\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    info->vertex = vertex;\n","    info->candidates = (int*)realloc(candidates, sizeCandidates * sizeof(int));\n","    info->sizeCandidates = sizeCandidates;\n","    info->candidateIndex = 0;\n","    return info;\n","}\n","\n","StackNode* createStackNode(Info* info) {\n","    StackNode* node = (StackNode*)malloc(sizeof(StackNode));\n","    if (node == NULL) {\n","        printf(\"Error allocating memory in createStackNode\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    node->info = info;\n","    node->next = NULL;\n","    return node;\n","}\n","\n","void push(StackNode** top, Info* info) {\n","    StackNode* node = createStackNode(info);\n","    node->next = *top;\n","    *top = node;\n","}\n","\n","Info* pop(StackNode** top) {\n","    if (isStackEmpty(*top)) {\n","        printf(\"Stack is empty, cannot pop\\n\");\n","        return NULL;\n","    }\n","    StackNode* node = *top;\n","    Info* info = node->info;\n","    *top = node->next;\n","    free(node);\n","    return info;\n","}\n","\n","bool isStackEmpty(StackNode* top) {\n","    return top == NULL;\n","}\n","\n","void freeStack(StackNode* top) {\n","    while (!isStackEmpty(top)) {\n","        StackNode* node = top;\n","        top = top->next;\n","        freeInfo(node->info);\n","        free(node);\n","    }\n","}\n","\n","void printStack(StackNode* top) {\n","    StackNode* current = top;\n","    while (current != NULL) {\n","        printInfo(current->info);\n","        current = current->next;\n","    }\n","}\n","\n","void printInfo(Info* info) {\n","    printf(\"\\nVertex: %d\\n\", info->vertex);\n","    printf(\"Index seen: %d\\n\", info->candidateIndex);\n","    printf(\"Candidates: \");\n","    for (int i = 0; i < info->sizeCandidates; i++) {\n","        printf(\"%d \", info->candidates[i]);\n","    }\n","    printf(\"\\n\");\n","}\n","\n","void freeInfo(Info* info) {\n","    free(info->candidates);\n","    free(info);\n","}\n","\n","StackNode* createStack() {\n","    return NULL;\n","}\n","\n","Info* peek(StackNode** top) {\n","    return (*top)->info;\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}