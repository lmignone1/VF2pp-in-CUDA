{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20468,"status":"ok","timestamp":1721124314887,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"OVKsPm9V3gQL","outputId":"1bbd7558-9177-4f2e-9002-c69d197602e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","os.chdir(\"drive/MyDrive/VF2pp-in-CUDA/\")\n","os.listdir()"]},{"cell_type":"markdown","metadata":{"id":"h498tzxnwDyi"},"source":["CUDA SETUP"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164693,"status":"ok","timestamp":1721124481855,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"rbh20Z5utmFQ","outputId":"562de352-dc05-4bb7-83f2-ddf4d6ccc9f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nvcc4jupyter\n","  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n","Installing collected packages: nvcc4jupyter\n","Successfully installed nvcc4jupyter-1.2.1\n","Collecting pycuda\n","  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytools>=2011.2 (from pycuda)\n","  Downloading pytools-2024.1.8-py3-none-any.whl (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Collecting mako (from pycuda)\n","  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs>=2.2 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n","Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n","  Downloading siphash24-1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n","Building wheels for collected packages: pycuda\n","  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661204 sha256=1f652d08366177650a9bd139a22ee3dabd6d1f59492f243542b12620291cb3db\n","  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n","Successfully built pycuda\n","Installing collected packages: appdirs, siphash24, mako, pytools, pycuda\n","Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.8 siphash24-1.6\n"]}],"source":["!pip install nvcc4jupyter\n","!pip install pycuda"]},{"cell_type":"markdown","metadata":{"id":"8_P-CD0hwSmY"},"source":["GPU TYPE"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":424,"status":"ok","timestamp":1721124482267,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"ntofwbc0tEKS","outputId":"1c891bec-b1fb-4b83-d04c-509dfda7f16b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Jul 16 10:08:01 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp0pcg8kke\".\n"]}],"source":["!nvidia-smi\n","!nvcc --version\n","%load_ext nvcc4jupyter"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59674,"status":"ok","timestamp":1721124905952,"user":{"displayName":"LORENZO MIGNONE","userId":"02647048791547825809"},"user_tz":-120},"id":"QNGy3BpeqZGs","outputId":"803605dc-7575-4224-b067-15b57052cb37"},"outputs":[],"source":["%%cuda\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <stdbool.h>\n","#include <cuda_runtime.h>\n","#include <limits.h>\n","#include <string.h>\n","\n","#define FILENAME_QUERY \"data/graph_query_3000_2.csv\"\n","#define FILENAME_TARGET \"data/graph_target_3000_2.csv\"\n","#define STREAMS 6\n","#define LABELS 10\n","#define INF 99999\n","#define MAXCONSTMEM 16000\n","\n","int blockSize;\n","__constant__ int constMem[MAXCONSTMEM];\n","__constant__ int constMemVar;\n","\n","#define CUDA_CHECK_ERROR(err)           \\\n","    if (err != cudaSuccess) {            \\\n","        printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n","        printf(\"Error in file: %s, line: %i\\n\", __FILE__, __LINE__); \\\n","        exit(EXIT_FAILURE);              \\\n","    }\n","\n","/***** STRUCTS *****/\n","typedef struct {\n","    int* matrix;\n","    int numVertices;\n","    int* nodesToLabel;\n","    int** labelToNodes;\n","    int* labelsCardinalities;\n","    int* degrees;\n","} Graph;\n","\n","typedef struct {\n","    int *mapping1;  // mapping from query to target\n","    int *mapping2;  // mapping from target to query\n","    int *T1;        // Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are neighbors of nodes that are.\n","    int *T2;\n","    int* T1_out;     //Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti. Cioe nodi che non sono in mapping e non sono vicini di nodi coperti\n","    int* T2_out;\n","} State;\n","\n","typedef struct {\n","    int vertex;\n","    int* candidates;\n","    int sizeCandidates;\n","    int candidateIndex;\n","} Info;\n","\n","typedef struct StackNode {\n","    Info* info;\n","    struct StackNode* next;\n","} StackNode;\n","\n","/***** GRAPH PROTOTYPES *****/\n","void initGraphGPU(Graph*);\n","Graph* createGraph();\n","void addEdge(Graph*, int, int);\n","Graph* readGraph(char*);\n","void printGraph(Graph*);\n","void freeGraph(Graph*);\n","void setLabel(Graph*, int, int);\n","\n","Graph* graphGPU(Graph*);\n","void freeGraphGPU(Graph*);\n","\n","/***** STATE PROTOTYPES *****/\n","State* createStateGPU(Graph*, Graph*, cudaStream_t*);\n","void freeStateGPU(State*);\n","void printState(State*, int);\n","void updateStateGPU(Graph*, Graph*, State*, int, int, cudaStream_t*);\n","void restoreStateGPU(Graph*, Graph*, State*, int, int, cudaStream_t*);\n","\n","/***** VF2++ PROTOTYPES *****/\n","void vf2ppGPU(Graph*, Graph*, State*, Graph*, Graph*, cudaStream_t*);\n","bool checkGraphPropertiesGPU(Graph*, Graph*, Graph*, Graph*, cudaStream_t*);\n","int compare(const void*, const void*);\n","int* orderingGPU(Graph*, Graph*, cudaStream_t*, Graph*);\n","void findRootGPU(Graph*, int*, int*, int*, int*, cudaStream_t*);\n","void processDepthGPU(Graph*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, int*, cudaStream_t*, Graph*);\n","int* findCandidatesGPU(Graph*, Graph*, State*, int, int*, Graph*, Graph*, cudaStream_t*);\n","bool cutISOGPU(Graph*, Graph*, State*, int, int, Graph*, Graph*, cudaStream_t*);\n","\n","/***** STACK PROTOTYPES *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex);\n","StackNode* createStackNode(Info*);\n","void push(StackNode**, Info*);\n","Info* pop(StackNode**);\n","bool isStackEmpty(StackNode*);\n","void freeStack(StackNode*);\n","void printStack(StackNode*);\n","void printInfo(Info*);\n","void freeInfo(Info*);\n","StackNode* createStack();\n","Info* peek(StackNode**);\n","\n","int main() {\n","    blockSize = 256;\n","\n","    Graph* h_g1 = readGraph(FILENAME_QUERY);\n","    Graph* h_g2 = readGraph(FILENAME_TARGET);\n","    Graph* d_g1 = graphGPU(h_g1);\n","    Graph* d_g2 = graphGPU(h_g2);\n","\n","    cudaStream_t streams[STREAMS];\n","\n","    for(int i = 0; i < STREAMS; i++) {\n","        cudaStreamCreate(&streams[i]);\n","    }\n","\n","    State* d_state = createStateGPU(d_g1, d_g2, streams);\n","\n","    vf2ppGPU(d_g1, d_g2, d_state, h_g1, h_g2, streams);\n","\n","    int* mapping1 = (int*)malloc(d_g1->numVertices * sizeof(int));\n","\n","    if(mapping1 == NULL) {\n","        printf(\"Error allocating memory in main\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(mapping1, d_state->mapping1, d_g1->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    printf(\"Mapping\\n\");\n","    for(int i = 0; i < d_g1->numVertices; i++) {\n","        printf(\"%d -> %d\\n\", i, mapping1[i]);\n","    }\n","\n","    for(int i = 0; i < STREAMS; i++) {\n","        cudaStreamDestroy(streams[i]);\n","    }\n","\n","    freeGraph(h_g1);\n","    freeGraph(h_g2);\n","    freeGraphGPU(d_g1);\n","    freeGraphGPU(d_g2);\n","\n","    freeStateGPU(d_state);\n","\n","    return EXIT_SUCCESS;\n","}\n","\n","__global__ void initArrayKernel(int* d_array, int size, int value) {\n","    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n","    if (idx < size) {\n","        d_array[idx] = value;\n","    }\n","}\n","\n","__global__ void initMatrixKernel(int* d_matrix, int V, int value) {\n","    int col = threadIdx.y + blockIdx.y * blockDim.y;\n","    int row = threadIdx.x + blockIdx.x * blockDim.x;\n","\n","    if (row < V && col < V) {\n","        d_matrix[row * V + col] = value;\n","    }\n","}\n","\n","/***** GRAPH FUNCTIONS *****/\n","void initGraphGPU(Graph* g) {\n","    g->matrix = (int*)malloc(g->numVertices * g->numVertices * sizeof(int));\n","    g->nodesToLabel = (int*)malloc(g->numVertices * sizeof(int));\n","    g->labelsCardinalities = (int*)malloc(LABELS * sizeof(int));\n","    g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));\n","    g->degrees = (int*)malloc(g->numVertices * sizeof(int));\n","\n","    if (g->nodesToLabel == NULL || g->labelsCardinalities == NULL || g->labelToNodes == NULL || g->degrees == NULL || g->matrix == NULL) {\n","        printf(\"Error allocating memory in initGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    int *d_nodesToLabel, *d_degrees, *d_matrix;\n","\n","    cudaStream_t stream1, stream2, stream3;\n","    cudaStreamCreate(&stream1);\n","    cudaStreamCreate(&stream2);\n","    cudaStreamCreate(&stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodesToLabel, g->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_degrees, g->numVertices * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_matrix, g->numVertices * g->numVertices * sizeof(int)));\n","\n","    int gridSize = (g->numVertices + blockSize - 1) / blockSize;\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_nodesToLabel, g->numVertices, -1);\n","    initArrayKernel<<<gridSize, blockSize, 0, stream2>>>(d_degrees, g->numVertices, 0);\n","\n","    int gridSizeX = (g->numVertices + blockSize - 1) / blockSize;\n","    int gridSizeY = (g->numVertices + blockSize - 1) / blockSize;\n","    dim3 gridSizeM(gridSizeX, gridSizeY);\n","\n","    dim3 blockSizeM(blockSize, blockSize);\n","    initMatrixKernel<<<gridSizeM, blockSizeM, 0, stream3>>>(d_matrix, g->numVertices, 0);\n","\n","    for (int label = 0; label < LABELS; label++) {\n","        g->labelsCardinalities[label] = 0;\n","        g->labelToNodes[label] = (int*)malloc(g->numVertices * sizeof(int));\n","    }\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","    cudaStreamSynchronize(stream3);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->nodesToLabel, d_nodesToLabel, g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->degrees, d_degrees, g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","    CUDA_CHECK_ERROR(cudaMemcpy(g->matrix, d_matrix, g->numVertices * g->numVertices * sizeof(int), cudaMemcpyDeviceToHost));\n","\n","    cudaStreamDestroy(stream1);\n","    cudaStreamDestroy(stream2);\n","    cudaStreamDestroy(stream3);\n","\n","    cudaFree(d_nodesToLabel);\n","    cudaFree(d_degrees);\n","    cudaFree(d_matrix);\n","}\n","\n","void setLabel(Graph* g, int node, int label) {\n","    if (g->nodesToLabel[node] == -1) {\n","        g->nodesToLabel[node] = label;\n","        g->labelsCardinalities[label]++;\n","        g->labelToNodes[label][g->labelsCardinalities[label] - 1] = node;\n","    }\n","}\n","\n","void addEdge(Graph* g, int src, int target) {\n","    g->matrix[src * g->numVertices + target] = 1;\n","    g->matrix[target * g->numVertices + src] = 1;\n","    g->degrees[src]++;\n","    g->degrees[target]++;\n","}\n","\n","Graph* createGraph() {\n","    Graph* g = (Graph*)malloc(sizeof(Graph));\n","\n","    if (g == NULL) {\n","        printf(\"Error allocating memory in createGraph\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    g->matrix = NULL;\n","    g->numVertices = 0;\n","    g->nodesToLabel = NULL;\n","    g->labelsCardinalities = NULL;\n","    g->degrees = NULL;\n","    g->labelToNodes = NULL;\n","    return g;\n","}\n","\n","Graph* readGraph(char* path) {\n","    int src, target, srcLabel, targetLabel;\n","    Graph* g = createGraph();\n","\n","    FILE* f = fopen(path, \"r\");\n","    if (f == NULL) {\n","        printf(\"Error opening file\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    char line[128];\n","    fgets(line, sizeof(line), f);\n","    sscanf(line, \"%*s%*s%*s%d\", &g->numVertices);\n","    fgets(line, sizeof(line), f); // skip the header\n","\n","    initGraphGPU(g);\n","\n","    while (fgets(line, sizeof(line), f)) {\n","        sscanf(line, \"%d,%d,%d,%d\", &src, &target, &srcLabel, &targetLabel);\n","        addEdge(g, src, target);\n","        setLabel(g, src, srcLabel);\n","        setLabel(g, target, targetLabel);\n","    }\n","\n","    fclose(f);\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        g->labelToNodes[label] = (int*)realloc(g->labelToNodes[label], g->labelsCardinalities[label] * sizeof(int));\n","    }\n","\n","    return g;\n","}\n","\n","void printGraph(Graph* g) {\n","    for (int i = 0; i < g->numVertices; i++) {\n","        for (int j = 0; j < g->numVertices; j++) {\n","            printf(\"%d \", g->matrix[i * g->numVertices + j]);\n","        }\n","        printf(\"\\tVertex %d, label %d, degree %d\\n\", i, g->nodesToLabel[i], g->degrees[i]);\n","    }\n","\n","    printf(\"\\nCardinalities\\n\");\n","    for (int i = 0; i < LABELS; i++) {\n","        printf(\"Label %d: %d\\n\", i, g->labelsCardinalities[i]);\n","    }\n","\n","    for(int i = 0; i < LABELS; i++) {\n","       printf(\"\\nLabel %d\\n\", i);\n","       for(int j = 0; j < g->labelsCardinalities[i]; j++) {\n","           printf(\"%d \", g->labelToNodes[i][j]);\n","       }\n","    }\n","}\n","\n","void freeGraph(Graph* g) {\n","    for(int i = 0; i < LABELS; i++) {\n","        free(g->labelToNodes[i]);\n","    }\n","    free(g->labelToNodes);\n","    free(g->matrix);\n","    free(g->nodesToLabel);\n","    free(g->labelsCardinalities);\n","    free(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","Graph* graphGPU(Graph* h_g) {\n","    Graph* d_g = createGraph();\n","\n","    size_t sizeMatrix = h_g->numVertices * h_g->numVertices * sizeof(int);\n","    size_t size1 = h_g->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    d_g->numVertices = h_g->numVertices;\n","    d_g->labelToNodes = (int**)malloc(LABELS * sizeof(int*));   // it is a vector on host wich contains pointers to vectors on device\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->matrix, sizeMatrix));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->nodesToLabel, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->labelsCardinalities, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->degrees, size1));\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        CUDA_CHECK_ERROR(cudaMalloc((void**)&d_g->labelToNodes[label], h_g->labelsCardinalities[label] * sizeof(int))); // each vector on device has a size equal to the cardinality of the label\n","    }\n","\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->matrix, h_g->matrix, sizeMatrix, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->nodesToLabel, h_g->nodesToLabel, size1, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->labelsCardinalities, h_g->labelsCardinalities, size2, cudaMemcpyHostToDevice));\n","    CUDA_CHECK_ERROR(cudaMemcpy(d_g->degrees, h_g->degrees, size1, cudaMemcpyHostToDevice));\n","\n","    for(int label = 0; label < LABELS; label++) {\n","        CUDA_CHECK_ERROR(cudaMemcpy(d_g->labelToNodes[label], h_g->labelToNodes[label], h_g->labelsCardinalities[label] * sizeof(int), cudaMemcpyHostToDevice));\n","    }\n","\n","    return d_g;\n","}\n","\n","void freeGraphGPU(Graph* g) {\n","    for(int label = 0; label < LABELS; label++) {\n","        cudaFree(g->labelToNodes[label]);\n","    }\n","    free(g->labelToNodes);\n","    cudaFree(g->matrix);\n","    cudaFree(g->nodesToLabel);\n","    cudaFree(g->labelsCardinalities);\n","    cudaFree(g->degrees);\n","    free(g);\n","    g = NULL;\n","}\n","\n","/***** STATE FUNCTIONS *****/\n","State* createStateGPU(Graph* g1, Graph* g2, cudaStream_t* streams) {\n","    State* s = (State*)malloc(sizeof(State));\n","\n","    if (s == NULL) {\n","        printf(\"Error allocating memory in createStateGPU\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","\n","    size_t size1 = g1->numVertices * sizeof(int);\n","    size_t size2 = g2->numVertices * sizeof(int);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->mapping1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->mapping2, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T2, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T1_out, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&s->T2_out, size2));\n","\n","    int gridSize1 = (g1->numVertices + blockSize - 1) / blockSize;\n","    int gridSize2 = (g2->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream1>>>(s->mapping1, g1->numVertices, -1);\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream2>>>(s->T1, g1->numVertices, -1);\n","    initArrayKernel<<<gridSize1, blockSize, 0, stream3>>>(s->T1_out, g1->numVertices, 1);\n","\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream1>>>(s->mapping2, g2->numVertices, -1);\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream2>>>(s->T2, g2->numVertices, -1);\n","    initArrayKernel<<<gridSize2, blockSize, 0, stream3>>>(s->T2_out, g2->numVertices, 1);\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","    cudaStreamSynchronize(stream3);\n","\n","    return s;\n","}\n","\n","void freeStateGPU(State* s) {\n","    cudaFree(s->mapping1);\n","    cudaFree(s->mapping2);\n","    cudaFree(s->T1);\n","    cudaFree(s->T2);\n","    cudaFree(s->T1_out);\n","    cudaFree(s->T2_out);\n","    free(s);\n","    s = NULL;\n","}\n","\n","void printState(State* s, int numVertices) {\n","    printf(\"Mapping 1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping1[i]);\n","    }\n","\n","    printf(\"\\nMapping 2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        printf(\"%d \", s->mapping2[i]);\n","    }\n","\n","    printf(\"\\nT1\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1[i]);\n","    }\n","\n","    printf(\"\\nT2\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2[i]);\n","    }\n","\n","    printf(\"\\nT1_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T1_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T1_out[i]);\n","    }\n","\n","    printf(\"\\nT2_out\\n\");\n","    for (int i = 0; i < numVertices; i++) {\n","        if(s->T2_out[i] != -1) {\n","            printf(\"%d \", i);\n","        }\n","        // printf(\"%d \", s->T2_out[i]);\n","    }\n","}\n","\n","__global__ void updateStateKernel(int* matrix, int V, int* mapping, int* T, int* T_out, int node1, int node2) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if( idx < V) {\n","        if(idx == node1) {\n","            mapping[idx] = node2;\n","        }\n","\n","        __syncthreads();\n","\n","        if(matrix[node1 * V + idx] == 1 && mapping[idx] == -1) {\n","            T[idx] = 1;\n","            T_out[idx] = -1;\n","        }\n","\n","        __syncthreads();\n","\n","        if(idx == node1) {\n","            T[idx] = -1;\n","            T_out[idx] = -1;\n","        }\n","    }\n","}\n","\n","void updateStateGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int candidate, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","    updateStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_g1->numVertices, d_state->mapping1, d_state->T1, d_state->T1_out, node, candidate);\n","\n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    updateStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, d_g2->numVertices, d_state->mapping2, d_state->T2, d_state->T2_out, candidate, node);\n","}\n","\n","__global__ void restoreStateKernel(int* matrix, int V, int node, int* T, int* T_out, int offset, int usage, int* mapping) {  // offset introduced because of the two streams\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx >= V) {\n","        return;\n","    }\n","\n","    int isAdded = 0;\n","\n","    if(matrix[node * V + idx] == 1) {\n","\n","        if(usage == 1) {\n","\n","            if(constMem[offset + idx] != -1) {  // constMem contains mapping1 if offset is 0, mapping2 if offset is V\n","                atomicExch(&T[node], 1);\n","                isAdded = 1;\n","            }\n","            else {\n","                int hasCoveredNeighbor = 0;\n","                for(int adjVertex2 = 0; adjVertex2 < V; adjVertex2++) {\n","                    if(matrix[idx * V + adjVertex2] == 1 && constMem[offset + adjVertex2] != -1) {   // constMem contains mapping1 if offset is 0, mapping2 if offset is V\n","                        hasCoveredNeighbor = 1;\n","                        break;\n","                    }\n","                }\n","\n","                if(hasCoveredNeighbor == 0) {\n","                    T[idx] = -1;\n","                    T_out[idx] = 1;\n","                }\n","            }\n","\n","        } else {\n","\n","            if(mapping[idx] != -1) { \n","                atomicExch(&T[node], 1);\n","                isAdded = 1;\n","            }\n","            else {\n","                int hasCoveredNeighbor = 0;\n","                for(int adjVertex2 = 0; adjVertex2 < V; adjVertex2++) {\n","                    if(matrix[idx * V + adjVertex2] == 1 && mapping[adjVertex2] != -1) {   \n","                        hasCoveredNeighbor = 1;\n","                        break;\n","                    }\n","                }\n","\n","                if(hasCoveredNeighbor == 0) {\n","                    T[idx] = -1;\n","                    T_out[idx] = 1;\n","                }\n","            }\n","\n","        }\n","    }\n","\n","    if(isAdded == 0) {\n","        atomicExch(&T_out[node], 1);\n","    }\n","}\n","\n","void restoreStateGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int candidate, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    int value = -1;\n","\n","    int useConstMem = d_g1->numVertices + d_g2->numVertices < MAXCONSTMEM;\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_state->mapping1 + node, &value, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_state->mapping2 + candidate, &value, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping1, size1, 0, cudaMemcpyDeviceToDevice, stream1));\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping2, size2, size1, cudaMemcpyDeviceToDevice, stream2));\n","    }\n","    \n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_g1->numVertices, node, d_state->T1, d_state->T1_out, 0, useConstMem, d_state->mapping1);\n","    \n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    restoreStateKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, d_g2->numVertices, candidate, d_state->T2, d_state->T2_out, d_g1->numVertices, useConstMem, d_state->mapping2);\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","}\n","\n","/***** VF2++ FUNCTIONS *****/\n","__global__ void equalKernel(int* arr1, int* arr2, int size, int* ret) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if (idx < size) {\n","        if (arr1[idx] != arr2[idx]) {\n","            *ret = 0;\n","        }\n","    }\n","}\n","\n","int compare(const void* a, const void* b) {\n","    return (*(int*)a - *(int*)b);\n","}\n","\n","bool checkGraphPropertiesGPU(Graph* h_g1, Graph* h_g2, Graph* d_g1, Graph* d_g2, cudaStream_t* streams) {\n","    if (h_g1->numVertices != h_g2->numVertices || h_g1->numVertices == 0 || h_g2->numVertices == 0) {\n","        return false;\n","    }\n","\n","    int h_ret1 = 1, h_ret2 = 1;\n","    int* d_ret1, *d_ret2;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    // first check: the cardinalities of the labels must be the same\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_ret1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_ret1, &h_ret1, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","\n","    int gridSize1 = (LABELS + blockSize - 1) / blockSize;\n","    equalKernel<<<gridSize1, blockSize, 0, stream1>>>(d_g1->labelsCardinalities, d_g2->labelsCardinalities, LABELS, d_ret1);\n","\n","    // second check: the sequence of the degrees must be the same\n","    int size = h_g1->numVertices;\n","    int *d_tmp1, *d_tmp2;\n","\n","    int* tmp1 = (int*)malloc(size * sizeof(int));\n","    int* tmp2 = (int*)malloc(size * sizeof(int));\n","\n","    memcpy(tmp1, h_g1->degrees, size * sizeof(int));\n","    memcpy(tmp2, h_g2->degrees, size * sizeof(int));\n","    \n","    qsort(tmp1, size, sizeof(int), compare);\n","    qsort(tmp2, size, sizeof(int), compare);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_tmp1, size * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_tmp2, size * sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_ret2, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_tmp1, tmp1, size * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_tmp2, tmp2, size * sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_ret2, &h_ret2, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","   \n","    int gridSize2 = (size + blockSize - 1) / blockSize;\n","    equalKernel<<<gridSize2, blockSize, 0, stream2>>>(d_tmp1, d_tmp2, size, d_ret2);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_ret1, d_ret1, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_ret2, d_ret2, sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    cudaFree(d_ret1);\n","    cudaFree(d_ret2);\n","    cudaFree(d_tmp2);\n","    cudaFree(d_tmp1);\n","    \n","    free(tmp1);\n","    free(tmp2);\n","\n","    return h_ret1 && h_ret2;\n","}\n","\n","__global__ void bfsKernel(int* matrix, int V, int* levels, int* d_done, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    __shared__ int s_done;\n","    extern __shared__ int s_levels[];\n","\n","    if(threadIdx.x == 0) {\n","        s_done = 0;\n","    }\n","\n","    // it copies the whole levels array in shared memory\n","    // example: thread 0 loads levels[0] and levels[4] if blockSize is 4\n","    for(int i = threadIdx.x; i < V; i += blockDim.x) {\n","      s_levels[i] = levels[i];\n","    }\n","\n","    __syncthreads();\n","\n","    // levels is used as visited too\n","    if(idx < V && s_levels[idx] == depth) {    // it blocks all thread with size greater than V and all threads not at the current depth\n","\n","        for(int adjVertex = 0; adjVertex < V; adjVertex++) {\n","            if(matrix[idx * V + adjVertex] == 1 && s_levels[adjVertex] == -1) {\n","                levels[adjVertex] = depth + 1;  // depth is a \"constant\" so no need to use atomicExch\n","                s_done = 1;\n","            }\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0 && s_done) {\n","        *d_done = 1;\n","    }\n","}\n","\n","__global__ void maxRarityConstMemKernel(int V, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = constMem[d_nodesToLabel[idx]];    // constMem contains labelRarity\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityConstMemFilterKernel(int V, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(constMem[d_nodesToLabel[idx]] != *d_maxRarity) {     // constMem contains labelRarity\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void maxDegreeKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_degrees[idx];\n","    }\n","    else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxDegree, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxDegreeFilterKernel(int V, int* d_degrees, int* d_maxDegree, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_degrees[idx] != *d_maxDegree) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void findNodeKernel(int V, int* is_good, int* d_node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    if(idx < V && is_good[idx]) {\n","        atomicExch(d_node, idx);    // we don't know which thread will be the first to find the node but are equivalent\n","    }\n","}\n","\n","void findRootGPU(Graph* d_g, int* d_root, int* d_is_good, int* d_maxRarity, int* d_maxDegree, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    int h_maxRarity = INF, h_maxDegree = -INF;\n","    int gridSize = (d_g->numVertices + blockSize - 1) / blockSize;\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_g->numVertices, 1);\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxRarity, &h_maxRarity, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxDegree, &h_maxDegree, sizeof(int), cudaMemcpyHostToDevice, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_g->labelsCardinalities, size2, 0, cudaMemcpyDeviceToDevice, stream2)); // constMem contains labelRarity (d_g1->labelsCardinalities)\n","\n","    size_t sharedMemSize = blockSize * sizeof(int); // each block has a shared memory of size blockSize\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    maxRarityConstMemKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","    maxRarityConstMemFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","    maxDegreeKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","    maxDegreeFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","    findNodeKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_is_good, d_root);\n","}\n","\n","__global__ void findLevelNodesKernel(int* levels, int depth, int* levelNodes, int V, int* levelSize) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    __shared__ int s_levelSize;\n","\n","    if(threadIdx.x == 0) {\n","        s_levelSize = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < V && levels[idx] == depth) {\n","        atomicAdd(&s_levelSize, 1);\n","        levelNodes[idx] = 1;\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {\n","        atomicAdd(levelSize, s_levelSize);\n","    }\n","}\n","\n","__global__ void initBfsKernel(int* levels, int V, int* root, int depth) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    \n","    if(idx < V) {\n","        if(idx == *root) {\n","            levels[idx] = depth;\n","        }\n","        else {\n","            levels[idx] = -1;\n","        }\n","    }\n","}\n","\n","int* orderingGPU(Graph* d_g1, Graph* d_g2, cudaStream_t* streams, Graph* h_g1) {\n","    // findRootGPU\n","    int *d_root, *d_is_good, *d_maxRarity, *d_maxDegree;\n","\n","    // BFS\n","    int* d_levels, *d_done;\n","    int* h_done;\n","    int depth = 0;\n","\n","    // findLevelNodesKernel\n","    int *d_levelNodes, *d_levelSize;\n","\n","    // processDepth\n","    int *d_V1Unordered, *d_labelRarity, *d_connectivityG1, *d_maxConnectivity;  // d_root is reused as d_nextNode\n","    int* order = (int*)malloc(d_g1->numVertices * sizeof(int));   // order of the nodes of g1\n","    int order_index = 0;\n","\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_root, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_is_good, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxDegree, sizeof(int)));\n","    \n","    findRootGPU(d_g1, d_root, d_is_good, d_maxRarity, d_maxDegree, streams);\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levels, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_done, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_done, sizeof(int))); // pinned memory for faster host-device communication\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelNodes, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_levelSize, sizeof(int)));\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_V1Unordered, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelRarity, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_connectivityG1, size1));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_maxConnectivity, sizeof(int)));\n","\n","    initArrayKernel<<<gridSize, blockSize, 0, stream2>>>(d_V1Unordered, d_g1->numVertices, -1); // stream2 empty because already sync in findRootGPU\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_labelRarity, d_g1->labelsCardinalities, size2, cudaMemcpyDeviceToDevice, stream3));\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_connectivityG1, 0, size1, stream4));\n","\n","    initBfsKernel<<<gridSize, blockSize, 0, stream1>>>(d_levels, d_g1->numVertices, d_root, depth); // executed after findRootGPU because of same stream\n","    \n","    size_t sharedMemSize = size1;\n","\n","    do {\n","        *h_done = 0;\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_done, 0, sizeof(int), stream1));\n","\n","        bfsKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g1->matrix, d_g1->numVertices, d_levels, d_done, depth);\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_done, d_done, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","        depth++;\n","       \n","        cudaStreamSynchronize(stream1);\n","    } while(*h_done);\n","  \n","    cudaStreamSynchronize(stream2); // bfs at stream1 is implicitly synchronized \n","    cudaStreamSynchronize(stream3);\n","    cudaStreamSynchronize(stream4);\n","    \n","    for (int d = 0; d < depth; d++) {\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_levelNodes, 0, size1, stream2));\n","        CUDA_CHECK_ERROR(cudaMemsetAsync(d_levelSize, 0, sizeof(int), stream1));\n","\n","        cudaStreamSynchronize(stream2);\n","        \n","        findLevelNodesKernel<<<gridSize, blockSize, 0, stream1>>>(d_levels, d, d_levelNodes, d_g1->numVertices, d_levelSize);\n","        \n","        processDepthGPU(d_g1, order, &order_index, d_connectivityG1, d_labelRarity, d_V1Unordered, d_levelNodes, d_levelSize, \n","                        d_is_good, d_maxRarity, d_maxDegree, d_maxConnectivity, d_root, streams, h_g1);\n","    }\n","\n","    // findRootGPU\n","    cudaFree(d_root);\n","    cudaFree(d_is_good);\n","    cudaFree(d_maxRarity);\n","    cudaFree(d_maxDegree);\n","\n","    //BFS\n","    cudaFreeHost(h_done);\n","    cudaFree(d_done);\n","    cudaFree(d_levels);\n","    \n","    // findLevelNodesKernel\n","    cudaFree(d_levelNodes);\n","    cudaFree(d_levelSize);\n","\n","    // processDepth\n","    cudaFree(d_V1Unordered);\n","    cudaFree(d_labelRarity);\n","    cudaFree(d_connectivityG1);\n","    cudaFree(d_maxConnectivity);\n","\n","    return order;\n","}\n","\n","__global__ void maxConnectivityKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {         // d_is_good already contains the information about the nodes of the current level\n","        int conn = d_connectivityG1[idx];\n","        s_data[threadIdx.x] = conn;\n","    } else {\n","        s_data[threadIdx.x] = -INF;\n","    }\n","\n","    __syncthreads();\n","\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = max(s_data[threadIdx.x], s_data[threadIdx.x + s]);\n","        }\n","        __syncthreads();\n","    }\n","\n","    if(threadIdx.x == 0) {\n","        atomicMax(d_maxConnectivity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxConnectivityFilterKernel(int* d_connectivityG1, int* d_maxConnectivity, int* d_is_good, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_connectivityG1[idx] != *d_maxConnectivity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","__global__ void unorderedFilterKernel(int* d_is_good, int* d_V1Unordered, int* d_levelNodes, int V) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= V) {\n","        return;\n","    }\n","\n","    if(d_levelNodes[idx]) {\n","        if(d_V1Unordered[idx] == 1) {\n","            d_is_good[idx] = 0;\n","        }\n","    } else {\n","        d_is_good[idx] = 0;\n","    }\n","}\n","\n","__global__ void updateConnKernel(int* matrix, int V, int* connectivity, int node) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(matrix[node * V + idx] == 1) {\n","            connectivity[idx]++;\n","        }\n","    }\n","}\n","\n","// we need to update labelRarity so we cannot use constMem in order to avoid overhead\n","__global__ void maxRarityKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","    extern __shared__ int s_data[];\n","\n","    if(idx < V && d_is_good[idx]) {\n","        s_data[threadIdx.x] = d_labelRarity[d_nodesToLabel[idx]];\n","    } else {\n","        s_data[threadIdx.x] = INF;\n","    }\n","\n","    __syncthreads();\n","\n","    // parallel reduction: at each step, the block is halved and each thread computes the min of its value with the value of the other one at distance s in\n","    // the same block\n","    for(unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n","        if(threadIdx.x < s) {\n","            s_data[threadIdx.x] = min(s_data[threadIdx.x], s_data[threadIdx.x + s]);   // At the end of the loop, the min value is in sdata[0]\n","        }\n","        __syncthreads();\n","    }\n","\n","    // each thread 0 of each block computes the global min\n","    if(threadIdx.x == 0) {\n","        atomicMin(d_maxRarity, s_data[0]);\n","    }\n","}\n","\n","__global__ void maxRarityFilterKernel(int V, int* d_labelRarity, int* d_nodesToLabel, int* d_maxRarity, int* d_is_good) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < V) {\n","        if(d_labelRarity[d_nodesToLabel[idx]] != *d_maxRarity) {\n","            d_is_good[idx] = 0;\n","        }\n","    }\n","}\n","\n","void processDepthGPU(Graph* d_g, int* order, int* order_index, int* d_connectivityG1, int* d_labelRarity, int* d_V1Unordered, int* d_levelNodes,\n","                    int* d_levelSize, int* d_is_good, int* d_maxRarity, int* d_maxDegree, int* d_maxConnectivity, int* d_nextNode, cudaStream_t* streams,\n","                    Graph* h_g) {\n","    \n","    size_t size1 = d_g->numVertices * sizeof(int);\n","    size_t size2 = LABELS * sizeof(int);\n","\n","    int h_levelSize;\n","    int* h_nodesToLabel = h_g->nodesToLabel;\n","\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&h_levelSize, d_levelSize, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","    // pinned memory for faster host-device communication\n","    int *h_maxRarity, *h_maxDegree, *h_maxConnectivity, *h_nextNode; \n","    int* h_labelRarity, *h_V1Unordered;\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxRarity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxDegree, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_maxConnectivity, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_nextNode, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_labelRarity, size2));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_V1Unordered, size1));       \n","\n","    *h_maxRarity = INF; *h_maxDegree = -INF; *h_maxConnectivity = -INF;\n","\n","    int gridSize = (d_g->numVertices + blockSize - 1) / blockSize;\n","    size_t sharedMemSize = blockSize * sizeof(int);\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    while(h_levelSize > 0) {\n","        initArrayKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_g->numVertices, 1);\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxRarity, h_maxRarity, sizeof(int), cudaMemcpyHostToDevice, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxDegree, h_maxDegree, sizeof(int), cudaMemcpyHostToDevice, stream3));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_maxConnectivity, h_maxConnectivity, sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","        cudaStreamSynchronize(stream4);\n","\n","        unorderedFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_is_good, d_V1Unordered, d_levelNodes, d_g->numVertices);\n","        maxConnectivityKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_connectivityG1, d_maxConnectivity, d_is_good, d_g->numVertices);\n","        maxConnectivityFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_connectivityG1, d_maxConnectivity, d_is_good, d_g->numVertices);\n","        maxDegreeKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);   \n","        maxDegreeFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_g->degrees, d_maxDegree, d_is_good);\n","        maxRarityKernel<<<gridSize, blockSize, sharedMemSize, stream1>>>(d_g->numVertices, d_labelRarity, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","        maxRarityFilterKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_labelRarity, d_g->nodesToLabel, d_maxRarity, d_is_good);\n","        findNodeKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->numVertices, d_is_good, d_nextNode);\n","        \n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_nextNode, d_nextNode, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_labelRarity, d_labelRarity, size2, cudaMemcpyDeviceToHost, stream2));        \n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(h_V1Unordered, d_V1Unordered, size1, cudaMemcpyDeviceToHost, stream3)); \n","        \n","        cudaStreamSynchronize(stream1);\n","        \n","        updateConnKernel<<<gridSize, blockSize, 0, stream1>>>(d_g->matrix, d_g->numVertices, d_connectivityG1, *h_nextNode);\n","        order[(*order_index)++] = *h_nextNode;\n","        h_levelSize--;\n","\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","\n","        h_labelRarity[h_nodesToLabel[*h_nextNode]]--;\n","        h_V1Unordered[*h_nextNode] = 1;\n","\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_labelRarity, h_labelRarity, size2, cudaMemcpyHostToDevice, stream2));\n","        CUDA_CHECK_ERROR(cudaMemcpyAsync(d_V1Unordered, h_V1Unordered, size1, cudaMemcpyHostToDevice, stream3));\n","\n","        cudaStreamSynchronize(stream1);\n","        cudaStreamSynchronize(stream2);\n","        cudaStreamSynchronize(stream3);\n","    }\n","\n","    cudaFreeHost(h_maxRarity);\n","    cudaFreeHost(h_maxDegree);\n","    cudaFreeHost(h_maxConnectivity);\n","    cudaFreeHost(h_nextNode);\n","    cudaFreeHost(h_labelRarity);\n","    cudaFreeHost(h_V1Unordered);\n","}\n","\n","__global__ void findCoveredNeighborsKernel(int* matrix1, int* mapping1, int node, int* coveredNeighbors, int* coveredNeighborsSize,\n","                                            int numVertices) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx >= numVertices)\n","        return;\n","\n","    if(matrix1[node * numVertices + idx] == 1 && mapping1[idx] != -1) {\n","        int index = atomicAdd(coveredNeighborsSize, 1);\n","        coveredNeighbors[index] = idx;\n","    }\n","}\n","\n","__global__ void findCandidatesKernel(int g1_label, int maxSizeCandidates, int* g2_vertexList, int g1_degree, int* g2_degrees, int* T2_out, \n","                                    int* mapping2, int* candidates, int* candidateSize, int g2_numVertices, int* commonNodes, int* g2_matrix, \n","                                    int* g2_nodesToLabel, int offset, int useConstMem, int* coveredNeighbors, int* mapping1) {\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(constMemVar == 0) {  // constMemVar contains coveredNeighborsSize\n","\n","        if(idx < maxSizeCandidates) {\n","            int vertex = g2_vertexList[idx];  // g2_labelToNodes[label]\n","\n","            if(g2_degrees[vertex] == g1_degree && T2_out[vertex] == 1 && mapping2[vertex] == -1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = vertex;\n","            }\n","        }\n","    }\n","    else {\n","\n","        if(idx < g2_numVertices) {\n","           commonNodes[idx] = 1;\n","\n","            if(useConstMem) {\n","                \n","                for (int i = 0; i < constMemVar; i++) {\n","                    int nbrG1 = constMem[i];        // constMem contains coveredNeighbors with offset 0\n","                    int mappedG2 = constMem[offset + nbrG1]; // constMem contains mapping1 with offset degrees[node]\n","                    if (g2_matrix[mappedG2 * g2_numVertices + idx] == 0) {\n","                        commonNodes[idx] = 0;\n","                    }\n","                }\n","\n","            } else {\n","\n","                for (int i = 0; i < constMemVar; i++) {\n","                    int nbrG1 = coveredNeighbors[i];\n","                    int mappedG2 = mapping1[nbrG1];\n","                    if (g2_matrix[mappedG2 * g2_numVertices + idx] == 0) {\n","                        commonNodes[idx] = 0;\n","                    }\n","                }\n","\n","            }\n","\n","            if (commonNodes[idx] && mapping2[idx] != -1) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_degrees[idx] != g1_degree) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] && g2_nodesToLabel[idx] != g1_label) {\n","                commonNodes[idx] = 0;\n","            }\n","\n","            if (commonNodes[idx] == 1) {\n","                int index = atomicAdd(candidateSize, 1);\n","                candidates[index] = idx;\n","            }\n","        }\n","    }\n","}\n","\n","int* findCandidatesGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node, int* sizeCandidates, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","\n","    size_t neighSize = h_g1->degrees[node] * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","\n","    int useConstMem = d_g1->numVertices + h_g1->degrees[node] < MAXCONSTMEM;\n","\n","    // writes on constMem must be sequential\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_state->mapping1, size1, neighSize, cudaMemcpyDeviceToDevice, stream2));    // neighSize is the offset\n","    }\n","    \n","    int* d_coveredNeighbors, *d_coveredNeighborsSize;\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighbors, neighSize));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_coveredNeighborsSize, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_coveredNeighborsSize, 0, sizeof(int), stream1));\n","    \n","    int gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findCoveredNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, d_state->mapping1, node, d_coveredNeighbors, \n","                                d_coveredNeighborsSize, d_g1->numVertices);\n","\n","    int g1_label = h_g1->nodesToLabel[node];\n","    int g1_degree = h_g1->degrees[node];\n","    int maxSizeCandidates = h_g2->labelsCardinalities[g1_label];\n","    \n","    int* d_candidates, *d_candidateSize, *d_commonNodes;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidates, size2));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_candidateSize, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_commonNodes, size2));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_candidateSize, 0, sizeof(int), stream2));\n","    \n","    gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","    \n","    cudaStreamSynchronize(stream2); // transfer of constMem in the stream2 must be finished before the second write on constMem\n","\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_coveredNeighbors, neighSize, 0, cudaMemcpyDeviceToDevice, stream1));   // it is queued before the kernel call on stream1 so implicit synchronization\n","    }\n","    \n","    CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMemVar, d_coveredNeighborsSize, sizeof(int), 0, cudaMemcpyDeviceToDevice, stream1)); // constMemVar contains coveredNeighborsSize\n","\n","    findCandidatesKernel<<<gridSize, blockSize, 0, stream1>>>(g1_label, maxSizeCandidates, d_g2->labelToNodes[g1_label], g1_degree, \n","                            d_g2->degrees, d_state->T2_out, d_state->mapping2, d_candidates, d_candidateSize, d_g2->numVertices, \n","                            d_commonNodes, d_g2->matrix, d_g2->nodesToLabel, h_g1->degrees[node], useConstMem, d_coveredNeighbors, d_state->mapping1);\n","\n","    int* candidates = (int*)malloc(maxSizeCandidates * sizeof(int));\n","    \n","    cudaStreamSynchronize(stream1);\n","\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(candidates, d_candidates, maxSizeCandidates * sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(sizeCandidates, d_candidateSize, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","\n","    cudaStreamSynchronize(stream1);\n","    cudaStreamSynchronize(stream2);\n","\n","    cudaFree(d_coveredNeighbors);\n","    cudaFree(d_coveredNeighborsSize);\n","\n","    cudaFree(d_candidates);\n","    cudaFree(d_candidateSize);\n","    cudaFree(d_commonNodes);\n","\n","    return candidates;\n","}\n","\n","\n","void vf2ppGPU(Graph* d_g1, Graph* d_g2, State* d_state, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    if (!checkGraphPropertiesGPU(h_g1, h_g2, d_g1, d_g2, streams)) {\n","        return;\n","    }\n","\n","    int* order = orderingGPU(d_g1, d_g2, streams, h_g1);\n","\n","     //printf(\"Order:\\t\");\n","     //for(int i = 0; i < h_g1->numVertices; i++) {\n","     //   printf(\"%d \", order[i]);\n","     //}\n","     //printf(\"\\n\");\n","\n","    int sizeCandidates = 0;\n","    int* candidates = findCandidatesGPU(d_g1, d_g2, d_state, order[0], &sizeCandidates, h_g1, h_g2, streams);\n","\n","    StackNode* stack = createStack();\n","    Info* info = createInfo(candidates, sizeCandidates, order[0]);\n","    push(&stack, info);\n","\n","    int matchingNode = 1;\n","    while (!isStackEmpty(stack)) {\n","        Info* info = peek(&stack);\n","        bool isMatch = false;\n","\n","        // printInfo(info);\n","\n","        for(int i = info->candidateIndex; i < info->sizeCandidates; i++) {\n","            int candidate = info->candidates[i];\n","            info->candidateIndex = i + 1;\n","\n","            int ret = cutISOGPU(d_g1, d_g2, d_state, info->vertex, candidate, h_g1, h_g2, streams);\n","\n","            // printf(\"CutISO: %d\\n\", ret);\n","            // printf(\"\\n\");\n","\n","            if(!ret) {\n","                // printf(\"\\nMatch %d -> %d\\n\", info->vertex, candidate);\n","\n","                updateStateGPU(d_g1, d_g2, d_state, info->vertex, candidate, streams);\n","\n","                if(matchingNode >= d_g1->numVertices) {\n","                    freeStack(stack);\n","                    free(order);\n","                    printf(\"Graphs are isomorphic\\n\");\n","                    cudaStreamSynchronize(streams[0]);  // wait for updates on the state\n","                    cudaStreamSynchronize(streams[1]);\n","                    return;\n","                }\n","\n","                cudaStreamSynchronize(streams[0]);\n","                cudaStreamSynchronize(streams[1]);\n","\n","                candidates = findCandidatesGPU(d_g1, d_g2, d_state, order[matchingNode], &sizeCandidates, h_g1, h_g2, streams);\n","                Info* info = createInfo(candidates, sizeCandidates, order[matchingNode]);\n","                push(&stack, info);\n","                matchingNode++;\n","                isMatch = true;\n","                break;\n","            }\n","        }\n","\n","        // no more candidates\n","        if(!isMatch) {\n","            Info* tmp = pop(&stack);\n","            freeInfo(tmp);\n","            matchingNode--;\n","\n","            // backtracking\n","            if(!isStackEmpty(stack)) {\n","                Info* prevInfo = peek(&stack);\n","                int candidate = prevInfo->candidates[prevInfo->candidateIndex - 1];\n","                restoreStateGPU(d_g1, d_g2, d_state, prevInfo->vertex, candidate, streams);\n","            }\n","        }\n","    }\n","    free(order);\n","    freeStack(stack);   \n","}\n","\n","__global__ void findNeighborsKernel(int* matrix, int node, int* neighbors, int* size, int numVertices) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < numVertices) {\n","        if(matrix[node * numVertices + idx] == 1) {\n","            int index = atomicAdd(size, 1);\n","            neighbors[index] = idx;\n","        }\n","    }\n","}\n","\n","__global__ void checkLabelsKernel(int* neighbors1, int nbrSize1, int nbrSize2, int* labelsNbr, int numVertices,\n","    int* g1_nodesToLabel, int* d_result, int offset, int usage, int* neighbors2, int* g2_nodesToLabel) {   // idx is the id of the thread in the grid\n","\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < nbrSize1) {\n","        int nbr1 = neighbors1[idx];\n","        int labelNbr1 = g1_nodesToLabel[nbr1];\n","        bool found = false;\n","\n","        if(usage) {\n","\n","            for(int i = 0; i < nbrSize2; i++) {\n","                int nbr2 = constMem[i];     // constMem contains the neighbors2 with offset 0\n","                if(labelNbr1 == constMem[offset + nbr2]) {  // constMem contains the g2_nodesToLabel with offset degrees[node]\n","                    found = true;\n","                    labelsNbr[labelNbr1] = 1;\n","                    break;\n","                }\n","            }\n","\n","        } else {\n","\n","            for(int i = 0; i < nbrSize2; i++) {\n","                int nbr2 = neighbors2[i];\n","                if(labelNbr1 == g2_nodesToLabel[nbr2]) {\n","                    found = true;\n","                    labelsNbr[labelNbr1] = 1;\n","                    break;\n","                }\n","            }\n","        }\n","\n","        if(!found) {\n","            *d_result = 0;   // d_result is initialized to 1 by default\n","        }\n","    }\n","}\n","\n","__global__ void findNodesOfLabelKernel(int* neighbors, int* g_nodesToLabel, int label, int maxSize, int* size, int* nodes) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    if(idx < maxSize) {\n","        int vertex = neighbors[idx];\n","\n","        if(g_nodesToLabel[vertex] == label) {\n","            int index = atomicAdd(size, 1);   // atomicAdd returns the index respect to global memory (so can't be used shared memory)\n","            nodes[index] = vertex;\n","        }\n","    }\n","}\n","\n","__global__ void intersectionCountKernel(int* nodes, int* size, int* stateSet, int* count) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;    // idx is the id of the thread in the grid among all threads\n","    __shared__ int localCount;  // each block has own localCount variable (shared memory)\n","\n","    if(threadIdx.x == 0) {   // only one thread in the block initializes the localCount\n","        localCount = 0;\n","    }\n","\n","    __syncthreads();\n","\n","    if(idx < *size) {\n","        int vertex = nodes[idx];\n","\n","        if(stateSet[vertex] == 1) {\n","            atomicAdd(&localCount, 1);\n","        }\n","    }\n","\n","    __syncthreads();\n","\n","    if(threadIdx.x == 0) {  // only the thread with id 0 of each block updates the global size\n","        atomicAdd(count, localCount);\n","    }\n","}\n","\n","bool cutISOGPU(Graph* d_g1, Graph* d_g2, State* d_state, int node1, int node2, Graph* h_g1, Graph* h_g2, cudaStream_t* streams) {\n","    cudaStream_t stream1 = streams[0];\n","    cudaStream_t stream2 = streams[1];\n","    cudaStream_t stream3 = streams[2];\n","    cudaStream_t stream4 = streams[3];\n","    cudaStream_t stream5 = streams[4];\n","    cudaStream_t stream6 = streams[5];\n","\n","    int nbrSize1 = h_g1->degrees[node1];\n","    int nbrSize2 = h_g2->degrees[node2];\n","\n","    size_t nbrSize1_bytes = nbrSize1 * sizeof(int);\n","    size_t nbrSize2_bytes = nbrSize2 * sizeof(int);\n","    size_t size1 = d_g1->numVertices * sizeof(int);\n","    size_t size2 = d_g2->numVertices * sizeof(int);\n","    size_t labelSize = LABELS * sizeof(int);\n","\n","    int useConstMem = d_g2->numVertices + nbrSize2 < MAXCONSTMEM;\n","\n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_g2->nodesToLabel, size2, nbrSize2_bytes, cudaMemcpyDeviceToDevice, stream3)); // constMem contains g2_nodesToLabel\n","    }\n","\n","    int* d_neighbors1, *d_neighbors2;\n","    int* d_nbrSize1, *d_nbrSize2;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors2, nbrSize2_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize2, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize2, 0, sizeof(int), stream2));\n","   \n","    int gridSize = (d_g2->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream2>>>(d_g2->matrix, node2, d_neighbors2, d_nbrSize2, d_g2->numVertices);\n","    \n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_neighbors1, nbrSize1_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nbrSize1, sizeof(int)));\n","    \n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_nbrSize1, 0, sizeof(int), stream1));\n","    \n","    gridSize = (d_g1->numVertices + blockSize - 1) / blockSize;\n","\n","    findNeighborsKernel<<<gridSize, blockSize, 0, stream1>>>(d_g1->matrix, node1, d_neighbors1, d_nbrSize1, d_g1->numVertices);\n","\n","    int* d_labelsNbr;\n","    int* d_result;\n","    int result = 1;                               \n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_labelsNbr, labelSize));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_result, sizeof(int)));\n","    \n","    CUDA_CHECK_ERROR(cudaMemsetAsync(d_labelsNbr, 0, labelSize, stream4));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(d_result, &result, sizeof(int), cudaMemcpyHostToDevice, stream4));\n","\n","    gridSize = (nbrSize1 + blockSize - 1) / blockSize;\n","\n","    cudaStreamSynchronize(stream3); // the first write on constMem must be finished before the second write on constMem\n","    \n","    if(useConstMem) {\n","        CUDA_CHECK_ERROR(cudaMemcpyToSymbolAsync(constMem, d_neighbors2, nbrSize2_bytes, 0, cudaMemcpyDeviceToDevice, stream2)); // constMem contains neighbors2. it is queued after findNeighborsKernel on stream2 \n","    }\n","\n","    cudaStreamSynchronize(stream4);\n","    cudaStreamSynchronize(stream2);\n","\n","    checkLabelsKernel<<<gridSize, blockSize, 0, stream1>>>(d_neighbors1, nbrSize1, nbrSize2, d_labelsNbr, d_g1->numVertices, \n","                        d_g1->nodesToLabel, d_result, nbrSize2, useConstMem, d_neighbors2, d_g2->nodesToLabel);\n","\n","    int* labelsNbr = (int*)malloc(labelSize);\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(&result, d_result, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","    CUDA_CHECK_ERROR(cudaMemcpyAsync(labelsNbr, d_labelsNbr, labelSize, cudaMemcpyDeviceToHost, stream1));\n","\n","    cudaStreamSynchronize(stream1);\n","\n","    if(result == 0) {\n","        cudaFree(d_neighbors1);\n","        cudaFree(d_neighbors2);\n","        cudaFree(d_nbrSize1);\n","        cudaFree(d_nbrSize2);\n","        cudaFree(d_labelsNbr);\n","        cudaFree(d_result);\n","        free(labelsNbr);\n","        return true;\n","    }\n","\n","    // pinned memory for faster host-device communication\n","    int *h_size1, *h_size2;\n","    int *h_count1, *h_count2, *h_count3, *h_count4;\n","    int *d_count1, *d_count2, *d_count3, *d_count4;\n","\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_size1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_size2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMallocHost((void**)&h_count4, sizeof(int)));\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count2, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count3, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_count4, sizeof(int)));\n","\n","    int *d_nodes_g1, *d_size_g1;\n","    int *d_nodes_g2, *d_size_g2;\n","\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g1, nbrSize1_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g1, sizeof(int)));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_nodes_g2, nbrSize2_bytes));\n","    CUDA_CHECK_ERROR(cudaMalloc((void**)&d_size_g2, sizeof(int)));\n","\n","    int gridSize1 = (nbrSize1 + blockSize - 1) / blockSize;\n","    int gridSize2 = (nbrSize2 + blockSize - 1) / blockSize;\n","   \n","    bool ret = false;\n","    for(int label = 0; label < LABELS; label++) {\n","        if(labelsNbr[label] == 1) {\n","\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g1, 0, sizeof(int), stream1));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_size_g2, 0, sizeof(int), stream2));\n","\n","            findNodesOfLabelKernel<<<gridSize1, blockSize, 0, stream1>>>(d_neighbors1, d_g1->nodesToLabel, label, nbrSize1, d_size_g1, d_nodes_g1);\n","            findNodesOfLabelKernel<<<gridSize2, blockSize, 0, stream2>>>(d_neighbors2, d_g2->nodesToLabel, label, nbrSize2, d_size_g2, d_nodes_g2);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_size1, d_size_g1, sizeof(int), cudaMemcpyDeviceToHost, stream1));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_size2, d_size_g2, sizeof(int), cudaMemcpyDeviceToHost, stream2));\n","\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count1, 0, sizeof(int), stream3));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count2, 0, sizeof(int), stream4));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count3, 0, sizeof(int), stream5));\n","            CUDA_CHECK_ERROR(cudaMemsetAsync(d_count4, 0, sizeof(int), stream6));\n","\n","            cudaStreamSynchronize(stream1);\n","            cudaStreamSynchronize(stream2);\n","\n","            gridSize = (*h_size1 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream3>>>(d_nodes_g1, d_size_g1, d_state->T1, d_count1);\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream5>>>(d_nodes_g1, d_size_g1, d_state->T1_out, d_count3);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count1, d_count1, sizeof(int), cudaMemcpyDeviceToHost, stream3));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count3, d_count3, sizeof(int), cudaMemcpyDeviceToHost, stream5));\n","            \n","            gridSize = (*h_size2 + blockSize - 1) / blockSize;\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream4>>>(d_nodes_g2, d_size_g2, d_state->T2, d_count2);\n","            intersectionCountKernel<<<gridSize, blockSize, 0, stream6>>>(d_nodes_g2, d_size_g2, d_state->T2_out, d_count4);\n","\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count2, d_count2, sizeof(int), cudaMemcpyDeviceToHost, stream4));\n","            CUDA_CHECK_ERROR(cudaMemcpyAsync(h_count4, d_count4, sizeof(int), cudaMemcpyDeviceToHost, stream6));\n","           \n","            cudaStreamSynchronize(stream3);\n","            cudaStreamSynchronize(stream4);\n","            cudaStreamSynchronize(stream5);\n","            cudaStreamSynchronize(stream6);\n","\n","            if(*h_count1 != *h_count2 || *h_count3 != *h_count4) {\n","                ret = true;\n","                break;\n","            }\n","        }\n","    }\n","\n","    cudaFree(d_neighbors1);\n","    cudaFree(d_neighbors2);\n","    cudaFree(d_nbrSize1);\n","    cudaFree(d_nbrSize2);\n","    cudaFree(d_labelsNbr);\n","    cudaFree(d_result);\n","    free(labelsNbr);\n","\n","    cudaFreeHost(h_size1);\n","    cudaFreeHost(h_size2);\n","    cudaFreeHost(h_count1);\n","    cudaFreeHost(h_count2);\n","    cudaFreeHost(h_count3);\n","    cudaFreeHost(h_count4);\n","    \n","    cudaFree(d_count1);\n","    cudaFree(d_count2);\n","    cudaFree(d_count3);\n","    cudaFree(d_count4);\n","\n","    cudaFree(d_nodes_g1);\n","    cudaFree(d_size_g1);\n","    cudaFree(d_nodes_g2);\n","    cudaFree(d_size_g2);\n","\n","    return ret;\n","}\n","\n","/***** STACK FUNCTIONS *****/\n","Info* createInfo(int* candidates, int sizeCandidates, int vertex) {\n","    Info* info = (Info*)malloc(sizeof(Info));\n","    if (info == NULL) {\n","        printf(\"Error allocating memory in createInfo\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    info->vertex = vertex;\n","    info->candidates = (int*)realloc(candidates, sizeCandidates * sizeof(int));\n","    info->sizeCandidates = sizeCandidates;\n","    info->candidateIndex = 0;\n","    return info;\n","}\n","\n","StackNode* createStackNode(Info* info) {\n","    StackNode* node = (StackNode*)malloc(sizeof(StackNode));\n","    if (node == NULL) {\n","        printf(\"Error allocating memory in createStackNode\\n\");\n","        exit(EXIT_FAILURE);\n","    }\n","    node->info = info;\n","    node->next = NULL;\n","    return node;\n","}\n","\n","void push(StackNode** top, Info* info) {\n","    StackNode* node = createStackNode(info);\n","    node->next = *top;\n","    *top = node;\n","}\n","\n","Info* pop(StackNode** top) {\n","    if (isStackEmpty(*top)) {\n","        printf(\"Stack is empty, cannot pop\\n\");\n","        return NULL;\n","    }\n","    StackNode* node = *top;\n","    Info* info = node->info;\n","    *top = node->next;\n","    free(node);\n","    return info;\n","}\n","\n","bool isStackEmpty(StackNode* top) {\n","    return top == NULL;\n","}\n","\n","void freeStack(StackNode* top) {\n","    while (!isStackEmpty(top)) {\n","        StackNode* node = top;\n","        top = top->next;\n","        freeInfo(node->info);\n","        free(node);\n","    }\n","}\n","\n","void printStack(StackNode* top) {\n","    StackNode* current = top;\n","    while (current != NULL) {\n","        printInfo(current->info);\n","        current = current->next;\n","    }\n","}\n","\n","void printInfo(Info* info) {\n","    printf(\"\\nVertex: %d\\n\", info->vertex);\n","    printf(\"Index seen: %d\\n\", info->candidateIndex);\n","    printf(\"Candidates: \");\n","    for (int i = 0; i < info->sizeCandidates; i++) {\n","        printf(\"%d \", info->candidates[i]);\n","    }\n","    printf(\"\\n\");\n","}\n","\n","void freeInfo(Info* info) {\n","    free(info->candidates);\n","    free(info);\n","}\n","\n","StackNode* createStack() {\n","    return NULL;\n","}\n","\n","Info* peek(StackNode** top) {\n","    return (*top)->info;\n","}"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
